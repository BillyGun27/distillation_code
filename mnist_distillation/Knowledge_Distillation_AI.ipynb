{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "_uBhi585gQt2",
    "outputId": "af160177-e730-4edd-ed7a-0fb74047d016"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Activation, Input, Embedding, LSTM, Dense, Lambda, GaussianNoise, concatenate\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, merge\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.constraints import max_norm\n",
    "from keras.layers import MaxPooling2D, Dropout, Dense, Flatten, Activation, Conv2D\n",
    "from keras.models import Sequential\n",
    "from keras.losses import categorical_crossentropy as logloss\n",
    "from keras.metrics import categorical_accuracy\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v1dtl7u_g70f"
   },
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zK8cBHCjhCIl"
   },
   "outputs": [],
   "source": [
    "nb_classes = 10\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# convert y_train and y_test to categorical binary values \n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "TQoj-dFLhEhr",
    "outputId": "7c10f38e-1629-4ee2-e0ec-c856ceee98fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "z-Sz-WQ5zZ65",
    "outputId": "71d200a8-069c-4e9e-e97f-8b0fdc474e6f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DFtmUKSEhHbu"
   },
   "source": [
    "# Preprocessing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "3QC7acYHhMiR",
    "outputId": "49c20d10-19fe-4bd8-f97f-396323e01388"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# Reshape them to batch_size, width,height,#channels\n",
    "X_train = X_train.reshape(60000, 28, 28, 1)\n",
    "X_test = X_test.reshape(10000, 28, 28, 1)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Normalize the values\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "44cw18qKhT_V"
   },
   "source": [
    "# Define Teacher Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 625
    },
    "colab_type": "code",
    "id": "f4RVzgD_hSvo",
    "outputId": "94d5f256-8d8c-43e2-fb11-72b32e310181"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/miniconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/miniconda3/envs/tf36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /opt/miniconda3/envs/tf36/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py:3043: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Teacher model\n",
    "\n",
    "input_shape = (28, 28, 1) # Input shape of each image\n",
    "\n",
    "# Hyperparameters\n",
    "nb_filters = 64 # number of convolutional filters to use\n",
    "pool_size = (2, 2) # size of pooling area for max pooling\n",
    "kernel_size = (3, 3) # convolution kernel size\n",
    "\n",
    "teacher = Sequential()\n",
    "teacher.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "teacher.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "teacher.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "teacher.add(Dropout(0.25)) # For reguralization\n",
    "\n",
    "teacher.add(Flatten())\n",
    "teacher.add(Dense(128, activation='relu'))\n",
    "teacher.add(Dropout(0.5)) # For reguralization\n",
    "\n",
    "teacher.add(Dense(nb_classes))\n",
    "teacher.add(Activation('softmax')) # Note that we add a normal softmax layer to begin with\n",
    "\n",
    "teacher.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(teacher.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6RHkGp4tha8A"
   },
   "source": [
    "# Define Student Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329
    },
    "colab_type": "code",
    "id": "vR_gAsUqhZjP",
    "outputId": "7fe1afdf-7ba2-41a6-b2b1-2bbd21c8a1fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 25,450\n",
      "Trainable params: 25,450\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Student model that is stand-alone. We will evaluate its accuracy compared to a teacher trained student model\n",
    "\n",
    "student = Sequential()\n",
    "student.add(Flatten(input_shape=input_shape))\n",
    "student.add(Dense(32, activation='relu'))\n",
    "student.add(Dropout(0.2))\n",
    "student.add(Dense(nb_classes))\n",
    "student.add(Activation('softmax'))\n",
    "\n",
    "#sgd = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "student.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "student.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gc2uy8MUhiXs"
   },
   "source": [
    "# Training the Teacher model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "colab_type": "code",
    "id": "toF5EnoAhnQm",
    "outputId": "f4a4a220-8f15-44af-f66e-fc764951729f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nlog_dir = 'logs/teacher/'\\nlogging = TensorBoard(log_dir=log_dir)\\ncheckpoint = ModelCheckpoint(log_dir + 'ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5',\\n        monitor='val_loss', save_weights_only=True, save_best_only=True, period=3)\\n\\nepochs = 500\\nbatch_size = 256\\nteacher.fit(X_train, Y_train,\\n          batch_size=batch_size,\\n          epochs=epochs,\\n          verbose=1,\\n          validation_data=(X_test, Y_test),\\n           callbacks=[logging,checkpoint] )\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the teacher model as usual\n",
    "'''\n",
    "\n",
    "log_dir = 'logs/teacher/'\n",
    "logging = TensorBoard(log_dir=log_dir)\n",
    "checkpoint = ModelCheckpoint(log_dir + 'ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5',\n",
    "        monitor='val_loss', save_weights_only=True, save_best_only=True, period=3)\n",
    "\n",
    "epochs = 500\n",
    "batch_size = 256\n",
    "teacher.fit(X_train, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, Y_test),\n",
    "           callbacks=[logging,checkpoint] )\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 235us/step\n",
      "[0.0327168962046811, 0.9949]\n"
     ]
    }
   ],
   "source": [
    "teacher.load_weights(\"ep183-acc0.9988-val_acc0.9949-loss0.0040-val_loss0.0327.h5\")\n",
    "\n",
    "score = teacher.evaluate(X_test, Y_test)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o6g9inT2hqAU"
   },
   "source": [
    "# Define a new model that outputs only teacher logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oNuFpi3nh3p6"
   },
   "outputs": [],
   "source": [
    "# Raise the temperature of teacher model and gather the soft targets\n",
    "\n",
    "#Collect the logits from the previous layer output and store it in a different model\n",
    "teacher_WO_Softmax = Model(teacher.input, teacher.get_layer('dense_2').output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "colab_type": "code",
    "id": "RKe7WUXWgqFy",
    "outputId": "9d462f1e-befb-4f7e-a6bd-56a17313fd19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1_input (InputLayer)  (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "teacher_WO_Softmax.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oshn9uRTh6j-"
   },
   "source": [
    "# Define a manual softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8vb7CuA9h_Mb"
   },
   "outputs": [],
   "source": [
    "# Define a manual softmax function\n",
    "def softmax(x):\n",
    "    return np.exp(x)/(np.exp(x).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xtROpLm-iBb9"
   },
   "source": [
    "# Understanding the concept of temperature in softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 973
    },
    "colab_type": "code",
    "id": "6yzduRJSiAsu",
    "outputId": "b4ae4feb-8b99-4137-8f32-8de7ef942da6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.2035841e-29 3.8803228e-31 1.1746689e-33 7.2228541e-28 1.9147977e-16\n",
      "  9.1969972e-24 2.1563771e-36 4.2656528e-20 1.5456886e-19 1.0000000e+00]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa6f87fe828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa6f877a9b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For example, just grab the first image and lets see how softening of probabilities work\n",
    "intermediate_output = teacher_WO_Softmax.predict(X_test[9].reshape(1,28,28,1))\n",
    "print(softmax(intermediate_output))\n",
    "\n",
    "pixels = X_test[9]\n",
    "pixels = pixels.reshape((28, 28))\n",
    "plt.imshow(pixels)\n",
    "plt.savefig('Kimg.jpg')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# logits for the first number in test dataset\n",
    "x = intermediate_output[0]\n",
    "plt.figure(figsize=(20, 10));\n",
    "\n",
    "temperature = [1,3,7,10,20,50]\n",
    "\n",
    "for temp in temperature:\n",
    "    plt.plot((softmax(x/temp)), label='$T='+str(temp)+'$', linewidth=2);\n",
    "    plt.legend();\n",
    "plt.xlabel('classes ->');\n",
    "plt.ylabel('probability');\n",
    "plt.xlim([0, 10]);\n",
    "plt.savefig('Kgraph.jpg')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zvNRMD9HiMnr"
   },
   "source": [
    "# Prepare the soft targets and the target data for student to be trained upon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qDei6m9ZiMKU"
   },
   "outputs": [],
   "source": [
    "temp = 1\n",
    "teacher_train_logits = teacher_WO_Softmax.predict(X_train)\n",
    "teacher_test_logits = teacher_WO_Softmax.predict(X_test) # This model directly gives the logits ( see the teacher_WO_softmax model above)\n",
    "\n",
    "# Perform a manual softmax at raised temperature\n",
    "train_logits_T = teacher_train_logits/temp\n",
    "test_logits_T = teacher_test_logits / temp \n",
    "\n",
    "#Y_train_soft = softmax(train_logits_T)\n",
    "#Y_test_soft = softmax(test_logits_T)\n",
    "\n",
    "Y_train_soft = []\n",
    "Y_test_soft = []\n",
    "\n",
    "for i in range( len( train_logits_T ) ):\n",
    "  Y_train_soft.append( softmax( train_logits_T[i] ) )\n",
    "\n",
    "for i in range( len( test_logits_T ) ):\n",
    "  Y_test_soft.append( softmax( test_logits_T[i] ) )\n",
    "\n",
    "#len(Y_train_soft[0])\n",
    "Y_train_soft = np.array(Y_train_soft)\n",
    "Y_test_soft = np.array(Y_test_soft)\n",
    "\n",
    "\n",
    "# Concatenate so that this becomes a 10 + 10 dimensional vector\n",
    "Y_train_new = np.concatenate([Y_train, Y_train_soft], axis=1)\n",
    "Y_test_new =  np.concatenate([Y_test, Y_test_soft], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "colab_type": "code",
    "id": "FLFpB2hKieMp",
    "outputId": "8a220610-9261-4a5a-951e-97cd97894c11"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 1.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 1.5356115e-27, 3.4459211e-22,\n",
       "       7.7777399e-35, 1.1800338e-06, 3.3861200e-35, 9.9999881e-01,\n",
       "       1.0668360e-24, 3.5405121e-34, 4.3196108e-20, 1.4041691e-19],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_new[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.3319924, -2.0998735, -5.0118265,  1.4770977, -5.094985 ,\n",
       "        2.8420944, -2.67764  , -4.8602676, -1.6167591, -1.4988725],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_logits_T[0]/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UOLwI1vTh9Id"
   },
   "outputs": [],
   "source": [
    "# This is a standalone student model (same number of layers as original student model) trained on same data\n",
    "# for comparing it with teacher trained student.\n",
    "student = Sequential()\n",
    "student.add(Flatten(input_shape=input_shape))\n",
    "student.add(Dense(32, activation='relu'))\n",
    "student.add(Dropout(0.2))\n",
    "student.add(Dense(nb_classes))\n",
    "student.add(Activation('softmax'))\n",
    "\n",
    "#sgd = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "student.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy']\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "colab_type": "code",
    "id": "Df5P49XRjDyQ",
    "outputId": "abce734f-3307-481b-d270-77a77d7e4d48",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.7256 - acc: 0.7850 - val_loss: 0.3238 - val_acc: 0.9114\n",
      "Epoch 2/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.3835 - acc: 0.8885 - val_loss: 0.2505 - val_acc: 0.9309\n",
      "Epoch 3/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.3210 - acc: 0.9052 - val_loss: 0.2197 - val_acc: 0.9390\n",
      "Epoch 4/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.2874 - acc: 0.9161 - val_loss: 0.1949 - val_acc: 0.9449\n",
      "Epoch 5/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.2637 - acc: 0.9228 - val_loss: 0.1793 - val_acc: 0.9477\n",
      "Epoch 6/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.2489 - acc: 0.9270 - val_loss: 0.1685 - val_acc: 0.9506\n",
      "Epoch 7/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.2385 - acc: 0.9292 - val_loss: 0.1589 - val_acc: 0.9517\n",
      "Epoch 8/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.2281 - acc: 0.9331 - val_loss: 0.1534 - val_acc: 0.9558\n",
      "Epoch 9/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.2183 - acc: 0.9346 - val_loss: 0.1443 - val_acc: 0.9579\n",
      "Epoch 10/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.2123 - acc: 0.9367 - val_loss: 0.1456 - val_acc: 0.9571\n",
      "Epoch 11/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.2058 - acc: 0.9375 - val_loss: 0.1399 - val_acc: 0.9590\n",
      "Epoch 12/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.2005 - acc: 0.9389 - val_loss: 0.1352 - val_acc: 0.9597\n",
      "Epoch 13/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1932 - acc: 0.9410 - val_loss: 0.1331 - val_acc: 0.9607\n",
      "Epoch 14/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1878 - acc: 0.9430 - val_loss: 0.1303 - val_acc: 0.9617\n",
      "Epoch 15/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1835 - acc: 0.9444 - val_loss: 0.1299 - val_acc: 0.9610\n",
      "Epoch 16/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1822 - acc: 0.9448 - val_loss: 0.1262 - val_acc: 0.9619\n",
      "Epoch 17/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1779 - acc: 0.9472 - val_loss: 0.1259 - val_acc: 0.9626\n",
      "Epoch 18/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1774 - acc: 0.9453 - val_loss: 0.1258 - val_acc: 0.9638\n",
      "Epoch 19/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1744 - acc: 0.9471 - val_loss: 0.1234 - val_acc: 0.9633\n",
      "Epoch 20/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1690 - acc: 0.9473 - val_loss: 0.1226 - val_acc: 0.9621\n",
      "Epoch 21/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1685 - acc: 0.9476 - val_loss: 0.1211 - val_acc: 0.9653\n",
      "Epoch 22/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1660 - acc: 0.9494 - val_loss: 0.1190 - val_acc: 0.9654\n",
      "Epoch 23/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1609 - acc: 0.9513 - val_loss: 0.1185 - val_acc: 0.9645\n",
      "Epoch 24/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1595 - acc: 0.9512 - val_loss: 0.1181 - val_acc: 0.9667\n",
      "Epoch 25/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1594 - acc: 0.9511 - val_loss: 0.1153 - val_acc: 0.9661\n",
      "Epoch 26/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1545 - acc: 0.9530 - val_loss: 0.1157 - val_acc: 0.9666\n",
      "Epoch 27/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1524 - acc: 0.9532 - val_loss: 0.1157 - val_acc: 0.9666\n",
      "Epoch 28/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1538 - acc: 0.9517 - val_loss: 0.1193 - val_acc: 0.9643\n",
      "Epoch 29/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1518 - acc: 0.9524 - val_loss: 0.1172 - val_acc: 0.9654\n",
      "Epoch 30/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1517 - acc: 0.9536 - val_loss: 0.1163 - val_acc: 0.9675\n",
      "Epoch 31/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1465 - acc: 0.9551 - val_loss: 0.1141 - val_acc: 0.9668\n",
      "Epoch 32/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1467 - acc: 0.9542 - val_loss: 0.1160 - val_acc: 0.9674\n",
      "Epoch 33/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1473 - acc: 0.9540 - val_loss: 0.1120 - val_acc: 0.9676\n",
      "Epoch 34/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1451 - acc: 0.9549 - val_loss: 0.1119 - val_acc: 0.9669\n",
      "Epoch 35/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1439 - acc: 0.9547 - val_loss: 0.1137 - val_acc: 0.9674\n",
      "Epoch 36/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1438 - acc: 0.9554 - val_loss: 0.1109 - val_acc: 0.9686\n",
      "Epoch 37/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1422 - acc: 0.9556 - val_loss: 0.1131 - val_acc: 0.9672\n",
      "Epoch 38/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1392 - acc: 0.9563 - val_loss: 0.1131 - val_acc: 0.9672\n",
      "Epoch 39/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1385 - acc: 0.9562 - val_loss: 0.1144 - val_acc: 0.9672\n",
      "Epoch 40/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1386 - acc: 0.9574 - val_loss: 0.1129 - val_acc: 0.9675\n",
      "Epoch 41/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1380 - acc: 0.9563 - val_loss: 0.1121 - val_acc: 0.9679\n",
      "Epoch 42/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1400 - acc: 0.9563 - val_loss: 0.1125 - val_acc: 0.9684\n",
      "Epoch 43/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1327 - acc: 0.9579 - val_loss: 0.1142 - val_acc: 0.9679\n",
      "Epoch 44/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1355 - acc: 0.9576 - val_loss: 0.1116 - val_acc: 0.9685\n",
      "Epoch 45/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1338 - acc: 0.9584 - val_loss: 0.1142 - val_acc: 0.9673\n",
      "Epoch 46/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1302 - acc: 0.9599 - val_loss: 0.1100 - val_acc: 0.9682\n",
      "Epoch 47/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1311 - acc: 0.9591 - val_loss: 0.1111 - val_acc: 0.9683\n",
      "Epoch 48/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1330 - acc: 0.9587 - val_loss: 0.1092 - val_acc: 0.9692\n",
      "Epoch 49/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1295 - acc: 0.9593 - val_loss: 0.1120 - val_acc: 0.9678\n",
      "Epoch 50/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1249 - acc: 0.9607 - val_loss: 0.1092 - val_acc: 0.9687\n",
      "Epoch 51/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1298 - acc: 0.9595 - val_loss: 0.1106 - val_acc: 0.9681\n",
      "Epoch 52/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1264 - acc: 0.9600 - val_loss: 0.1097 - val_acc: 0.9682\n",
      "Epoch 53/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1289 - acc: 0.9592 - val_loss: 0.1137 - val_acc: 0.9686\n",
      "Epoch 54/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1304 - acc: 0.9587 - val_loss: 0.1098 - val_acc: 0.9687\n",
      "Epoch 55/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1251 - acc: 0.9611 - val_loss: 0.1108 - val_acc: 0.9686\n",
      "Epoch 56/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1239 - acc: 0.9613 - val_loss: 0.1109 - val_acc: 0.9686\n",
      "Epoch 57/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1275 - acc: 0.9595 - val_loss: 0.1117 - val_acc: 0.9685\n",
      "Epoch 58/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1218 - acc: 0.9614 - val_loss: 0.1110 - val_acc: 0.9688\n",
      "Epoch 59/500\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 0.1298 - acc: 0.9588 - val_loss: 0.1113 - val_acc: 0.9691\n",
      "Epoch 60/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1247 - acc: 0.9611 - val_loss: 0.1103 - val_acc: 0.9684\n",
      "Epoch 61/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1242 - acc: 0.9602 - val_loss: 0.1094 - val_acc: 0.9690\n",
      "Epoch 62/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1228 - acc: 0.9610 - val_loss: 0.1110 - val_acc: 0.9679\n",
      "Epoch 63/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1260 - acc: 0.9601 - val_loss: 0.1100 - val_acc: 0.9688\n",
      "Epoch 64/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1220 - acc: 0.9615 - val_loss: 0.1099 - val_acc: 0.9693\n",
      "Epoch 65/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1210 - acc: 0.9615 - val_loss: 0.1077 - val_acc: 0.9686\n",
      "Epoch 66/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1206 - acc: 0.9614 - val_loss: 0.1129 - val_acc: 0.9686\n",
      "Epoch 67/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1191 - acc: 0.9629 - val_loss: 0.1129 - val_acc: 0.9679\n",
      "Epoch 68/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1181 - acc: 0.9630 - val_loss: 0.1120 - val_acc: 0.9684\n",
      "Epoch 69/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1193 - acc: 0.9622 - val_loss: 0.1113 - val_acc: 0.9682\n",
      "Epoch 70/500\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 0.1192 - acc: 0.9628 - val_loss: 0.1157 - val_acc: 0.9681\n",
      "Epoch 71/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1200 - acc: 0.9619 - val_loss: 0.1117 - val_acc: 0.9690\n",
      "Epoch 72/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1184 - acc: 0.9630 - val_loss: 0.1122 - val_acc: 0.9682\n",
      "Epoch 73/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1173 - acc: 0.9628 - val_loss: 0.1123 - val_acc: 0.9687\n",
      "Epoch 74/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1186 - acc: 0.9618 - val_loss: 0.1123 - val_acc: 0.9686\n",
      "Epoch 75/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1176 - acc: 0.9622 - val_loss: 0.1130 - val_acc: 0.9681\n",
      "Epoch 76/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1166 - acc: 0.9638 - val_loss: 0.1120 - val_acc: 0.9691\n",
      "Epoch 77/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1149 - acc: 0.9635 - val_loss: 0.1124 - val_acc: 0.9688\n",
      "Epoch 78/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1159 - acc: 0.9637 - val_loss: 0.1114 - val_acc: 0.9681\n",
      "Epoch 79/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1149 - acc: 0.9631 - val_loss: 0.1119 - val_acc: 0.9687\n",
      "Epoch 80/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1125 - acc: 0.9637 - val_loss: 0.1136 - val_acc: 0.9683\n",
      "Epoch 81/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1151 - acc: 0.9635 - val_loss: 0.1109 - val_acc: 0.9686\n",
      "Epoch 82/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1147 - acc: 0.9635 - val_loss: 0.1176 - val_acc: 0.9680\n",
      "Epoch 83/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1132 - acc: 0.9637 - val_loss: 0.1120 - val_acc: 0.9695\n",
      "Epoch 84/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1132 - acc: 0.9640 - val_loss: 0.1115 - val_acc: 0.9696\n",
      "Epoch 85/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1121 - acc: 0.9640 - val_loss: 0.1145 - val_acc: 0.9683\n",
      "Epoch 86/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1139 - acc: 0.9633 - val_loss: 0.1162 - val_acc: 0.9687\n",
      "Epoch 87/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1132 - acc: 0.9642 - val_loss: 0.1162 - val_acc: 0.9687\n",
      "Epoch 88/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1145 - acc: 0.9640 - val_loss: 0.1156 - val_acc: 0.9692\n",
      "Epoch 89/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1139 - acc: 0.9636 - val_loss: 0.1161 - val_acc: 0.9683\n",
      "Epoch 90/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1096 - acc: 0.9648 - val_loss: 0.1177 - val_acc: 0.9676\n",
      "Epoch 91/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1103 - acc: 0.9648 - val_loss: 0.1175 - val_acc: 0.9685\n",
      "Epoch 92/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1124 - acc: 0.9641 - val_loss: 0.1159 - val_acc: 0.9675\n",
      "Epoch 93/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1088 - acc: 0.9656 - val_loss: 0.1182 - val_acc: 0.9685\n",
      "Epoch 94/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1090 - acc: 0.9651 - val_loss: 0.1128 - val_acc: 0.9676\n",
      "Epoch 95/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1077 - acc: 0.9655 - val_loss: 0.1163 - val_acc: 0.9685\n",
      "Epoch 96/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1120 - acc: 0.9639 - val_loss: 0.1145 - val_acc: 0.9687\n",
      "Epoch 97/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1086 - acc: 0.9649 - val_loss: 0.1139 - val_acc: 0.9696\n",
      "Epoch 98/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1076 - acc: 0.9657 - val_loss: 0.1142 - val_acc: 0.9688\n",
      "Epoch 99/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1075 - acc: 0.9652 - val_loss: 0.1130 - val_acc: 0.9694\n",
      "Epoch 100/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1093 - acc: 0.9655 - val_loss: 0.1210 - val_acc: 0.9674\n",
      "Epoch 101/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1102 - acc: 0.9643 - val_loss: 0.1151 - val_acc: 0.9701\n",
      "Epoch 102/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1108 - acc: 0.9651 - val_loss: 0.1140 - val_acc: 0.9686\n",
      "Epoch 103/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1073 - acc: 0.9659 - val_loss: 0.1167 - val_acc: 0.9679\n",
      "Epoch 104/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1069 - acc: 0.9655 - val_loss: 0.1152 - val_acc: 0.9697\n",
      "Epoch 105/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1085 - acc: 0.9655 - val_loss: 0.1184 - val_acc: 0.9672\n",
      "Epoch 106/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1084 - acc: 0.9649 - val_loss: 0.1160 - val_acc: 0.9693\n",
      "Epoch 107/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1057 - acc: 0.9663 - val_loss: 0.1150 - val_acc: 0.9694\n",
      "Epoch 108/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1067 - acc: 0.9650 - val_loss: 0.1155 - val_acc: 0.9692\n",
      "Epoch 109/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1058 - acc: 0.9651 - val_loss: 0.1168 - val_acc: 0.9692\n",
      "Epoch 110/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1070 - acc: 0.9654 - val_loss: 0.1182 - val_acc: 0.9699\n",
      "Epoch 111/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1045 - acc: 0.9666 - val_loss: 0.1184 - val_acc: 0.9690\n",
      "Epoch 112/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1049 - acc: 0.9657 - val_loss: 0.1169 - val_acc: 0.9678\n",
      "Epoch 113/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1051 - acc: 0.9661 - val_loss: 0.1157 - val_acc: 0.9696\n",
      "Epoch 114/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1048 - acc: 0.9667 - val_loss: 0.1175 - val_acc: 0.9684\n",
      "Epoch 115/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1030 - acc: 0.9663 - val_loss: 0.1163 - val_acc: 0.9691\n",
      "Epoch 116/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1072 - acc: 0.9657 - val_loss: 0.1180 - val_acc: 0.9682\n",
      "Epoch 117/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1032 - acc: 0.9671 - val_loss: 0.1175 - val_acc: 0.9693\n",
      "Epoch 118/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1036 - acc: 0.9655 - val_loss: 0.1158 - val_acc: 0.9693\n",
      "Epoch 119/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1034 - acc: 0.9674 - val_loss: 0.1211 - val_acc: 0.9676\n",
      "Epoch 120/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1050 - acc: 0.9662 - val_loss: 0.1188 - val_acc: 0.9692\n",
      "Epoch 121/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1035 - acc: 0.9667 - val_loss: 0.1230 - val_acc: 0.9678\n",
      "Epoch 122/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1049 - acc: 0.9661 - val_loss: 0.1194 - val_acc: 0.9688\n",
      "Epoch 123/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1032 - acc: 0.9662 - val_loss: 0.1223 - val_acc: 0.9689\n",
      "Epoch 124/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1036 - acc: 0.9673 - val_loss: 0.1181 - val_acc: 0.9685\n",
      "Epoch 125/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1017 - acc: 0.9672 - val_loss: 0.1195 - val_acc: 0.9700\n",
      "Epoch 126/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1046 - acc: 0.9661 - val_loss: 0.1184 - val_acc: 0.9686\n",
      "Epoch 127/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1035 - acc: 0.9661 - val_loss: 0.1192 - val_acc: 0.9694\n",
      "Epoch 128/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1013 - acc: 0.9675 - val_loss: 0.1194 - val_acc: 0.9687\n",
      "Epoch 129/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1023 - acc: 0.9675 - val_loss: 0.1231 - val_acc: 0.9685\n",
      "Epoch 130/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0991 - acc: 0.9680 - val_loss: 0.1252 - val_acc: 0.9679\n",
      "Epoch 131/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1035 - acc: 0.9668 - val_loss: 0.1209 - val_acc: 0.9689\n",
      "Epoch 132/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1002 - acc: 0.9673 - val_loss: 0.1221 - val_acc: 0.9686\n",
      "Epoch 133/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1018 - acc: 0.9675 - val_loss: 0.1195 - val_acc: 0.9680\n",
      "Epoch 134/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1016 - acc: 0.9671 - val_loss: 0.1203 - val_acc: 0.9687\n",
      "Epoch 135/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0997 - acc: 0.9681 - val_loss: 0.1248 - val_acc: 0.9682\n",
      "Epoch 136/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1009 - acc: 0.9676 - val_loss: 0.1177 - val_acc: 0.9695\n",
      "Epoch 137/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0996 - acc: 0.9683 - val_loss: 0.1193 - val_acc: 0.9681\n",
      "Epoch 138/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1003 - acc: 0.9673 - val_loss: 0.1202 - val_acc: 0.9692\n",
      "Epoch 139/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0976 - acc: 0.9676 - val_loss: 0.1215 - val_acc: 0.9691\n",
      "Epoch 140/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1003 - acc: 0.9672 - val_loss: 0.1255 - val_acc: 0.9676\n",
      "Epoch 141/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1015 - acc: 0.9673 - val_loss: 0.1227 - val_acc: 0.9694\n",
      "Epoch 142/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1003 - acc: 0.9670 - val_loss: 0.1218 - val_acc: 0.9698\n",
      "Epoch 143/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1007 - acc: 0.9676 - val_loss: 0.1210 - val_acc: 0.9694\n",
      "Epoch 144/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0973 - acc: 0.9682 - val_loss: 0.1206 - val_acc: 0.9698\n",
      "Epoch 145/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0980 - acc: 0.9689 - val_loss: 0.1215 - val_acc: 0.9682\n",
      "Epoch 146/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0972 - acc: 0.9676 - val_loss: 0.1231 - val_acc: 0.9685\n",
      "Epoch 147/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0963 - acc: 0.9687 - val_loss: 0.1240 - val_acc: 0.9688\n",
      "Epoch 148/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0959 - acc: 0.9695 - val_loss: 0.1268 - val_acc: 0.9685\n",
      "Epoch 149/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0993 - acc: 0.9680 - val_loss: 0.1263 - val_acc: 0.9690\n",
      "Epoch 150/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1007 - acc: 0.9673 - val_loss: 0.1278 - val_acc: 0.9680\n",
      "Epoch 151/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0976 - acc: 0.9683 - val_loss: 0.1247 - val_acc: 0.9695\n",
      "Epoch 152/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0955 - acc: 0.9695 - val_loss: 0.1222 - val_acc: 0.9679\n",
      "Epoch 153/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0964 - acc: 0.9688 - val_loss: 0.1247 - val_acc: 0.9681\n",
      "Epoch 154/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0981 - acc: 0.9678 - val_loss: 0.1255 - val_acc: 0.9689\n",
      "Epoch 155/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0977 - acc: 0.9689 - val_loss: 0.1244 - val_acc: 0.9685\n",
      "Epoch 156/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0985 - acc: 0.9682 - val_loss: 0.1235 - val_acc: 0.9687\n",
      "Epoch 157/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0972 - acc: 0.9680 - val_loss: 0.1237 - val_acc: 0.9691\n",
      "Epoch 158/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0976 - acc: 0.9685 - val_loss: 0.1248 - val_acc: 0.9684\n",
      "Epoch 159/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0973 - acc: 0.9681 - val_loss: 0.1230 - val_acc: 0.9691\n",
      "Epoch 160/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0975 - acc: 0.9684 - val_loss: 0.1277 - val_acc: 0.9680\n",
      "Epoch 161/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0966 - acc: 0.9681 - val_loss: 0.1281 - val_acc: 0.9685\n",
      "Epoch 162/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0979 - acc: 0.9683 - val_loss: 0.1280 - val_acc: 0.9681\n",
      "Epoch 163/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0986 - acc: 0.9681 - val_loss: 0.1294 - val_acc: 0.9674\n",
      "Epoch 164/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0967 - acc: 0.9688 - val_loss: 0.1289 - val_acc: 0.9680\n",
      "Epoch 165/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0971 - acc: 0.9690 - val_loss: 0.1269 - val_acc: 0.9689\n",
      "Epoch 166/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0969 - acc: 0.9689 - val_loss: 0.1238 - val_acc: 0.9691\n",
      "Epoch 167/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0943 - acc: 0.9695 - val_loss: 0.1309 - val_acc: 0.9679\n",
      "Epoch 168/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0956 - acc: 0.9695 - val_loss: 0.1293 - val_acc: 0.9680\n",
      "Epoch 169/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0965 - acc: 0.9697 - val_loss: 0.1263 - val_acc: 0.9683\n",
      "Epoch 170/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0922 - acc: 0.9712 - val_loss: 0.1265 - val_acc: 0.9675\n",
      "Epoch 171/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0946 - acc: 0.9695 - val_loss: 0.1267 - val_acc: 0.9691\n",
      "Epoch 172/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0943 - acc: 0.9701 - val_loss: 0.1295 - val_acc: 0.9683\n",
      "Epoch 173/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0948 - acc: 0.9690 - val_loss: 0.1275 - val_acc: 0.9681\n",
      "Epoch 174/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0981 - acc: 0.9685 - val_loss: 0.1256 - val_acc: 0.9678\n",
      "Epoch 175/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0927 - acc: 0.9699 - val_loss: 0.1279 - val_acc: 0.9686\n",
      "Epoch 176/500\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 0.0952 - acc: 0.9693 - val_loss: 0.1295 - val_acc: 0.9679\n",
      "Epoch 177/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0940 - acc: 0.9691 - val_loss: 0.1318 - val_acc: 0.9670\n",
      "Epoch 178/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0975 - acc: 0.9683 - val_loss: 0.1302 - val_acc: 0.9682\n",
      "Epoch 179/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0924 - acc: 0.9704 - val_loss: 0.1322 - val_acc: 0.9672\n",
      "Epoch 180/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0945 - acc: 0.9696 - val_loss: 0.1291 - val_acc: 0.9687\n",
      "Epoch 181/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0943 - acc: 0.9696 - val_loss: 0.1294 - val_acc: 0.9684\n",
      "Epoch 182/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0948 - acc: 0.9687 - val_loss: 0.1295 - val_acc: 0.9678\n",
      "Epoch 183/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0954 - acc: 0.9692 - val_loss: 0.1258 - val_acc: 0.9689\n",
      "Epoch 184/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0920 - acc: 0.9706 - val_loss: 0.1306 - val_acc: 0.9685\n",
      "Epoch 185/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0937 - acc: 0.9697 - val_loss: 0.1318 - val_acc: 0.9668\n",
      "Epoch 186/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0928 - acc: 0.9699 - val_loss: 0.1305 - val_acc: 0.9676\n",
      "Epoch 187/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0918 - acc: 0.9704 - val_loss: 0.1309 - val_acc: 0.9674\n",
      "Epoch 188/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0932 - acc: 0.9694 - val_loss: 0.1321 - val_acc: 0.9670\n",
      "Epoch 189/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0920 - acc: 0.9702 - val_loss: 0.1291 - val_acc: 0.9674\n",
      "Epoch 190/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0907 - acc: 0.9706 - val_loss: 0.1321 - val_acc: 0.9678\n",
      "Epoch 191/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0939 - acc: 0.9693 - val_loss: 0.1308 - val_acc: 0.9676\n",
      "Epoch 192/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0927 - acc: 0.9684 - val_loss: 0.1316 - val_acc: 0.9676\n",
      "Epoch 193/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0928 - acc: 0.9699 - val_loss: 0.1324 - val_acc: 0.9683\n",
      "Epoch 194/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0924 - acc: 0.9702 - val_loss: 0.1340 - val_acc: 0.9670\n",
      "Epoch 195/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0937 - acc: 0.9696 - val_loss: 0.1317 - val_acc: 0.9683\n",
      "Epoch 196/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0927 - acc: 0.9702 - val_loss: 0.1356 - val_acc: 0.9667\n",
      "Epoch 197/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0916 - acc: 0.9695 - val_loss: 0.1336 - val_acc: 0.9672\n",
      "Epoch 198/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0919 - acc: 0.9697 - val_loss: 0.1327 - val_acc: 0.9677\n",
      "Epoch 199/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0923 - acc: 0.9698 - val_loss: 0.1318 - val_acc: 0.9687\n",
      "Epoch 200/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0919 - acc: 0.9698 - val_loss: 0.1332 - val_acc: 0.9674\n",
      "Epoch 201/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0919 - acc: 0.9699 - val_loss: 0.1320 - val_acc: 0.9683\n",
      "Epoch 202/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0932 - acc: 0.9699 - val_loss: 0.1330 - val_acc: 0.9671\n",
      "Epoch 203/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0918 - acc: 0.9704 - val_loss: 0.1355 - val_acc: 0.9672\n",
      "Epoch 204/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0906 - acc: 0.9693 - val_loss: 0.1328 - val_acc: 0.9679\n",
      "Epoch 205/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0875 - acc: 0.9715 - val_loss: 0.1321 - val_acc: 0.9679\n",
      "Epoch 206/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0905 - acc: 0.9715 - val_loss: 0.1345 - val_acc: 0.9679\n",
      "Epoch 207/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0896 - acc: 0.9704 - val_loss: 0.1324 - val_acc: 0.9683\n",
      "Epoch 208/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0927 - acc: 0.9698 - val_loss: 0.1335 - val_acc: 0.9674\n",
      "Epoch 209/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0904 - acc: 0.9703 - val_loss: 0.1339 - val_acc: 0.9672\n",
      "Epoch 210/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0952 - acc: 0.9694 - val_loss: 0.1339 - val_acc: 0.9668\n",
      "Epoch 211/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0887 - acc: 0.9712 - val_loss: 0.1363 - val_acc: 0.9667\n",
      "Epoch 212/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0946 - acc: 0.9683 - val_loss: 0.1365 - val_acc: 0.9659\n",
      "Epoch 213/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0907 - acc: 0.9702 - val_loss: 0.1387 - val_acc: 0.9666\n",
      "Epoch 214/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0892 - acc: 0.9707 - val_loss: 0.1359 - val_acc: 0.9681\n",
      "Epoch 215/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0915 - acc: 0.9711 - val_loss: 0.1402 - val_acc: 0.9655\n",
      "Epoch 216/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0887 - acc: 0.9710 - val_loss: 0.1353 - val_acc: 0.9670\n",
      "Epoch 217/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0913 - acc: 0.9704 - val_loss: 0.1362 - val_acc: 0.9671\n",
      "Epoch 218/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0907 - acc: 0.9710 - val_loss: 0.1361 - val_acc: 0.9666\n",
      "Epoch 219/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0900 - acc: 0.9706 - val_loss: 0.1364 - val_acc: 0.9674\n",
      "Epoch 220/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0926 - acc: 0.9703 - val_loss: 0.1367 - val_acc: 0.9664\n",
      "Epoch 221/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0916 - acc: 0.9702 - val_loss: 0.1402 - val_acc: 0.9663\n",
      "Epoch 222/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0890 - acc: 0.9707 - val_loss: 0.1409 - val_acc: 0.9660\n",
      "Epoch 223/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0884 - acc: 0.9708 - val_loss: 0.1371 - val_acc: 0.9673\n",
      "Epoch 224/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0910 - acc: 0.9708 - val_loss: 0.1366 - val_acc: 0.9676\n",
      "Epoch 225/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0875 - acc: 0.9714 - val_loss: 0.1381 - val_acc: 0.9666\n",
      "Epoch 226/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0875 - acc: 0.9714 - val_loss: 0.1379 - val_acc: 0.9674\n",
      "Epoch 227/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0879 - acc: 0.9715 - val_loss: 0.1410 - val_acc: 0.9666\n",
      "Epoch 228/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0870 - acc: 0.9712 - val_loss: 0.1368 - val_acc: 0.9669\n",
      "Epoch 229/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0881 - acc: 0.9713 - val_loss: 0.1396 - val_acc: 0.9670\n",
      "Epoch 230/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0900 - acc: 0.9711 - val_loss: 0.1375 - val_acc: 0.9664\n",
      "Epoch 231/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0901 - acc: 0.9700 - val_loss: 0.1373 - val_acc: 0.9675\n",
      "Epoch 232/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0894 - acc: 0.9718 - val_loss: 0.1387 - val_acc: 0.9659\n",
      "Epoch 233/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0900 - acc: 0.9699 - val_loss: 0.1376 - val_acc: 0.9657\n",
      "Epoch 234/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0883 - acc: 0.9716 - val_loss: 0.1364 - val_acc: 0.9666\n",
      "Epoch 235/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0896 - acc: 0.9712 - val_loss: 0.1389 - val_acc: 0.9665\n",
      "Epoch 236/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0895 - acc: 0.9704 - val_loss: 0.1379 - val_acc: 0.9668\n",
      "Epoch 237/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0877 - acc: 0.9712 - val_loss: 0.1381 - val_acc: 0.9663\n",
      "Epoch 238/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0888 - acc: 0.9709 - val_loss: 0.1397 - val_acc: 0.9661\n",
      "Epoch 239/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0882 - acc: 0.9714 - val_loss: 0.1420 - val_acc: 0.9657\n",
      "Epoch 240/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0906 - acc: 0.9709 - val_loss: 0.1368 - val_acc: 0.9672\n",
      "Epoch 241/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0873 - acc: 0.9718 - val_loss: 0.1394 - val_acc: 0.9662\n",
      "Epoch 242/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0887 - acc: 0.9711 - val_loss: 0.1399 - val_acc: 0.9659\n",
      "Epoch 243/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0845 - acc: 0.9720 - val_loss: 0.1397 - val_acc: 0.9668\n",
      "Epoch 244/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0902 - acc: 0.9703 - val_loss: 0.1381 - val_acc: 0.9676\n",
      "Epoch 245/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0853 - acc: 0.9719 - val_loss: 0.1415 - val_acc: 0.9660\n",
      "Epoch 246/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0858 - acc: 0.9713 - val_loss: 0.1415 - val_acc: 0.9679\n",
      "Epoch 247/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0874 - acc: 0.9715 - val_loss: 0.1434 - val_acc: 0.9658\n",
      "Epoch 248/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0874 - acc: 0.9716 - val_loss: 0.1391 - val_acc: 0.9673\n",
      "Epoch 249/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0885 - acc: 0.9715 - val_loss: 0.1455 - val_acc: 0.9654\n",
      "Epoch 250/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0860 - acc: 0.9722 - val_loss: 0.1426 - val_acc: 0.9657\n",
      "Epoch 251/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0883 - acc: 0.9710 - val_loss: 0.1368 - val_acc: 0.9679\n",
      "Epoch 252/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0857 - acc: 0.9726 - val_loss: 0.1480 - val_acc: 0.9647\n",
      "Epoch 253/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0895 - acc: 0.9704 - val_loss: 0.1411 - val_acc: 0.9657\n",
      "Epoch 254/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0855 - acc: 0.9719 - val_loss: 0.1432 - val_acc: 0.9665\n",
      "Epoch 255/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0893 - acc: 0.9707 - val_loss: 0.1413 - val_acc: 0.9678\n",
      "Epoch 256/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0865 - acc: 0.9717 - val_loss: 0.1438 - val_acc: 0.9662\n",
      "Epoch 257/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0864 - acc: 0.9720 - val_loss: 0.1439 - val_acc: 0.9668\n",
      "Epoch 258/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0870 - acc: 0.9717 - val_loss: 0.1465 - val_acc: 0.9648\n",
      "Epoch 259/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0868 - acc: 0.9709 - val_loss: 0.1441 - val_acc: 0.9666\n",
      "Epoch 260/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0862 - acc: 0.9716 - val_loss: 0.1460 - val_acc: 0.9663\n",
      "Epoch 261/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0862 - acc: 0.9718 - val_loss: 0.1485 - val_acc: 0.9650\n",
      "Epoch 262/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0861 - acc: 0.9723 - val_loss: 0.1440 - val_acc: 0.9673\n",
      "Epoch 263/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0857 - acc: 0.9721 - val_loss: 0.1436 - val_acc: 0.9676\n",
      "Epoch 264/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0871 - acc: 0.9717 - val_loss: 0.1435 - val_acc: 0.9672\n",
      "Epoch 265/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0900 - acc: 0.9711 - val_loss: 0.1430 - val_acc: 0.9663\n",
      "Epoch 266/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0840 - acc: 0.9730 - val_loss: 0.1437 - val_acc: 0.9662\n",
      "Epoch 267/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0839 - acc: 0.9726 - val_loss: 0.1447 - val_acc: 0.9674\n",
      "Epoch 268/500\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 0.0850 - acc: 0.9726 - val_loss: 0.1422 - val_acc: 0.9663\n",
      "Epoch 269/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0850 - acc: 0.9729 - val_loss: 0.1437 - val_acc: 0.9662\n",
      "Epoch 270/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0850 - acc: 0.9723 - val_loss: 0.1465 - val_acc: 0.9666\n",
      "Epoch 271/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0875 - acc: 0.9715 - val_loss: 0.1439 - val_acc: 0.9668\n",
      "Epoch 272/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0852 - acc: 0.9727 - val_loss: 0.1434 - val_acc: 0.9667\n",
      "Epoch 273/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0866 - acc: 0.9715 - val_loss: 0.1445 - val_acc: 0.9668\n",
      "Epoch 274/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0848 - acc: 0.9723 - val_loss: 0.1457 - val_acc: 0.9678\n",
      "Epoch 275/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0832 - acc: 0.9733 - val_loss: 0.1420 - val_acc: 0.9672\n",
      "Epoch 276/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0870 - acc: 0.9719 - val_loss: 0.1456 - val_acc: 0.9670\n",
      "Epoch 277/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0843 - acc: 0.9723 - val_loss: 0.1464 - val_acc: 0.9664\n",
      "Epoch 278/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0887 - acc: 0.9709 - val_loss: 0.1445 - val_acc: 0.9662\n",
      "Epoch 279/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0853 - acc: 0.9724 - val_loss: 0.1430 - val_acc: 0.9668\n",
      "Epoch 280/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0835 - acc: 0.9728 - val_loss: 0.1456 - val_acc: 0.9673\n",
      "Epoch 281/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0834 - acc: 0.9728 - val_loss: 0.1451 - val_acc: 0.9667\n",
      "Epoch 282/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0843 - acc: 0.9723 - val_loss: 0.1466 - val_acc: 0.9664\n",
      "Epoch 283/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0848 - acc: 0.9726 - val_loss: 0.1435 - val_acc: 0.9669\n",
      "Epoch 284/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0870 - acc: 0.9715 - val_loss: 0.1436 - val_acc: 0.9658\n",
      "Epoch 285/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0859 - acc: 0.9725 - val_loss: 0.1473 - val_acc: 0.9655\n",
      "Epoch 286/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0836 - acc: 0.9726 - val_loss: 0.1470 - val_acc: 0.9668\n",
      "Epoch 287/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0864 - acc: 0.9720 - val_loss: 0.1489 - val_acc: 0.9656\n",
      "Epoch 288/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0855 - acc: 0.9720 - val_loss: 0.1469 - val_acc: 0.9665\n",
      "Epoch 289/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0850 - acc: 0.9726 - val_loss: 0.1468 - val_acc: 0.9661\n",
      "Epoch 290/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0834 - acc: 0.9722 - val_loss: 0.1472 - val_acc: 0.9667\n",
      "Epoch 291/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0826 - acc: 0.9728 - val_loss: 0.1464 - val_acc: 0.9673\n",
      "Epoch 292/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0858 - acc: 0.9719 - val_loss: 0.1510 - val_acc: 0.9659\n",
      "Epoch 293/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0855 - acc: 0.9721 - val_loss: 0.1492 - val_acc: 0.9650\n",
      "Epoch 294/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0845 - acc: 0.9723 - val_loss: 0.1484 - val_acc: 0.9663\n",
      "Epoch 295/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0832 - acc: 0.9734 - val_loss: 0.1494 - val_acc: 0.9661\n",
      "Epoch 296/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0849 - acc: 0.9723 - val_loss: 0.1507 - val_acc: 0.9666\n",
      "Epoch 297/500\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 0.0844 - acc: 0.9733 - val_loss: 0.1508 - val_acc: 0.9655\n",
      "Epoch 298/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0838 - acc: 0.9729 - val_loss: 0.1473 - val_acc: 0.9660\n",
      "Epoch 299/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0837 - acc: 0.9731 - val_loss: 0.1528 - val_acc: 0.9657\n",
      "Epoch 300/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0835 - acc: 0.9725 - val_loss: 0.1501 - val_acc: 0.9663\n",
      "Epoch 301/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0847 - acc: 0.9722 - val_loss: 0.1476 - val_acc: 0.9666\n",
      "Epoch 302/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0856 - acc: 0.9723 - val_loss: 0.1491 - val_acc: 0.9657\n",
      "Epoch 303/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0842 - acc: 0.9723 - val_loss: 0.1462 - val_acc: 0.9673\n",
      "Epoch 304/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0828 - acc: 0.9730 - val_loss: 0.1507 - val_acc: 0.9654\n",
      "Epoch 305/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0842 - acc: 0.9726 - val_loss: 0.1517 - val_acc: 0.9655\n",
      "Epoch 306/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0844 - acc: 0.9729 - val_loss: 0.1491 - val_acc: 0.9668\n",
      "Epoch 307/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0830 - acc: 0.9735 - val_loss: 0.1483 - val_acc: 0.9659\n",
      "Epoch 308/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0824 - acc: 0.9733 - val_loss: 0.1505 - val_acc: 0.9667\n",
      "Epoch 309/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0835 - acc: 0.9726 - val_loss: 0.1466 - val_acc: 0.9658\n",
      "Epoch 310/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0828 - acc: 0.9729 - val_loss: 0.1539 - val_acc: 0.9647\n",
      "Epoch 311/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0862 - acc: 0.9712 - val_loss: 0.1508 - val_acc: 0.9663\n",
      "Epoch 312/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0838 - acc: 0.9727 - val_loss: 0.1557 - val_acc: 0.9646\n",
      "Epoch 313/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0836 - acc: 0.9728 - val_loss: 0.1511 - val_acc: 0.9670\n",
      "Epoch 314/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0836 - acc: 0.9725 - val_loss: 0.1529 - val_acc: 0.9654\n",
      "Epoch 315/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0851 - acc: 0.9725 - val_loss: 0.1542 - val_acc: 0.9650\n",
      "Epoch 316/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0834 - acc: 0.9729 - val_loss: 0.1487 - val_acc: 0.9672\n",
      "Epoch 317/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0831 - acc: 0.9732 - val_loss: 0.1540 - val_acc: 0.9648\n",
      "Epoch 318/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0835 - acc: 0.9729 - val_loss: 0.1548 - val_acc: 0.9658\n",
      "Epoch 319/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0838 - acc: 0.9727 - val_loss: 0.1542 - val_acc: 0.9652\n",
      "Epoch 320/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0829 - acc: 0.9730 - val_loss: 0.1584 - val_acc: 0.9646\n",
      "Epoch 321/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0841 - acc: 0.9726 - val_loss: 0.1507 - val_acc: 0.9663\n",
      "Epoch 322/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0792 - acc: 0.9739 - val_loss: 0.1514 - val_acc: 0.9668\n",
      "Epoch 323/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0841 - acc: 0.9719 - val_loss: 0.1557 - val_acc: 0.9657\n",
      "Epoch 324/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0818 - acc: 0.9727 - val_loss: 0.1514 - val_acc: 0.9665\n",
      "Epoch 325/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0820 - acc: 0.9726 - val_loss: 0.1558 - val_acc: 0.9650\n",
      "Epoch 326/500\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 0.0813 - acc: 0.9725 - val_loss: 0.1524 - val_acc: 0.9669\n",
      "Epoch 327/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0817 - acc: 0.9729 - val_loss: 0.1533 - val_acc: 0.9664\n",
      "Epoch 328/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0817 - acc: 0.9736 - val_loss: 0.1563 - val_acc: 0.9657\n",
      "Epoch 329/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0834 - acc: 0.9731 - val_loss: 0.1567 - val_acc: 0.9657\n",
      "Epoch 330/500\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 0.0812 - acc: 0.9733 - val_loss: 0.1575 - val_acc: 0.9646\n",
      "Epoch 331/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0825 - acc: 0.9728 - val_loss: 0.1547 - val_acc: 0.9663\n",
      "Epoch 332/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0816 - acc: 0.9734 - val_loss: 0.1500 - val_acc: 0.9683\n",
      "Epoch 333/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0816 - acc: 0.9732 - val_loss: 0.1523 - val_acc: 0.9675\n",
      "Epoch 334/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0848 - acc: 0.9724 - val_loss: 0.1541 - val_acc: 0.9657\n",
      "Epoch 335/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0818 - acc: 0.9738 - val_loss: 0.1569 - val_acc: 0.9666\n",
      "Epoch 336/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0842 - acc: 0.9722 - val_loss: 0.1583 - val_acc: 0.9666\n",
      "Epoch 337/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0788 - acc: 0.9740 - val_loss: 0.1574 - val_acc: 0.9664\n",
      "Epoch 338/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0803 - acc: 0.9731 - val_loss: 0.1563 - val_acc: 0.9655\n",
      "Epoch 339/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0780 - acc: 0.9742 - val_loss: 0.1534 - val_acc: 0.9654\n",
      "Epoch 340/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0810 - acc: 0.9736 - val_loss: 0.1571 - val_acc: 0.9656\n",
      "Epoch 341/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0813 - acc: 0.9735 - val_loss: 0.1570 - val_acc: 0.9663\n",
      "Epoch 342/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0821 - acc: 0.9733 - val_loss: 0.1577 - val_acc: 0.9660\n",
      "Epoch 343/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0806 - acc: 0.9735 - val_loss: 0.1545 - val_acc: 0.9663\n",
      "Epoch 344/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0838 - acc: 0.9724 - val_loss: 0.1530 - val_acc: 0.9668\n",
      "Epoch 345/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0800 - acc: 0.9743 - val_loss: 0.1556 - val_acc: 0.9653\n",
      "Epoch 346/500\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 0.0781 - acc: 0.9745 - val_loss: 0.1552 - val_acc: 0.9659\n",
      "Epoch 347/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0814 - acc: 0.9735 - val_loss: 0.1547 - val_acc: 0.9663\n",
      "Epoch 348/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0795 - acc: 0.9736 - val_loss: 0.1549 - val_acc: 0.9656\n",
      "Epoch 349/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0820 - acc: 0.9734 - val_loss: 0.1608 - val_acc: 0.9643\n",
      "Epoch 350/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0821 - acc: 0.9737 - val_loss: 0.1582 - val_acc: 0.9652\n",
      "Epoch 351/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0805 - acc: 0.9737 - val_loss: 0.1534 - val_acc: 0.9664\n",
      "Epoch 352/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0806 - acc: 0.9739 - val_loss: 0.1597 - val_acc: 0.9653\n",
      "Epoch 353/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0808 - acc: 0.9732 - val_loss: 0.1566 - val_acc: 0.9649\n",
      "Epoch 354/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0789 - acc: 0.9747 - val_loss: 0.1589 - val_acc: 0.9659\n",
      "Epoch 355/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0830 - acc: 0.9726 - val_loss: 0.1592 - val_acc: 0.9647\n",
      "Epoch 356/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0784 - acc: 0.9745 - val_loss: 0.1585 - val_acc: 0.9660\n",
      "Epoch 357/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0798 - acc: 0.9742 - val_loss: 0.1564 - val_acc: 0.9670\n",
      "Epoch 358/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0813 - acc: 0.9733 - val_loss: 0.1580 - val_acc: 0.9655\n",
      "Epoch 359/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0781 - acc: 0.9747 - val_loss: 0.1572 - val_acc: 0.9657\n",
      "Epoch 360/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0777 - acc: 0.9740 - val_loss: 0.1561 - val_acc: 0.9657\n",
      "Epoch 361/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0812 - acc: 0.9740 - val_loss: 0.1568 - val_acc: 0.9662\n",
      "Epoch 362/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0799 - acc: 0.9740 - val_loss: 0.1589 - val_acc: 0.9657\n",
      "Epoch 363/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0819 - acc: 0.9737 - val_loss: 0.1552 - val_acc: 0.9665\n",
      "Epoch 364/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0795 - acc: 0.9744 - val_loss: 0.1621 - val_acc: 0.9657\n",
      "Epoch 365/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0792 - acc: 0.9747 - val_loss: 0.1611 - val_acc: 0.9646\n",
      "Epoch 366/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0815 - acc: 0.9741 - val_loss: 0.1581 - val_acc: 0.9657\n",
      "Epoch 367/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0790 - acc: 0.9731 - val_loss: 0.1573 - val_acc: 0.9669\n",
      "Epoch 368/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0797 - acc: 0.9738 - val_loss: 0.1587 - val_acc: 0.9661\n",
      "Epoch 369/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0842 - acc: 0.9724 - val_loss: 0.1563 - val_acc: 0.9655\n",
      "Epoch 370/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0804 - acc: 0.9741 - val_loss: 0.1569 - val_acc: 0.9658\n",
      "Epoch 371/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0794 - acc: 0.9747 - val_loss: 0.1591 - val_acc: 0.9659\n",
      "Epoch 372/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0811 - acc: 0.9729 - val_loss: 0.1562 - val_acc: 0.9670\n",
      "Epoch 373/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0774 - acc: 0.9747 - val_loss: 0.1599 - val_acc: 0.9658\n",
      "Epoch 374/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0802 - acc: 0.9743 - val_loss: 0.1606 - val_acc: 0.9656\n",
      "Epoch 375/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0793 - acc: 0.9743 - val_loss: 0.1589 - val_acc: 0.9660\n",
      "Epoch 376/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0804 - acc: 0.9740 - val_loss: 0.1641 - val_acc: 0.9653\n",
      "Epoch 377/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0787 - acc: 0.9741 - val_loss: 0.1577 - val_acc: 0.9661\n",
      "Epoch 378/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0810 - acc: 0.9732 - val_loss: 0.1612 - val_acc: 0.9644\n",
      "Epoch 379/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0826 - acc: 0.9731 - val_loss: 0.1624 - val_acc: 0.9662\n",
      "Epoch 380/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0820 - acc: 0.9734 - val_loss: 0.1611 - val_acc: 0.9643\n",
      "Epoch 381/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0795 - acc: 0.9745 - val_loss: 0.1592 - val_acc: 0.9658\n",
      "Epoch 382/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0831 - acc: 0.9726 - val_loss: 0.1647 - val_acc: 0.9638\n",
      "Epoch 383/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0790 - acc: 0.9738 - val_loss: 0.1587 - val_acc: 0.9661\n",
      "Epoch 384/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0768 - acc: 0.9753 - val_loss: 0.1686 - val_acc: 0.9642\n",
      "Epoch 385/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0806 - acc: 0.9737 - val_loss: 0.1632 - val_acc: 0.9656\n",
      "Epoch 386/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0801 - acc: 0.9736 - val_loss: 0.1607 - val_acc: 0.9661\n",
      "Epoch 387/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0765 - acc: 0.9749 - val_loss: 0.1609 - val_acc: 0.9659\n",
      "Epoch 388/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0794 - acc: 0.9740 - val_loss: 0.1592 - val_acc: 0.9659\n",
      "Epoch 389/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0804 - acc: 0.9735 - val_loss: 0.1610 - val_acc: 0.9649\n",
      "Epoch 390/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0803 - acc: 0.9734 - val_loss: 0.1647 - val_acc: 0.9634\n",
      "Epoch 391/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0797 - acc: 0.9739 - val_loss: 0.1671 - val_acc: 0.9646\n",
      "Epoch 392/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0776 - acc: 0.9740 - val_loss: 0.1625 - val_acc: 0.9645\n",
      "Epoch 393/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0782 - acc: 0.9742 - val_loss: 0.1635 - val_acc: 0.9653\n",
      "Epoch 394/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0791 - acc: 0.9741 - val_loss: 0.1634 - val_acc: 0.9661\n",
      "Epoch 395/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0782 - acc: 0.9739 - val_loss: 0.1628 - val_acc: 0.9658\n",
      "Epoch 396/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0779 - acc: 0.9749 - val_loss: 0.1607 - val_acc: 0.9662\n",
      "Epoch 397/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0785 - acc: 0.9744 - val_loss: 0.1564 - val_acc: 0.9668\n",
      "Epoch 398/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0811 - acc: 0.9730 - val_loss: 0.1587 - val_acc: 0.9666\n",
      "Epoch 399/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0800 - acc: 0.9742 - val_loss: 0.1603 - val_acc: 0.9652\n",
      "Epoch 400/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0764 - acc: 0.9750 - val_loss: 0.1636 - val_acc: 0.9658\n",
      "Epoch 401/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0800 - acc: 0.9739 - val_loss: 0.1665 - val_acc: 0.9656\n",
      "Epoch 402/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0790 - acc: 0.9736 - val_loss: 0.1666 - val_acc: 0.9647\n",
      "Epoch 403/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0782 - acc: 0.9747 - val_loss: 0.1624 - val_acc: 0.9653\n",
      "Epoch 404/500\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 0.0794 - acc: 0.9733 - val_loss: 0.1628 - val_acc: 0.9668\n",
      "Epoch 405/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0778 - acc: 0.9744 - val_loss: 0.1616 - val_acc: 0.9660\n",
      "Epoch 406/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0797 - acc: 0.9738 - val_loss: 0.1630 - val_acc: 0.9652\n",
      "Epoch 407/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0771 - acc: 0.9745 - val_loss: 0.1635 - val_acc: 0.9657\n",
      "Epoch 408/500\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 0.0784 - acc: 0.9748 - val_loss: 0.1655 - val_acc: 0.9655\n",
      "Epoch 409/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0807 - acc: 0.9737 - val_loss: 0.1669 - val_acc: 0.9646\n",
      "Epoch 410/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0785 - acc: 0.9740 - val_loss: 0.1618 - val_acc: 0.9670\n",
      "Epoch 411/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0780 - acc: 0.9742 - val_loss: 0.1696 - val_acc: 0.9639\n",
      "Epoch 412/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0761 - acc: 0.9752 - val_loss: 0.1667 - val_acc: 0.9658\n",
      "Epoch 413/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0779 - acc: 0.9749 - val_loss: 0.1645 - val_acc: 0.9662\n",
      "Epoch 414/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0783 - acc: 0.9745 - val_loss: 0.1636 - val_acc: 0.9645\n",
      "Epoch 415/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0775 - acc: 0.9748 - val_loss: 0.1627 - val_acc: 0.9663\n",
      "Epoch 416/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0784 - acc: 0.9747 - val_loss: 0.1693 - val_acc: 0.9642\n",
      "Epoch 417/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0770 - acc: 0.9742 - val_loss: 0.1701 - val_acc: 0.9644\n",
      "Epoch 418/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0791 - acc: 0.9738 - val_loss: 0.1669 - val_acc: 0.9653\n",
      "Epoch 419/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0761 - acc: 0.9746 - val_loss: 0.1625 - val_acc: 0.9661\n",
      "Epoch 420/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0755 - acc: 0.9750 - val_loss: 0.1644 - val_acc: 0.9661\n",
      "Epoch 421/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0784 - acc: 0.9742 - val_loss: 0.1677 - val_acc: 0.9646\n",
      "Epoch 422/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0793 - acc: 0.9735 - val_loss: 0.1674 - val_acc: 0.9651\n",
      "Epoch 423/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0783 - acc: 0.9747 - val_loss: 0.1656 - val_acc: 0.9648\n",
      "Epoch 424/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0776 - acc: 0.9754 - val_loss: 0.1646 - val_acc: 0.9648\n",
      "Epoch 425/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0763 - acc: 0.9749 - val_loss: 0.1641 - val_acc: 0.9654\n",
      "Epoch 426/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0784 - acc: 0.9737 - val_loss: 0.1662 - val_acc: 0.9663\n",
      "Epoch 427/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0774 - acc: 0.9745 - val_loss: 0.1671 - val_acc: 0.9640\n",
      "Epoch 428/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0808 - acc: 0.9741 - val_loss: 0.1727 - val_acc: 0.9637\n",
      "Epoch 429/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0778 - acc: 0.9754 - val_loss: 0.1703 - val_acc: 0.9642\n",
      "Epoch 430/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0772 - acc: 0.9745 - val_loss: 0.1668 - val_acc: 0.9643\n",
      "Epoch 431/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0777 - acc: 0.9751 - val_loss: 0.1686 - val_acc: 0.9646\n",
      "Epoch 432/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0766 - acc: 0.9744 - val_loss: 0.1690 - val_acc: 0.9640\n",
      "Epoch 433/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0773 - acc: 0.9741 - val_loss: 0.1687 - val_acc: 0.9644\n",
      "Epoch 434/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0776 - acc: 0.9739 - val_loss: 0.1662 - val_acc: 0.9639\n",
      "Epoch 435/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0795 - acc: 0.9748 - val_loss: 0.1637 - val_acc: 0.9663\n",
      "Epoch 436/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0761 - acc: 0.9751 - val_loss: 0.1658 - val_acc: 0.9661\n",
      "Epoch 437/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0770 - acc: 0.9743 - val_loss: 0.1664 - val_acc: 0.9653\n",
      "Epoch 438/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0789 - acc: 0.9735 - val_loss: 0.1683 - val_acc: 0.9654\n",
      "Epoch 439/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0769 - acc: 0.9744 - val_loss: 0.1702 - val_acc: 0.9647\n",
      "Epoch 440/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0784 - acc: 0.9741 - val_loss: 0.1671 - val_acc: 0.9657\n",
      "Epoch 441/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0761 - acc: 0.9753 - val_loss: 0.1649 - val_acc: 0.9659\n",
      "Epoch 442/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0767 - acc: 0.9751 - val_loss: 0.1682 - val_acc: 0.9653\n",
      "Epoch 443/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0745 - acc: 0.9757 - val_loss: 0.1650 - val_acc: 0.9659\n",
      "Epoch 444/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0774 - acc: 0.9748 - val_loss: 0.1667 - val_acc: 0.9658\n",
      "Epoch 445/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0761 - acc: 0.9751 - val_loss: 0.1641 - val_acc: 0.9652\n",
      "Epoch 446/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0783 - acc: 0.9741 - val_loss: 0.1687 - val_acc: 0.9649\n",
      "Epoch 447/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0747 - acc: 0.9758 - val_loss: 0.1676 - val_acc: 0.9655\n",
      "Epoch 448/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0775 - acc: 0.9749 - val_loss: 0.1698 - val_acc: 0.9647\n",
      "Epoch 449/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0778 - acc: 0.9756 - val_loss: 0.1710 - val_acc: 0.9651\n",
      "Epoch 450/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0756 - acc: 0.9752 - val_loss: 0.1684 - val_acc: 0.9648\n",
      "Epoch 451/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0734 - acc: 0.9758 - val_loss: 0.1679 - val_acc: 0.9653\n",
      "Epoch 452/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0778 - acc: 0.9746 - val_loss: 0.1691 - val_acc: 0.9651\n",
      "Epoch 453/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0785 - acc: 0.9741 - val_loss: 0.1745 - val_acc: 0.9643\n",
      "Epoch 454/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0778 - acc: 0.9750 - val_loss: 0.1710 - val_acc: 0.9652\n",
      "Epoch 455/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0794 - acc: 0.9740 - val_loss: 0.1699 - val_acc: 0.9652\n",
      "Epoch 456/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0787 - acc: 0.9746 - val_loss: 0.1735 - val_acc: 0.9651\n",
      "Epoch 457/500\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 0.0758 - acc: 0.9749 - val_loss: 0.1675 - val_acc: 0.9667\n",
      "Epoch 458/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0794 - acc: 0.9738 - val_loss: 0.1712 - val_acc: 0.9656\n",
      "Epoch 459/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0775 - acc: 0.9745 - val_loss: 0.1693 - val_acc: 0.9651\n",
      "Epoch 460/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0743 - acc: 0.9753 - val_loss: 0.1682 - val_acc: 0.9655\n",
      "Epoch 461/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0767 - acc: 0.9748 - val_loss: 0.1717 - val_acc: 0.9662\n",
      "Epoch 462/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0753 - acc: 0.9758 - val_loss: 0.1702 - val_acc: 0.9663\n",
      "Epoch 463/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0758 - acc: 0.9747 - val_loss: 0.1711 - val_acc: 0.9666\n",
      "Epoch 464/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0757 - acc: 0.9750 - val_loss: 0.1710 - val_acc: 0.9654\n",
      "Epoch 465/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0759 - acc: 0.9751 - val_loss: 0.1731 - val_acc: 0.9657\n",
      "Epoch 466/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0769 - acc: 0.9745 - val_loss: 0.1711 - val_acc: 0.9645\n",
      "Epoch 467/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0755 - acc: 0.9749 - val_loss: 0.1704 - val_acc: 0.9652\n",
      "Epoch 468/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0758 - acc: 0.9748 - val_loss: 0.1664 - val_acc: 0.9667\n",
      "Epoch 469/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0728 - acc: 0.9760 - val_loss: 0.1687 - val_acc: 0.9660\n",
      "Epoch 470/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0785 - acc: 0.9743 - val_loss: 0.1751 - val_acc: 0.9634\n",
      "Epoch 471/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0755 - acc: 0.9749 - val_loss: 0.1717 - val_acc: 0.9651\n",
      "Epoch 472/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0758 - acc: 0.9754 - val_loss: 0.1698 - val_acc: 0.9663\n",
      "Epoch 473/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0763 - acc: 0.9751 - val_loss: 0.1723 - val_acc: 0.9654\n",
      "Epoch 474/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0796 - acc: 0.9747 - val_loss: 0.1735 - val_acc: 0.9646\n",
      "Epoch 475/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0751 - acc: 0.9752 - val_loss: 0.1750 - val_acc: 0.9642\n",
      "Epoch 476/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0774 - acc: 0.9742 - val_loss: 0.1710 - val_acc: 0.9658\n",
      "Epoch 477/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0761 - acc: 0.9749 - val_loss: 0.1753 - val_acc: 0.9661\n",
      "Epoch 478/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0778 - acc: 0.9746 - val_loss: 0.1788 - val_acc: 0.9647\n",
      "Epoch 479/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0763 - acc: 0.9750 - val_loss: 0.1736 - val_acc: 0.9658\n",
      "Epoch 480/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0766 - acc: 0.9752 - val_loss: 0.1728 - val_acc: 0.9655\n",
      "Epoch 481/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0759 - acc: 0.9751 - val_loss: 0.1748 - val_acc: 0.9642\n",
      "Epoch 482/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0753 - acc: 0.9744 - val_loss: 0.1737 - val_acc: 0.9652\n",
      "Epoch 483/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0736 - acc: 0.9762 - val_loss: 0.1725 - val_acc: 0.9656\n",
      "Epoch 484/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0785 - acc: 0.9750 - val_loss: 0.1722 - val_acc: 0.9657\n",
      "Epoch 485/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0751 - acc: 0.9748 - val_loss: 0.1734 - val_acc: 0.9666\n",
      "Epoch 486/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0764 - acc: 0.9751 - val_loss: 0.1745 - val_acc: 0.9651\n",
      "Epoch 487/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0765 - acc: 0.9749 - val_loss: 0.1765 - val_acc: 0.9647\n",
      "Epoch 488/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0761 - acc: 0.9757 - val_loss: 0.1766 - val_acc: 0.9648\n",
      "Epoch 489/500\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 0.0754 - acc: 0.9761 - val_loss: 0.1704 - val_acc: 0.9659\n",
      "Epoch 490/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0769 - acc: 0.9748 - val_loss: 0.1778 - val_acc: 0.9655\n",
      "Epoch 491/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0753 - acc: 0.9745 - val_loss: 0.1721 - val_acc: 0.9650\n",
      "Epoch 492/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0754 - acc: 0.9754 - val_loss: 0.1745 - val_acc: 0.9655\n",
      "Epoch 493/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0759 - acc: 0.9750 - val_loss: 0.1796 - val_acc: 0.9641\n",
      "Epoch 494/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0730 - acc: 0.9766 - val_loss: 0.1739 - val_acc: 0.9647\n",
      "Epoch 495/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0758 - acc: 0.9752 - val_loss: 0.1765 - val_acc: 0.9650\n",
      "Epoch 496/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0770 - acc: 0.9749 - val_loss: 0.1777 - val_acc: 0.9646\n",
      "Epoch 497/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0749 - acc: 0.9751 - val_loss: 0.1771 - val_acc: 0.9648\n",
      "Epoch 498/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0743 - acc: 0.9754 - val_loss: 0.1713 - val_acc: 0.9659\n",
      "Epoch 499/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0742 - acc: 0.9758 - val_loss: 0.1740 - val_acc: 0.9653\n",
      "Epoch 500/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0776 - acc: 0.9748 - val_loss: 0.1770 - val_acc: 0.9643\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3c447b7a20>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_dir = 'logs/pure_student/'\n",
    "\n",
    "logging = TensorBoard(log_dir=log_dir)\n",
    "checkpoint = ModelCheckpoint(log_dir + 'ep{epoch:03d}-acc{acc:.4f}-val_acc{val_acc:.4f}-loss{loss:.4f}-val_loss{val_loss:.4f}.h5',\n",
    "        monitor='val_acc', save_weights_only=True, save_best_only=True, period=3)\n",
    "\n",
    "history = student.fit(X_train, Y_train,\n",
    "          batch_size=256,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, Y_test),\n",
    "           callbacks=[logging,checkpoint] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_acc = history.history['acc'][-1]\n",
    "last_val_acc = history.history['val_acc'][-1]\n",
    "last_loss = history.history['loss'][-1]\n",
    "last_val_loss = history.history['val_loss'][-1]\n",
    "\n",
    "hist = \"acc{0:.4f}-val_acc{0:.4f}-loss{0:.4f}-val_loss{0:.4f}\".format(last_acc,last_val_acc,last_loss,last_val_loss)\n",
    "student.save_weights(log_dir + \"last_\"+ hist + \".h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y90wE1T_rIRL"
   },
   "source": [
    "# Student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SoRFn3vGsi8A"
   },
   "outputs": [],
   "source": [
    "studentX = Sequential()\n",
    "studentX.add(Flatten(input_shape=input_shape))\n",
    "studentX.add(Dense(32, activation='relu'))\n",
    "studentX.add(Dropout(0.2))\n",
    "studentX.add(Dense(nb_classes))\n",
    "studentX.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "\n",
    "#sgd = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "studentX.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy']\n",
    "                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "colab_type": "code",
    "id": "x6gRyWwRs4_3",
    "outputId": "49c85bf4-f895-48fd-9b16-36ae30164c98",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.7026 - acc: 0.7954 - val_loss: 0.3142 - val_acc: 0.9123\n",
      "Epoch 2/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.3696 - acc: 0.8931 - val_loss: 0.2450 - val_acc: 0.9295\n",
      "Epoch 3/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.3122 - acc: 0.9092 - val_loss: 0.2199 - val_acc: 0.9384\n",
      "Epoch 4/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.2807 - acc: 0.9169 - val_loss: 0.1934 - val_acc: 0.9438\n",
      "Epoch 5/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.2619 - acc: 0.9229 - val_loss: 0.1797 - val_acc: 0.9475\n",
      "Epoch 6/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.2452 - acc: 0.9259 - val_loss: 0.1701 - val_acc: 0.9502\n",
      "Epoch 7/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.2359 - acc: 0.9306 - val_loss: 0.1582 - val_acc: 0.9530\n",
      "Epoch 8/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.2270 - acc: 0.9328 - val_loss: 0.1549 - val_acc: 0.9538\n",
      "Epoch 9/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.2194 - acc: 0.9349 - val_loss: 0.1480 - val_acc: 0.9554\n",
      "Epoch 10/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.2139 - acc: 0.9360 - val_loss: 0.1445 - val_acc: 0.9583\n",
      "Epoch 11/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.2051 - acc: 0.9385 - val_loss: 0.1428 - val_acc: 0.9583\n",
      "Epoch 12/500\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 0.2022 - acc: 0.9394 - val_loss: 0.1354 - val_acc: 0.9591\n",
      "Epoch 13/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1962 - acc: 0.9416 - val_loss: 0.1349 - val_acc: 0.9606\n",
      "Epoch 14/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1936 - acc: 0.9427 - val_loss: 0.1364 - val_acc: 0.9610\n",
      "Epoch 15/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1906 - acc: 0.9439 - val_loss: 0.1297 - val_acc: 0.9615\n",
      "Epoch 16/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1851 - acc: 0.9439 - val_loss: 0.1241 - val_acc: 0.9641\n",
      "Epoch 17/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1825 - acc: 0.9446 - val_loss: 0.1247 - val_acc: 0.9633\n",
      "Epoch 18/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1811 - acc: 0.9453 - val_loss: 0.1256 - val_acc: 0.9637\n",
      "Epoch 19/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1785 - acc: 0.9462 - val_loss: 0.1248 - val_acc: 0.9626\n",
      "Epoch 20/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1733 - acc: 0.9467 - val_loss: 0.1218 - val_acc: 0.9633\n",
      "Epoch 21/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1732 - acc: 0.9470 - val_loss: 0.1202 - val_acc: 0.9654\n",
      "Epoch 22/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1713 - acc: 0.9479 - val_loss: 0.1165 - val_acc: 0.9647\n",
      "Epoch 23/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1701 - acc: 0.9476 - val_loss: 0.1187 - val_acc: 0.9648\n",
      "Epoch 24/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1684 - acc: 0.9484 - val_loss: 0.1180 - val_acc: 0.9664\n",
      "Epoch 25/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1661 - acc: 0.9485 - val_loss: 0.1166 - val_acc: 0.9655\n",
      "Epoch 26/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1643 - acc: 0.9500 - val_loss: 0.1166 - val_acc: 0.9658\n",
      "Epoch 27/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1621 - acc: 0.9495 - val_loss: 0.1193 - val_acc: 0.9657\n",
      "Epoch 28/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1597 - acc: 0.9500 - val_loss: 0.1178 - val_acc: 0.9654\n",
      "Epoch 29/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1610 - acc: 0.9508 - val_loss: 0.1158 - val_acc: 0.9671\n",
      "Epoch 30/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1568 - acc: 0.9527 - val_loss: 0.1164 - val_acc: 0.9659\n",
      "Epoch 31/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1588 - acc: 0.9499 - val_loss: 0.1157 - val_acc: 0.9667\n",
      "Epoch 32/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1560 - acc: 0.9527 - val_loss: 0.1149 - val_acc: 0.9663\n",
      "Epoch 33/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1543 - acc: 0.9516 - val_loss: 0.1161 - val_acc: 0.9654\n",
      "Epoch 34/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1526 - acc: 0.9513 - val_loss: 0.1144 - val_acc: 0.9660\n",
      "Epoch 35/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1524 - acc: 0.9531 - val_loss: 0.1146 - val_acc: 0.9676\n",
      "Epoch 36/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1505 - acc: 0.9526 - val_loss: 0.1165 - val_acc: 0.9660\n",
      "Epoch 37/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1500 - acc: 0.9536 - val_loss: 0.1136 - val_acc: 0.9664\n",
      "Epoch 38/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1479 - acc: 0.9539 - val_loss: 0.1141 - val_acc: 0.9670\n",
      "Epoch 39/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1507 - acc: 0.9525 - val_loss: 0.1131 - val_acc: 0.9673\n",
      "Epoch 40/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1464 - acc: 0.9541 - val_loss: 0.1142 - val_acc: 0.9685\n",
      "Epoch 41/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1463 - acc: 0.9539 - val_loss: 0.1160 - val_acc: 0.9662\n",
      "Epoch 42/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1463 - acc: 0.9542 - val_loss: 0.1135 - val_acc: 0.9677\n",
      "Epoch 43/500\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 0.1424 - acc: 0.9555 - val_loss: 0.1121 - val_acc: 0.9682\n",
      "Epoch 44/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1442 - acc: 0.9545 - val_loss: 0.1125 - val_acc: 0.9659\n",
      "Epoch 45/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1420 - acc: 0.9554 - val_loss: 0.1127 - val_acc: 0.9666\n",
      "Epoch 46/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1402 - acc: 0.9561 - val_loss: 0.1115 - val_acc: 0.9684\n",
      "Epoch 47/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1406 - acc: 0.9549 - val_loss: 0.1132 - val_acc: 0.9682\n",
      "Epoch 48/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1406 - acc: 0.9570 - val_loss: 0.1124 - val_acc: 0.9668\n",
      "Epoch 49/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1370 - acc: 0.9568 - val_loss: 0.1135 - val_acc: 0.9665\n",
      "Epoch 50/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1394 - acc: 0.9570 - val_loss: 0.1116 - val_acc: 0.9688\n",
      "Epoch 51/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1383 - acc: 0.9564 - val_loss: 0.1130 - val_acc: 0.9676\n",
      "Epoch 52/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1371 - acc: 0.9561 - val_loss: 0.1110 - val_acc: 0.9682\n",
      "Epoch 53/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1356 - acc: 0.9567 - val_loss: 0.1144 - val_acc: 0.9663\n",
      "Epoch 54/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1360 - acc: 0.9567 - val_loss: 0.1109 - val_acc: 0.9683\n",
      "Epoch 55/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1358 - acc: 0.9569 - val_loss: 0.1148 - val_acc: 0.9676\n",
      "Epoch 56/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1318 - acc: 0.9587 - val_loss: 0.1115 - val_acc: 0.9681\n",
      "Epoch 57/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1318 - acc: 0.9575 - val_loss: 0.1153 - val_acc: 0.9668\n",
      "Epoch 58/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1327 - acc: 0.9579 - val_loss: 0.1125 - val_acc: 0.9671\n",
      "Epoch 59/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1363 - acc: 0.9566 - val_loss: 0.1130 - val_acc: 0.9671\n",
      "Epoch 60/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1327 - acc: 0.9576 - val_loss: 0.1151 - val_acc: 0.9684\n",
      "Epoch 61/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1310 - acc: 0.9587 - val_loss: 0.1134 - val_acc: 0.9671\n",
      "Epoch 62/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1297 - acc: 0.9586 - val_loss: 0.1124 - val_acc: 0.9677\n",
      "Epoch 63/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1310 - acc: 0.9585 - val_loss: 0.1139 - val_acc: 0.9681\n",
      "Epoch 64/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1292 - acc: 0.9592 - val_loss: 0.1142 - val_acc: 0.9660\n",
      "Epoch 65/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1294 - acc: 0.9587 - val_loss: 0.1157 - val_acc: 0.9674\n",
      "Epoch 66/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1285 - acc: 0.9599 - val_loss: 0.1107 - val_acc: 0.9696\n",
      "Epoch 67/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1292 - acc: 0.9588 - val_loss: 0.1154 - val_acc: 0.9683\n",
      "Epoch 68/500\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 0.1267 - acc: 0.9588 - val_loss: 0.1125 - val_acc: 0.9693\n",
      "Epoch 69/500\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 0.1290 - acc: 0.9594 - val_loss: 0.1153 - val_acc: 0.9677\n",
      "Epoch 70/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1254 - acc: 0.9601 - val_loss: 0.1149 - val_acc: 0.9677\n",
      "Epoch 71/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1267 - acc: 0.9595 - val_loss: 0.1168 - val_acc: 0.9677\n",
      "Epoch 72/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1228 - acc: 0.9606 - val_loss: 0.1157 - val_acc: 0.9680\n",
      "Epoch 73/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1261 - acc: 0.9598 - val_loss: 0.1147 - val_acc: 0.9682\n",
      "Epoch 74/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1241 - acc: 0.9612 - val_loss: 0.1117 - val_acc: 0.9669\n",
      "Epoch 75/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1240 - acc: 0.9609 - val_loss: 0.1153 - val_acc: 0.9677\n",
      "Epoch 76/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1258 - acc: 0.9596 - val_loss: 0.1152 - val_acc: 0.9673\n",
      "Epoch 77/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1235 - acc: 0.9608 - val_loss: 0.1154 - val_acc: 0.9673\n",
      "Epoch 78/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1228 - acc: 0.9598 - val_loss: 0.1156 - val_acc: 0.9681\n",
      "Epoch 79/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1236 - acc: 0.9604 - val_loss: 0.1151 - val_acc: 0.9689\n",
      "Epoch 80/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1228 - acc: 0.9604 - val_loss: 0.1143 - val_acc: 0.9682\n",
      "Epoch 81/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1225 - acc: 0.9611 - val_loss: 0.1166 - val_acc: 0.9666\n",
      "Epoch 82/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1195 - acc: 0.9614 - val_loss: 0.1146 - val_acc: 0.9670\n",
      "Epoch 83/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1216 - acc: 0.9607 - val_loss: 0.1140 - val_acc: 0.9677\n",
      "Epoch 84/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1209 - acc: 0.9619 - val_loss: 0.1161 - val_acc: 0.9682\n",
      "Epoch 85/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1219 - acc: 0.9600 - val_loss: 0.1162 - val_acc: 0.9691\n",
      "Epoch 86/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1203 - acc: 0.9622 - val_loss: 0.1164 - val_acc: 0.9684\n",
      "Epoch 87/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1186 - acc: 0.9623 - val_loss: 0.1169 - val_acc: 0.9670\n",
      "Epoch 88/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1195 - acc: 0.9611 - val_loss: 0.1150 - val_acc: 0.9684\n",
      "Epoch 89/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1194 - acc: 0.9617 - val_loss: 0.1171 - val_acc: 0.9678\n",
      "Epoch 90/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1205 - acc: 0.9614 - val_loss: 0.1169 - val_acc: 0.9675\n",
      "Epoch 91/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1181 - acc: 0.9623 - val_loss: 0.1165 - val_acc: 0.9682\n",
      "Epoch 92/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1190 - acc: 0.9622 - val_loss: 0.1148 - val_acc: 0.9682\n",
      "Epoch 93/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1209 - acc: 0.9608 - val_loss: 0.1155 - val_acc: 0.9682\n",
      "Epoch 94/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1148 - acc: 0.9636 - val_loss: 0.1141 - val_acc: 0.9686\n",
      "Epoch 95/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1177 - acc: 0.9618 - val_loss: 0.1168 - val_acc: 0.9680\n",
      "Epoch 96/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1157 - acc: 0.9632 - val_loss: 0.1165 - val_acc: 0.9680\n",
      "Epoch 97/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1173 - acc: 0.9622 - val_loss: 0.1208 - val_acc: 0.9663\n",
      "Epoch 98/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1177 - acc: 0.9624 - val_loss: 0.1189 - val_acc: 0.9675\n",
      "Epoch 99/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1177 - acc: 0.9626 - val_loss: 0.1189 - val_acc: 0.9676\n",
      "Epoch 100/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1140 - acc: 0.9634 - val_loss: 0.1188 - val_acc: 0.9682\n",
      "Epoch 101/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1169 - acc: 0.9626 - val_loss: 0.1174 - val_acc: 0.9679\n",
      "Epoch 102/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1161 - acc: 0.9634 - val_loss: 0.1173 - val_acc: 0.9681\n",
      "Epoch 103/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1125 - acc: 0.9633 - val_loss: 0.1167 - val_acc: 0.9685\n",
      "Epoch 104/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1138 - acc: 0.9629 - val_loss: 0.1202 - val_acc: 0.9672\n",
      "Epoch 105/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1145 - acc: 0.9631 - val_loss: 0.1195 - val_acc: 0.9668\n",
      "Epoch 106/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1143 - acc: 0.9636 - val_loss: 0.1193 - val_acc: 0.9676\n",
      "Epoch 107/500\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 0.1153 - acc: 0.9632 - val_loss: 0.1198 - val_acc: 0.9676\n",
      "Epoch 108/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1124 - acc: 0.9635 - val_loss: 0.1198 - val_acc: 0.9674\n",
      "Epoch 109/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1110 - acc: 0.9646 - val_loss: 0.1175 - val_acc: 0.9694\n",
      "Epoch 110/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1151 - acc: 0.9633 - val_loss: 0.1179 - val_acc: 0.9672\n",
      "Epoch 111/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1132 - acc: 0.9631 - val_loss: 0.1181 - val_acc: 0.9675\n",
      "Epoch 112/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1143 - acc: 0.9628 - val_loss: 0.1231 - val_acc: 0.9662\n",
      "Epoch 113/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1130 - acc: 0.9634 - val_loss: 0.1177 - val_acc: 0.9686\n",
      "Epoch 114/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1140 - acc: 0.9639 - val_loss: 0.1196 - val_acc: 0.9682\n",
      "Epoch 115/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1136 - acc: 0.9641 - val_loss: 0.1220 - val_acc: 0.9674\n",
      "Epoch 116/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1105 - acc: 0.9645 - val_loss: 0.1201 - val_acc: 0.9686\n",
      "Epoch 117/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1123 - acc: 0.9646 - val_loss: 0.1211 - val_acc: 0.9686\n",
      "Epoch 118/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1127 - acc: 0.9635 - val_loss: 0.1186 - val_acc: 0.9683\n",
      "Epoch 119/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1129 - acc: 0.9640 - val_loss: 0.1201 - val_acc: 0.9681\n",
      "Epoch 120/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1082 - acc: 0.9648 - val_loss: 0.1229 - val_acc: 0.9682\n",
      "Epoch 121/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1119 - acc: 0.9638 - val_loss: 0.1199 - val_acc: 0.9686\n",
      "Epoch 122/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1106 - acc: 0.9648 - val_loss: 0.1219 - val_acc: 0.9674\n",
      "Epoch 123/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1100 - acc: 0.9646 - val_loss: 0.1205 - val_acc: 0.9673\n",
      "Epoch 124/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1086 - acc: 0.9646 - val_loss: 0.1208 - val_acc: 0.9685\n",
      "Epoch 125/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1092 - acc: 0.9645 - val_loss: 0.1220 - val_acc: 0.9673\n",
      "Epoch 126/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1090 - acc: 0.9642 - val_loss: 0.1213 - val_acc: 0.9684\n",
      "Epoch 127/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1094 - acc: 0.9646 - val_loss: 0.1213 - val_acc: 0.9684\n",
      "Epoch 128/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1119 - acc: 0.9636 - val_loss: 0.1221 - val_acc: 0.9679\n",
      "Epoch 129/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1103 - acc: 0.9638 - val_loss: 0.1204 - val_acc: 0.9677\n",
      "Epoch 130/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1086 - acc: 0.9649 - val_loss: 0.1225 - val_acc: 0.9676\n",
      "Epoch 131/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1070 - acc: 0.9653 - val_loss: 0.1217 - val_acc: 0.9677\n",
      "Epoch 132/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1088 - acc: 0.9651 - val_loss: 0.1202 - val_acc: 0.9686\n",
      "Epoch 133/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1086 - acc: 0.9638 - val_loss: 0.1238 - val_acc: 0.9667\n",
      "Epoch 134/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1090 - acc: 0.9639 - val_loss: 0.1201 - val_acc: 0.9683\n",
      "Epoch 135/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1067 - acc: 0.9660 - val_loss: 0.1215 - val_acc: 0.9671\n",
      "Epoch 136/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1069 - acc: 0.9646 - val_loss: 0.1195 - val_acc: 0.9683\n",
      "Epoch 137/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1092 - acc: 0.9642 - val_loss: 0.1247 - val_acc: 0.9681\n",
      "Epoch 138/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1074 - acc: 0.9652 - val_loss: 0.1276 - val_acc: 0.9675\n",
      "Epoch 139/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1071 - acc: 0.9656 - val_loss: 0.1224 - val_acc: 0.9679\n",
      "Epoch 140/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1096 - acc: 0.9641 - val_loss: 0.1258 - val_acc: 0.9679\n",
      "Epoch 141/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1056 - acc: 0.9653 - val_loss: 0.1221 - val_acc: 0.9670\n",
      "Epoch 142/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1109 - acc: 0.9648 - val_loss: 0.1243 - val_acc: 0.9681\n",
      "Epoch 143/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1082 - acc: 0.9644 - val_loss: 0.1264 - val_acc: 0.9661\n",
      "Epoch 144/500\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 0.1058 - acc: 0.9657 - val_loss: 0.1227 - val_acc: 0.9675\n",
      "Epoch 145/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1087 - acc: 0.9652 - val_loss: 0.1243 - val_acc: 0.9669\n",
      "Epoch 146/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1055 - acc: 0.9658 - val_loss: 0.1264 - val_acc: 0.9670\n",
      "Epoch 147/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1070 - acc: 0.9657 - val_loss: 0.1260 - val_acc: 0.9674\n",
      "Epoch 148/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1036 - acc: 0.9661 - val_loss: 0.1249 - val_acc: 0.9681\n",
      "Epoch 149/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1062 - acc: 0.9652 - val_loss: 0.1259 - val_acc: 0.9664\n",
      "Epoch 150/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1046 - acc: 0.9666 - val_loss: 0.1294 - val_acc: 0.9681\n",
      "Epoch 151/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1061 - acc: 0.9657 - val_loss: 0.1249 - val_acc: 0.9677\n",
      "Epoch 152/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1056 - acc: 0.9651 - val_loss: 0.1252 - val_acc: 0.9684\n",
      "Epoch 153/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1037 - acc: 0.9658 - val_loss: 0.1258 - val_acc: 0.9683\n",
      "Epoch 154/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1062 - acc: 0.9651 - val_loss: 0.1265 - val_acc: 0.9672\n",
      "Epoch 155/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1040 - acc: 0.9660 - val_loss: 0.1255 - val_acc: 0.9669\n",
      "Epoch 156/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1042 - acc: 0.9663 - val_loss: 0.1246 - val_acc: 0.9672\n",
      "Epoch 157/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1051 - acc: 0.9656 - val_loss: 0.1284 - val_acc: 0.9670\n",
      "Epoch 158/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1047 - acc: 0.9662 - val_loss: 0.1263 - val_acc: 0.9673\n",
      "Epoch 159/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1024 - acc: 0.9673 - val_loss: 0.1284 - val_acc: 0.9686\n",
      "Epoch 160/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1050 - acc: 0.9656 - val_loss: 0.1264 - val_acc: 0.9672\n",
      "Epoch 161/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1047 - acc: 0.9654 - val_loss: 0.1274 - val_acc: 0.9666\n",
      "Epoch 162/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1039 - acc: 0.9664 - val_loss: 0.1273 - val_acc: 0.9671\n",
      "Epoch 163/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1062 - acc: 0.9652 - val_loss: 0.1282 - val_acc: 0.9663\n",
      "Epoch 164/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1011 - acc: 0.9670 - val_loss: 0.1284 - val_acc: 0.9671\n",
      "Epoch 165/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1018 - acc: 0.9676 - val_loss: 0.1291 - val_acc: 0.9680\n",
      "Epoch 166/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1040 - acc: 0.9654 - val_loss: 0.1277 - val_acc: 0.9669\n",
      "Epoch 167/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1033 - acc: 0.9673 - val_loss: 0.1295 - val_acc: 0.9669\n",
      "Epoch 168/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1001 - acc: 0.9673 - val_loss: 0.1281 - val_acc: 0.9674\n",
      "Epoch 169/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1016 - acc: 0.9661 - val_loss: 0.1300 - val_acc: 0.9659\n",
      "Epoch 170/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1025 - acc: 0.9666 - val_loss: 0.1290 - val_acc: 0.9668\n",
      "Epoch 171/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1025 - acc: 0.9660 - val_loss: 0.1268 - val_acc: 0.9684\n",
      "Epoch 172/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1020 - acc: 0.9663 - val_loss: 0.1282 - val_acc: 0.9676\n",
      "Epoch 173/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1013 - acc: 0.9669 - val_loss: 0.1282 - val_acc: 0.9683\n",
      "Epoch 174/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1021 - acc: 0.9663 - val_loss: 0.1288 - val_acc: 0.9669\n",
      "Epoch 175/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1010 - acc: 0.9670 - val_loss: 0.1323 - val_acc: 0.9671\n",
      "Epoch 176/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1020 - acc: 0.9666 - val_loss: 0.1351 - val_acc: 0.9660\n",
      "Epoch 177/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1015 - acc: 0.9664 - val_loss: 0.1294 - val_acc: 0.9675\n",
      "Epoch 178/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1024 - acc: 0.9663 - val_loss: 0.1309 - val_acc: 0.9680\n",
      "Epoch 179/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1008 - acc: 0.9678 - val_loss: 0.1297 - val_acc: 0.9665\n",
      "Epoch 180/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0998 - acc: 0.9675 - val_loss: 0.1294 - val_acc: 0.9672\n",
      "Epoch 181/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1014 - acc: 0.9662 - val_loss: 0.1320 - val_acc: 0.9669\n",
      "Epoch 182/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1026 - acc: 0.9664 - val_loss: 0.1318 - val_acc: 0.9667\n",
      "Epoch 183/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1010 - acc: 0.9666 - val_loss: 0.1320 - val_acc: 0.9663\n",
      "Epoch 184/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1016 - acc: 0.9663 - val_loss: 0.1334 - val_acc: 0.9686\n",
      "Epoch 185/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1025 - acc: 0.9669 - val_loss: 0.1315 - val_acc: 0.9674\n",
      "Epoch 186/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1000 - acc: 0.9677 - val_loss: 0.1306 - val_acc: 0.9671\n",
      "Epoch 187/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.1010 - acc: 0.9675 - val_loss: 0.1314 - val_acc: 0.9676\n",
      "Epoch 188/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0997 - acc: 0.9676 - val_loss: 0.1319 - val_acc: 0.9664\n",
      "Epoch 189/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1043 - acc: 0.9658 - val_loss: 0.1319 - val_acc: 0.9671\n",
      "Epoch 190/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1011 - acc: 0.9663 - val_loss: 0.1349 - val_acc: 0.9669\n",
      "Epoch 191/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0974 - acc: 0.9684 - val_loss: 0.1318 - val_acc: 0.9682\n",
      "Epoch 192/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0980 - acc: 0.9685 - val_loss: 0.1336 - val_acc: 0.9670\n",
      "Epoch 193/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0988 - acc: 0.9681 - val_loss: 0.1338 - val_acc: 0.9661\n",
      "Epoch 194/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0992 - acc: 0.9677 - val_loss: 0.1356 - val_acc: 0.9657\n",
      "Epoch 195/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0994 - acc: 0.9672 - val_loss: 0.1316 - val_acc: 0.9664\n",
      "Epoch 196/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1010 - acc: 0.9669 - val_loss: 0.1311 - val_acc: 0.9683\n",
      "Epoch 197/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0998 - acc: 0.9670 - val_loss: 0.1338 - val_acc: 0.9675\n",
      "Epoch 198/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0996 - acc: 0.9675 - val_loss: 0.1331 - val_acc: 0.9677\n",
      "Epoch 199/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0978 - acc: 0.9679 - val_loss: 0.1350 - val_acc: 0.9673\n",
      "Epoch 200/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0965 - acc: 0.9690 - val_loss: 0.1328 - val_acc: 0.9672\n",
      "Epoch 201/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0975 - acc: 0.9683 - val_loss: 0.1354 - val_acc: 0.9672\n",
      "Epoch 202/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0979 - acc: 0.9680 - val_loss: 0.1333 - val_acc: 0.9677\n",
      "Epoch 203/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0985 - acc: 0.9676 - val_loss: 0.1376 - val_acc: 0.9662\n",
      "Epoch 204/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1016 - acc: 0.9669 - val_loss: 0.1339 - val_acc: 0.9664\n",
      "Epoch 205/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0999 - acc: 0.9679 - val_loss: 0.1343 - val_acc: 0.9674\n",
      "Epoch 206/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0972 - acc: 0.9679 - val_loss: 0.1372 - val_acc: 0.9679\n",
      "Epoch 207/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1004 - acc: 0.9669 - val_loss: 0.1333 - val_acc: 0.9679\n",
      "Epoch 208/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0975 - acc: 0.9689 - val_loss: 0.1323 - val_acc: 0.9681\n",
      "Epoch 209/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1005 - acc: 0.9668 - val_loss: 0.1330 - val_acc: 0.9680\n",
      "Epoch 210/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0990 - acc: 0.9672 - val_loss: 0.1346 - val_acc: 0.9678\n",
      "Epoch 211/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0969 - acc: 0.9685 - val_loss: 0.1365 - val_acc: 0.9669\n",
      "Epoch 212/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0952 - acc: 0.9686 - val_loss: 0.1334 - val_acc: 0.9681\n",
      "Epoch 213/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0976 - acc: 0.9678 - val_loss: 0.1348 - val_acc: 0.9672\n",
      "Epoch 214/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0975 - acc: 0.9675 - val_loss: 0.1348 - val_acc: 0.9664\n",
      "Epoch 215/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0975 - acc: 0.9679 - val_loss: 0.1324 - val_acc: 0.9682\n",
      "Epoch 216/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0976 - acc: 0.9683 - val_loss: 0.1339 - val_acc: 0.9672\n",
      "Epoch 217/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0953 - acc: 0.9691 - val_loss: 0.1355 - val_acc: 0.9688\n",
      "Epoch 218/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0968 - acc: 0.9680 - val_loss: 0.1386 - val_acc: 0.9689\n",
      "Epoch 219/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0958 - acc: 0.9678 - val_loss: 0.1364 - val_acc: 0.9673\n",
      "Epoch 220/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0963 - acc: 0.9685 - val_loss: 0.1350 - val_acc: 0.9679\n",
      "Epoch 221/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0951 - acc: 0.9690 - val_loss: 0.1377 - val_acc: 0.9678\n",
      "Epoch 222/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0963 - acc: 0.9683 - val_loss: 0.1340 - val_acc: 0.9673\n",
      "Epoch 223/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0965 - acc: 0.9685 - val_loss: 0.1343 - val_acc: 0.9680\n",
      "Epoch 224/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0951 - acc: 0.9693 - val_loss: 0.1354 - val_acc: 0.9670\n",
      "Epoch 225/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0960 - acc: 0.9680 - val_loss: 0.1367 - val_acc: 0.9671\n",
      "Epoch 226/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0986 - acc: 0.9679 - val_loss: 0.1365 - val_acc: 0.9685\n",
      "Epoch 227/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0931 - acc: 0.9695 - val_loss: 0.1341 - val_acc: 0.9686\n",
      "Epoch 228/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0953 - acc: 0.9682 - val_loss: 0.1317 - val_acc: 0.9675\n",
      "Epoch 229/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0952 - acc: 0.9686 - val_loss: 0.1393 - val_acc: 0.9678\n",
      "Epoch 230/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0952 - acc: 0.9689 - val_loss: 0.1362 - val_acc: 0.9682\n",
      "Epoch 231/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0951 - acc: 0.9688 - val_loss: 0.1371 - val_acc: 0.9677\n",
      "Epoch 232/500\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 0.0938 - acc: 0.9695 - val_loss: 0.1348 - val_acc: 0.9678\n",
      "Epoch 233/500\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 0.0941 - acc: 0.9687 - val_loss: 0.1399 - val_acc: 0.9669\n",
      "Epoch 234/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0944 - acc: 0.9687 - val_loss: 0.1411 - val_acc: 0.9668\n",
      "Epoch 235/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0965 - acc: 0.9688 - val_loss: 0.1378 - val_acc: 0.9669\n",
      "Epoch 236/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0964 - acc: 0.9680 - val_loss: 0.1372 - val_acc: 0.9659\n",
      "Epoch 237/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0950 - acc: 0.9688 - val_loss: 0.1381 - val_acc: 0.9678\n",
      "Epoch 238/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0923 - acc: 0.9699 - val_loss: 0.1413 - val_acc: 0.9664\n",
      "Epoch 239/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0952 - acc: 0.9681 - val_loss: 0.1345 - val_acc: 0.9690\n",
      "Epoch 240/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0959 - acc: 0.9679 - val_loss: 0.1385 - val_acc: 0.9676\n",
      "Epoch 241/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0936 - acc: 0.9689 - val_loss: 0.1389 - val_acc: 0.9674\n",
      "Epoch 242/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0959 - acc: 0.9673 - val_loss: 0.1385 - val_acc: 0.9679\n",
      "Epoch 243/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0966 - acc: 0.9682 - val_loss: 0.1377 - val_acc: 0.9682\n",
      "Epoch 244/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0946 - acc: 0.9689 - val_loss: 0.1377 - val_acc: 0.9674\n",
      "Epoch 245/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0934 - acc: 0.9700 - val_loss: 0.1387 - val_acc: 0.9667\n",
      "Epoch 246/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0959 - acc: 0.9693 - val_loss: 0.1387 - val_acc: 0.9674\n",
      "Epoch 247/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0940 - acc: 0.9693 - val_loss: 0.1389 - val_acc: 0.9675\n",
      "Epoch 248/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0934 - acc: 0.9698 - val_loss: 0.1397 - val_acc: 0.9665\n",
      "Epoch 249/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0953 - acc: 0.9681 - val_loss: 0.1402 - val_acc: 0.9667\n",
      "Epoch 250/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0927 - acc: 0.9694 - val_loss: 0.1435 - val_acc: 0.9674\n",
      "Epoch 251/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0945 - acc: 0.9688 - val_loss: 0.1434 - val_acc: 0.9651\n",
      "Epoch 252/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0937 - acc: 0.9692 - val_loss: 0.1396 - val_acc: 0.9679\n",
      "Epoch 253/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0960 - acc: 0.9675 - val_loss: 0.1426 - val_acc: 0.9661\n",
      "Epoch 254/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0925 - acc: 0.9694 - val_loss: 0.1429 - val_acc: 0.9677\n",
      "Epoch 255/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0933 - acc: 0.9692 - val_loss: 0.1413 - val_acc: 0.9667\n",
      "Epoch 256/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0950 - acc: 0.9684 - val_loss: 0.1402 - val_acc: 0.9675\n",
      "Epoch 257/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0913 - acc: 0.9701 - val_loss: 0.1400 - val_acc: 0.9666\n",
      "Epoch 258/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0918 - acc: 0.9698 - val_loss: 0.1399 - val_acc: 0.9683\n",
      "Epoch 259/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0920 - acc: 0.9701 - val_loss: 0.1415 - val_acc: 0.9661\n",
      "Epoch 260/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0923 - acc: 0.9699 - val_loss: 0.1397 - val_acc: 0.9682\n",
      "Epoch 261/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0922 - acc: 0.9693 - val_loss: 0.1413 - val_acc: 0.9674\n",
      "Epoch 262/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0930 - acc: 0.9693 - val_loss: 0.1463 - val_acc: 0.9656\n",
      "Epoch 263/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0948 - acc: 0.9685 - val_loss: 0.1410 - val_acc: 0.9664\n",
      "Epoch 264/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0937 - acc: 0.9693 - val_loss: 0.1428 - val_acc: 0.9659\n",
      "Epoch 265/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0929 - acc: 0.9689 - val_loss: 0.1398 - val_acc: 0.9663\n",
      "Epoch 266/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0953 - acc: 0.9682 - val_loss: 0.1406 - val_acc: 0.9673\n",
      "Epoch 267/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0961 - acc: 0.9687 - val_loss: 0.1411 - val_acc: 0.9672\n",
      "Epoch 268/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0899 - acc: 0.9699 - val_loss: 0.1449 - val_acc: 0.9668\n",
      "Epoch 269/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0916 - acc: 0.9704 - val_loss: 0.1420 - val_acc: 0.9677\n",
      "Epoch 270/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0888 - acc: 0.9710 - val_loss: 0.1440 - val_acc: 0.9671\n",
      "Epoch 271/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0943 - acc: 0.9699 - val_loss: 0.1435 - val_acc: 0.9665\n",
      "Epoch 272/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0927 - acc: 0.9697 - val_loss: 0.1433 - val_acc: 0.9665\n",
      "Epoch 273/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0923 - acc: 0.9693 - val_loss: 0.1426 - val_acc: 0.9666\n",
      "Epoch 274/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0914 - acc: 0.9705 - val_loss: 0.1430 - val_acc: 0.9668\n",
      "Epoch 275/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0920 - acc: 0.9695 - val_loss: 0.1432 - val_acc: 0.9666\n",
      "Epoch 276/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0920 - acc: 0.9689 - val_loss: 0.1423 - val_acc: 0.9668\n",
      "Epoch 277/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0940 - acc: 0.9694 - val_loss: 0.1419 - val_acc: 0.9661\n",
      "Epoch 278/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0924 - acc: 0.9693 - val_loss: 0.1446 - val_acc: 0.9659\n",
      "Epoch 279/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0910 - acc: 0.9699 - val_loss: 0.1453 - val_acc: 0.9673\n",
      "Epoch 280/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0923 - acc: 0.9697 - val_loss: 0.1458 - val_acc: 0.9667\n",
      "Epoch 281/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0911 - acc: 0.9698 - val_loss: 0.1460 - val_acc: 0.9661\n",
      "Epoch 282/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0925 - acc: 0.9698 - val_loss: 0.1441 - val_acc: 0.9665\n",
      "Epoch 283/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0903 - acc: 0.9700 - val_loss: 0.1450 - val_acc: 0.9677\n",
      "Epoch 284/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0912 - acc: 0.9703 - val_loss: 0.1485 - val_acc: 0.9665\n",
      "Epoch 285/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0927 - acc: 0.9696 - val_loss: 0.1467 - val_acc: 0.9673\n",
      "Epoch 286/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0916 - acc: 0.9697 - val_loss: 0.1452 - val_acc: 0.9673\n",
      "Epoch 287/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0890 - acc: 0.9706 - val_loss: 0.1462 - val_acc: 0.9666\n",
      "Epoch 288/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0885 - acc: 0.9701 - val_loss: 0.1459 - val_acc: 0.9662\n",
      "Epoch 289/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0924 - acc: 0.9699 - val_loss: 0.1452 - val_acc: 0.9673\n",
      "Epoch 290/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0925 - acc: 0.9701 - val_loss: 0.1438 - val_acc: 0.9675\n",
      "Epoch 291/500\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 0.0897 - acc: 0.9702 - val_loss: 0.1464 - val_acc: 0.9673\n",
      "Epoch 292/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0925 - acc: 0.9699 - val_loss: 0.1465 - val_acc: 0.9671\n",
      "Epoch 293/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0920 - acc: 0.9705 - val_loss: 0.1436 - val_acc: 0.9679\n",
      "Epoch 294/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0877 - acc: 0.9712 - val_loss: 0.1503 - val_acc: 0.9658\n",
      "Epoch 295/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0889 - acc: 0.9700 - val_loss: 0.1455 - val_acc: 0.9680\n",
      "Epoch 296/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0909 - acc: 0.9699 - val_loss: 0.1475 - val_acc: 0.9659\n",
      "Epoch 297/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0904 - acc: 0.9695 - val_loss: 0.1478 - val_acc: 0.9672\n",
      "Epoch 298/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0900 - acc: 0.9704 - val_loss: 0.1459 - val_acc: 0.9682\n",
      "Epoch 299/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0914 - acc: 0.9694 - val_loss: 0.1458 - val_acc: 0.9689\n",
      "Epoch 300/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0877 - acc: 0.9704 - val_loss: 0.1459 - val_acc: 0.9667\n",
      "Epoch 301/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0893 - acc: 0.9704 - val_loss: 0.1484 - val_acc: 0.9673\n",
      "Epoch 302/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0896 - acc: 0.9699 - val_loss: 0.1496 - val_acc: 0.9659\n",
      "Epoch 303/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0917 - acc: 0.9699 - val_loss: 0.1460 - val_acc: 0.9664\n",
      "Epoch 304/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0896 - acc: 0.9707 - val_loss: 0.1493 - val_acc: 0.9671\n",
      "Epoch 305/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0885 - acc: 0.9710 - val_loss: 0.1482 - val_acc: 0.9661\n",
      "Epoch 306/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0890 - acc: 0.9706 - val_loss: 0.1490 - val_acc: 0.9674\n",
      "Epoch 307/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0888 - acc: 0.9705 - val_loss: 0.1471 - val_acc: 0.9668\n",
      "Epoch 308/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0884 - acc: 0.9711 - val_loss: 0.1498 - val_acc: 0.9666\n",
      "Epoch 309/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0869 - acc: 0.9716 - val_loss: 0.1473 - val_acc: 0.9667\n",
      "Epoch 310/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0892 - acc: 0.9709 - val_loss: 0.1501 - val_acc: 0.9661\n",
      "Epoch 311/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0883 - acc: 0.9712 - val_loss: 0.1478 - val_acc: 0.9679\n",
      "Epoch 312/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0902 - acc: 0.9704 - val_loss: 0.1501 - val_acc: 0.9652\n",
      "Epoch 313/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0886 - acc: 0.9708 - val_loss: 0.1514 - val_acc: 0.9668\n",
      "Epoch 314/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0894 - acc: 0.9707 - val_loss: 0.1505 - val_acc: 0.9666\n",
      "Epoch 315/500\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 0.0857 - acc: 0.9720 - val_loss: 0.1482 - val_acc: 0.9665\n",
      "Epoch 316/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0895 - acc: 0.9700 - val_loss: 0.1451 - val_acc: 0.9666\n",
      "Epoch 317/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0891 - acc: 0.9707 - val_loss: 0.1492 - val_acc: 0.9661\n",
      "Epoch 318/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0862 - acc: 0.9714 - val_loss: 0.1486 - val_acc: 0.9675\n",
      "Epoch 319/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0905 - acc: 0.9697 - val_loss: 0.1493 - val_acc: 0.9666\n",
      "Epoch 320/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0895 - acc: 0.9710 - val_loss: 0.1489 - val_acc: 0.9671\n",
      "Epoch 321/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0856 - acc: 0.9715 - val_loss: 0.1483 - val_acc: 0.9668\n",
      "Epoch 322/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0880 - acc: 0.9714 - val_loss: 0.1540 - val_acc: 0.9682\n",
      "Epoch 323/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0886 - acc: 0.9701 - val_loss: 0.1469 - val_acc: 0.9677\n",
      "Epoch 324/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0883 - acc: 0.9709 - val_loss: 0.1491 - val_acc: 0.9666\n",
      "Epoch 325/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0892 - acc: 0.9708 - val_loss: 0.1464 - val_acc: 0.9679\n",
      "Epoch 326/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0866 - acc: 0.9713 - val_loss: 0.1515 - val_acc: 0.9668\n",
      "Epoch 327/500\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 0.0854 - acc: 0.9717 - val_loss: 0.1521 - val_acc: 0.9663\n",
      "Epoch 328/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0886 - acc: 0.9701 - val_loss: 0.1526 - val_acc: 0.9679\n",
      "Epoch 329/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0869 - acc: 0.9714 - val_loss: 0.1505 - val_acc: 0.9668\n",
      "Epoch 330/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0885 - acc: 0.9716 - val_loss: 0.1537 - val_acc: 0.9662\n",
      "Epoch 331/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0884 - acc: 0.9702 - val_loss: 0.1507 - val_acc: 0.9668\n",
      "Epoch 332/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0861 - acc: 0.9720 - val_loss: 0.1499 - val_acc: 0.9679\n",
      "Epoch 333/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0904 - acc: 0.9702 - val_loss: 0.1532 - val_acc: 0.9661\n",
      "Epoch 334/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0890 - acc: 0.9707 - val_loss: 0.1589 - val_acc: 0.9650\n",
      "Epoch 335/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0862 - acc: 0.9714 - val_loss: 0.1517 - val_acc: 0.9675\n",
      "Epoch 336/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0887 - acc: 0.9708 - val_loss: 0.1542 - val_acc: 0.9675\n",
      "Epoch 337/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0879 - acc: 0.9706 - val_loss: 0.1504 - val_acc: 0.9666\n",
      "Epoch 338/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0878 - acc: 0.9710 - val_loss: 0.1551 - val_acc: 0.9648\n",
      "Epoch 339/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0884 - acc: 0.9711 - val_loss: 0.1525 - val_acc: 0.9667\n",
      "Epoch 340/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0885 - acc: 0.9709 - val_loss: 0.1494 - val_acc: 0.9662\n",
      "Epoch 341/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0856 - acc: 0.9722 - val_loss: 0.1527 - val_acc: 0.9670\n",
      "Epoch 342/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0908 - acc: 0.9701 - val_loss: 0.1529 - val_acc: 0.9657\n",
      "Epoch 343/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0881 - acc: 0.9710 - val_loss: 0.1528 - val_acc: 0.9664\n",
      "Epoch 344/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0863 - acc: 0.9712 - val_loss: 0.1537 - val_acc: 0.9665\n",
      "Epoch 345/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0873 - acc: 0.9709 - val_loss: 0.1523 - val_acc: 0.9669\n",
      "Epoch 346/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0861 - acc: 0.9716 - val_loss: 0.1531 - val_acc: 0.9660\n",
      "Epoch 347/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0841 - acc: 0.9720 - val_loss: 0.1518 - val_acc: 0.9672\n",
      "Epoch 348/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0874 - acc: 0.9714 - val_loss: 0.1539 - val_acc: 0.9663\n",
      "Epoch 349/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0851 - acc: 0.9716 - val_loss: 0.1524 - val_acc: 0.9675\n",
      "Epoch 350/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0855 - acc: 0.9713 - val_loss: 0.1521 - val_acc: 0.9677\n",
      "Epoch 351/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0842 - acc: 0.9726 - val_loss: 0.1564 - val_acc: 0.9657\n",
      "Epoch 352/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0869 - acc: 0.9714 - val_loss: 0.1536 - val_acc: 0.9659\n",
      "Epoch 353/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0874 - acc: 0.9713 - val_loss: 0.1540 - val_acc: 0.9669\n",
      "Epoch 354/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0837 - acc: 0.9724 - val_loss: 0.1549 - val_acc: 0.9659\n",
      "Epoch 355/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0865 - acc: 0.9716 - val_loss: 0.1525 - val_acc: 0.9664\n",
      "Epoch 356/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0910 - acc: 0.9704 - val_loss: 0.1514 - val_acc: 0.9667\n",
      "Epoch 357/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0875 - acc: 0.9715 - val_loss: 0.1538 - val_acc: 0.9666\n",
      "Epoch 358/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0856 - acc: 0.9718 - val_loss: 0.1565 - val_acc: 0.9662\n",
      "Epoch 359/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0864 - acc: 0.9715 - val_loss: 0.1535 - val_acc: 0.9667\n",
      "Epoch 360/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0870 - acc: 0.9717 - val_loss: 0.1575 - val_acc: 0.9666\n",
      "Epoch 361/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0856 - acc: 0.9721 - val_loss: 0.1555 - val_acc: 0.9649\n",
      "Epoch 362/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0857 - acc: 0.9719 - val_loss: 0.1581 - val_acc: 0.9658\n",
      "Epoch 363/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0840 - acc: 0.9722 - val_loss: 0.1577 - val_acc: 0.9644\n",
      "Epoch 364/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0844 - acc: 0.9726 - val_loss: 0.1546 - val_acc: 0.9680\n",
      "Epoch 365/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0858 - acc: 0.9715 - val_loss: 0.1529 - val_acc: 0.9672\n",
      "Epoch 366/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0856 - acc: 0.9722 - val_loss: 0.1535 - val_acc: 0.9662\n",
      "Epoch 367/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0864 - acc: 0.9715 - val_loss: 0.1569 - val_acc: 0.9657\n",
      "Epoch 368/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0863 - acc: 0.9718 - val_loss: 0.1551 - val_acc: 0.9659\n",
      "Epoch 369/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0864 - acc: 0.9725 - val_loss: 0.1564 - val_acc: 0.9655\n",
      "Epoch 370/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0863 - acc: 0.9721 - val_loss: 0.1550 - val_acc: 0.9665\n",
      "Epoch 371/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0865 - acc: 0.9718 - val_loss: 0.1556 - val_acc: 0.9665\n",
      "Epoch 372/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0849 - acc: 0.9722 - val_loss: 0.1552 - val_acc: 0.9652\n",
      "Epoch 373/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0873 - acc: 0.9709 - val_loss: 0.1517 - val_acc: 0.9671\n",
      "Epoch 374/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0850 - acc: 0.9722 - val_loss: 0.1581 - val_acc: 0.9662\n",
      "Epoch 375/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0851 - acc: 0.9725 - val_loss: 0.1542 - val_acc: 0.9662\n",
      "Epoch 376/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0854 - acc: 0.9726 - val_loss: 0.1590 - val_acc: 0.9664\n",
      "Epoch 377/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0856 - acc: 0.9717 - val_loss: 0.1570 - val_acc: 0.9651\n",
      "Epoch 378/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0874 - acc: 0.9706 - val_loss: 0.1557 - val_acc: 0.9678\n",
      "Epoch 379/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0849 - acc: 0.9717 - val_loss: 0.1579 - val_acc: 0.9647\n",
      "Epoch 380/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0850 - acc: 0.9711 - val_loss: 0.1530 - val_acc: 0.9668\n",
      "Epoch 381/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0872 - acc: 0.9711 - val_loss: 0.1565 - val_acc: 0.9651\n",
      "Epoch 382/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0818 - acc: 0.9735 - val_loss: 0.1550 - val_acc: 0.9669\n",
      "Epoch 383/500\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 0.0846 - acc: 0.9724 - val_loss: 0.1566 - val_acc: 0.9655\n",
      "Epoch 384/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0847 - acc: 0.9714 - val_loss: 0.1549 - val_acc: 0.9659\n",
      "Epoch 385/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0857 - acc: 0.9720 - val_loss: 0.1533 - val_acc: 0.9665\n",
      "Epoch 386/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0848 - acc: 0.9729 - val_loss: 0.1562 - val_acc: 0.9673\n",
      "Epoch 387/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0871 - acc: 0.9708 - val_loss: 0.1582 - val_acc: 0.9651\n",
      "Epoch 388/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0868 - acc: 0.9713 - val_loss: 0.1597 - val_acc: 0.9638\n",
      "Epoch 389/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0846 - acc: 0.9717 - val_loss: 0.1598 - val_acc: 0.9660\n",
      "Epoch 390/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0844 - acc: 0.9719 - val_loss: 0.1605 - val_acc: 0.9648\n",
      "Epoch 391/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0879 - acc: 0.9714 - val_loss: 0.1569 - val_acc: 0.9652\n",
      "Epoch 392/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0857 - acc: 0.9714 - val_loss: 0.1587 - val_acc: 0.9657\n",
      "Epoch 393/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0823 - acc: 0.9728 - val_loss: 0.1578 - val_acc: 0.9668\n",
      "Epoch 394/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0825 - acc: 0.9727 - val_loss: 0.1572 - val_acc: 0.9654\n",
      "Epoch 395/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0845 - acc: 0.9721 - val_loss: 0.1578 - val_acc: 0.9659\n",
      "Epoch 396/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0840 - acc: 0.9715 - val_loss: 0.1567 - val_acc: 0.9663\n",
      "Epoch 397/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0860 - acc: 0.9728 - val_loss: 0.1589 - val_acc: 0.9653\n",
      "Epoch 398/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0823 - acc: 0.9730 - val_loss: 0.1621 - val_acc: 0.9646\n",
      "Epoch 399/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0827 - acc: 0.9729 - val_loss: 0.1572 - val_acc: 0.9657\n",
      "Epoch 400/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0846 - acc: 0.9720 - val_loss: 0.1587 - val_acc: 0.9653\n",
      "Epoch 401/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0826 - acc: 0.9732 - val_loss: 0.1588 - val_acc: 0.9651\n",
      "Epoch 402/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0852 - acc: 0.9716 - val_loss: 0.1660 - val_acc: 0.9655\n",
      "Epoch 403/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0825 - acc: 0.9730 - val_loss: 0.1624 - val_acc: 0.9667\n",
      "Epoch 404/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0815 - acc: 0.9732 - val_loss: 0.1601 - val_acc: 0.9652\n",
      "Epoch 405/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0845 - acc: 0.9723 - val_loss: 0.1593 - val_acc: 0.9659\n",
      "Epoch 406/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0821 - acc: 0.9727 - val_loss: 0.1600 - val_acc: 0.9648\n",
      "Epoch 407/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0848 - acc: 0.9714 - val_loss: 0.1624 - val_acc: 0.9662\n",
      "Epoch 408/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0855 - acc: 0.9716 - val_loss: 0.1591 - val_acc: 0.9658\n",
      "Epoch 409/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0823 - acc: 0.9728 - val_loss: 0.1630 - val_acc: 0.9645\n",
      "Epoch 410/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0830 - acc: 0.9727 - val_loss: 0.1605 - val_acc: 0.9666\n",
      "Epoch 411/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0829 - acc: 0.9726 - val_loss: 0.1600 - val_acc: 0.9658\n",
      "Epoch 412/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0834 - acc: 0.9725 - val_loss: 0.1608 - val_acc: 0.9658\n",
      "Epoch 413/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0841 - acc: 0.9722 - val_loss: 0.1586 - val_acc: 0.9651\n",
      "Epoch 414/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0840 - acc: 0.9725 - val_loss: 0.1634 - val_acc: 0.9654\n",
      "Epoch 415/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0809 - acc: 0.9727 - val_loss: 0.1646 - val_acc: 0.9665\n",
      "Epoch 416/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0830 - acc: 0.9723 - val_loss: 0.1620 - val_acc: 0.9654\n",
      "Epoch 417/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0842 - acc: 0.9719 - val_loss: 0.1582 - val_acc: 0.9666\n",
      "Epoch 418/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0832 - acc: 0.9727 - val_loss: 0.1585 - val_acc: 0.9664\n",
      "Epoch 419/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0838 - acc: 0.9723 - val_loss: 0.1595 - val_acc: 0.9656\n",
      "Epoch 420/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0843 - acc: 0.9723 - val_loss: 0.1593 - val_acc: 0.9667\n",
      "Epoch 421/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0852 - acc: 0.9715 - val_loss: 0.1628 - val_acc: 0.9656\n",
      "Epoch 422/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0831 - acc: 0.9721 - val_loss: 0.1620 - val_acc: 0.9654\n",
      "Epoch 423/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0835 - acc: 0.9718 - val_loss: 0.1611 - val_acc: 0.9654\n",
      "Epoch 424/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0842 - acc: 0.9727 - val_loss: 0.1654 - val_acc: 0.9651\n",
      "Epoch 425/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0831 - acc: 0.9719 - val_loss: 0.1658 - val_acc: 0.9647\n",
      "Epoch 426/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0836 - acc: 0.9724 - val_loss: 0.1633 - val_acc: 0.9660\n",
      "Epoch 427/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0791 - acc: 0.9738 - val_loss: 0.1632 - val_acc: 0.9648\n",
      "Epoch 428/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0827 - acc: 0.9724 - val_loss: 0.1623 - val_acc: 0.9643\n",
      "Epoch 429/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0822 - acc: 0.9736 - val_loss: 0.1635 - val_acc: 0.9656\n",
      "Epoch 430/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0837 - acc: 0.9719 - val_loss: 0.1615 - val_acc: 0.9667\n",
      "Epoch 431/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0843 - acc: 0.9722 - val_loss: 0.1638 - val_acc: 0.9660\n",
      "Epoch 432/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0814 - acc: 0.9731 - val_loss: 0.1661 - val_acc: 0.9652\n",
      "Epoch 433/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0838 - acc: 0.9725 - val_loss: 0.1641 - val_acc: 0.9655\n",
      "Epoch 434/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0825 - acc: 0.9732 - val_loss: 0.1644 - val_acc: 0.9644\n",
      "Epoch 435/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0817 - acc: 0.9730 - val_loss: 0.1612 - val_acc: 0.9652\n",
      "Epoch 436/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0843 - acc: 0.9722 - val_loss: 0.1650 - val_acc: 0.9656\n",
      "Epoch 437/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0847 - acc: 0.9719 - val_loss: 0.1633 - val_acc: 0.9660\n",
      "Epoch 438/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0817 - acc: 0.9723 - val_loss: 0.1640 - val_acc: 0.9656\n",
      "Epoch 439/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0826 - acc: 0.9727 - val_loss: 0.1634 - val_acc: 0.9660\n",
      "Epoch 440/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0816 - acc: 0.9731 - val_loss: 0.1641 - val_acc: 0.9652\n",
      "Epoch 441/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0806 - acc: 0.9742 - val_loss: 0.1640 - val_acc: 0.9652\n",
      "Epoch 442/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0838 - acc: 0.9725 - val_loss: 0.1650 - val_acc: 0.9649\n",
      "Epoch 443/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0795 - acc: 0.9728 - val_loss: 0.1645 - val_acc: 0.9658\n",
      "Epoch 444/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0840 - acc: 0.9721 - val_loss: 0.1639 - val_acc: 0.9648\n",
      "Epoch 445/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0816 - acc: 0.9733 - val_loss: 0.1647 - val_acc: 0.9658\n",
      "Epoch 446/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0791 - acc: 0.9731 - val_loss: 0.1657 - val_acc: 0.9658\n",
      "Epoch 447/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0805 - acc: 0.9738 - val_loss: 0.1638 - val_acc: 0.9653\n",
      "Epoch 448/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0808 - acc: 0.9739 - val_loss: 0.1680 - val_acc: 0.9647\n",
      "Epoch 449/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0821 - acc: 0.9732 - val_loss: 0.1641 - val_acc: 0.9657\n",
      "Epoch 450/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0837 - acc: 0.9724 - val_loss: 0.1689 - val_acc: 0.9646\n",
      "Epoch 451/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0822 - acc: 0.9729 - val_loss: 0.1637 - val_acc: 0.9656\n",
      "Epoch 452/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0832 - acc: 0.9731 - val_loss: 0.1630 - val_acc: 0.9653\n",
      "Epoch 453/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0827 - acc: 0.9724 - val_loss: 0.1633 - val_acc: 0.9656\n",
      "Epoch 454/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0790 - acc: 0.9738 - val_loss: 0.1614 - val_acc: 0.9651\n",
      "Epoch 455/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0845 - acc: 0.9724 - val_loss: 0.1629 - val_acc: 0.9632\n",
      "Epoch 456/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0828 - acc: 0.9729 - val_loss: 0.1676 - val_acc: 0.9651\n",
      "Epoch 457/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0817 - acc: 0.9730 - val_loss: 0.1619 - val_acc: 0.9660\n",
      "Epoch 458/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0805 - acc: 0.9736 - val_loss: 0.1655 - val_acc: 0.9651\n",
      "Epoch 459/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0811 - acc: 0.9728 - val_loss: 0.1636 - val_acc: 0.9656\n",
      "Epoch 460/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0816 - acc: 0.9724 - val_loss: 0.1707 - val_acc: 0.9650\n",
      "Epoch 461/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0815 - acc: 0.9731 - val_loss: 0.1665 - val_acc: 0.9660\n",
      "Epoch 462/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0829 - acc: 0.9731 - val_loss: 0.1661 - val_acc: 0.9650\n",
      "Epoch 463/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0821 - acc: 0.9731 - val_loss: 0.1738 - val_acc: 0.9644\n",
      "Epoch 464/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0812 - acc: 0.9731 - val_loss: 0.1703 - val_acc: 0.9642\n",
      "Epoch 465/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0817 - acc: 0.9727 - val_loss: 0.1683 - val_acc: 0.9649\n",
      "Epoch 466/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0808 - acc: 0.9730 - val_loss: 0.1705 - val_acc: 0.9655\n",
      "Epoch 467/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0818 - acc: 0.9730 - val_loss: 0.1712 - val_acc: 0.9649\n",
      "Epoch 468/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0826 - acc: 0.9720 - val_loss: 0.1685 - val_acc: 0.9650\n",
      "Epoch 469/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0818 - acc: 0.9722 - val_loss: 0.1692 - val_acc: 0.9656\n",
      "Epoch 470/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0830 - acc: 0.9720 - val_loss: 0.1707 - val_acc: 0.9634\n",
      "Epoch 471/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0803 - acc: 0.9736 - val_loss: 0.1670 - val_acc: 0.9643\n",
      "Epoch 472/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0815 - acc: 0.9731 - val_loss: 0.1657 - val_acc: 0.9644\n",
      "Epoch 473/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0824 - acc: 0.9728 - val_loss: 0.1724 - val_acc: 0.9635\n",
      "Epoch 474/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0825 - acc: 0.9728 - val_loss: 0.1654 - val_acc: 0.9652\n",
      "Epoch 475/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0808 - acc: 0.9735 - val_loss: 0.1664 - val_acc: 0.9656\n",
      "Epoch 476/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0805 - acc: 0.9736 - val_loss: 0.1744 - val_acc: 0.9641\n",
      "Epoch 477/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0794 - acc: 0.9735 - val_loss: 0.1677 - val_acc: 0.9643\n",
      "Epoch 478/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0796 - acc: 0.9730 - val_loss: 0.1671 - val_acc: 0.9653\n",
      "Epoch 479/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0817 - acc: 0.9735 - val_loss: 0.1686 - val_acc: 0.9642\n",
      "Epoch 480/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0796 - acc: 0.9731 - val_loss: 0.1686 - val_acc: 0.9643\n",
      "Epoch 481/500\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0818 - acc: 0.9731 - val_loss: 0.1705 - val_acc: 0.9644\n",
      "Epoch 482/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0814 - acc: 0.9738 - val_loss: 0.1734 - val_acc: 0.9640\n",
      "Epoch 483/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0777 - acc: 0.9744 - val_loss: 0.1681 - val_acc: 0.9648\n",
      "Epoch 484/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0812 - acc: 0.9736 - val_loss: 0.1693 - val_acc: 0.9645\n",
      "Epoch 485/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0814 - acc: 0.9732 - val_loss: 0.1668 - val_acc: 0.9653\n",
      "Epoch 486/500\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.0812 - acc: 0.9730 - val_loss: 0.1659 - val_acc: 0.9645\n",
      "Epoch 487/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0795 - acc: 0.9742 - val_loss: 0.1667 - val_acc: 0.9648\n",
      "Epoch 488/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0790 - acc: 0.9740 - val_loss: 0.1681 - val_acc: 0.9636\n",
      "Epoch 489/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0810 - acc: 0.9731 - val_loss: 0.1724 - val_acc: 0.9644\n",
      "Epoch 490/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0802 - acc: 0.9742 - val_loss: 0.1724 - val_acc: 0.9657\n",
      "Epoch 491/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0809 - acc: 0.9734 - val_loss: 0.1683 - val_acc: 0.9642\n",
      "Epoch 492/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0821 - acc: 0.9728 - val_loss: 0.1727 - val_acc: 0.9644\n",
      "Epoch 493/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0847 - acc: 0.9719 - val_loss: 0.1724 - val_acc: 0.9651\n",
      "Epoch 494/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0818 - acc: 0.9732 - val_loss: 0.1684 - val_acc: 0.9649\n",
      "Epoch 495/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0802 - acc: 0.9733 - val_loss: 0.1749 - val_acc: 0.9646\n",
      "Epoch 496/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0817 - acc: 0.9729 - val_loss: 0.1697 - val_acc: 0.9655\n",
      "Epoch 497/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0831 - acc: 0.9720 - val_loss: 0.1696 - val_acc: 0.9653\n",
      "Epoch 498/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0790 - acc: 0.9737 - val_loss: 0.1728 - val_acc: 0.9645\n",
      "Epoch 499/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0789 - acc: 0.9735 - val_loss: 0.1691 - val_acc: 0.9642\n",
      "Epoch 500/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0802 - acc: 0.9736 - val_loss: 0.1706 - val_acc: 0.9642\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3c4c08c978>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_dir = 'logs/no_loss_function/'\n",
    "logging = TensorBoard(log_dir=log_dir)\n",
    "checkpoint = ModelCheckpoint(log_dir + 'ep{epoch:03d}-acc{acc:.4f}-val_acc{val_acc:.4f}-loss{loss:.4f}-val_loss{val_loss:.4f}.h5',\n",
    "        monitor='val_acc', save_weights_only=True, save_best_only=True, period=3)\n",
    "\n",
    "\n",
    "history = studentX.fit(X_train, Y_train_soft,\n",
    "                      batch_size=256,\n",
    "                      epochs=epochs,\n",
    "                      verbose=1,\n",
    "                      validation_data=(X_test, Y_test),\n",
    "                      callbacks=[logging,checkpoint] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_acc = history.history['acc'][-1]\n",
    "last_val_acc = history.history['val_acc'][-1]\n",
    "last_loss = history.history['loss'][-1]\n",
    "last_val_loss = history.history['val_loss'][-1]\n",
    "\n",
    "hist = \"acc{0:.4f}-val_acc{0:.4f}-loss{0:.4f}-val_loss{0:.4f}\".format(last_acc,last_val_acc,last_loss,last_val_loss)\n",
    "studentX.save_weights(log_dir + \"last_\"+ hist + \".h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QcbgRhIIrTNl"
   },
   "source": [
    "# StudentA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "colab_type": "code",
    "id": "jRPkY70-I434",
    "outputId": "4387db0b-4f69-4178-c350-84a52ccfd0b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_5 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 25,450\n",
      "Trainable params: 25,450\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "studentA = Sequential()\n",
    "studentA.add(Flatten(input_shape=input_shape))\n",
    "studentA.add(Dense(32, activation='relu'))\n",
    "studentA.add(Dropout(0.2))\n",
    "studentA.add(Dense(nb_classes))\n",
    "studentA.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "##sgd = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "studentA.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#studentA = Model(student.input,student.output)\n",
    "studentA.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "colab_type": "code",
    "id": "B8MbPXV7it21",
    "outputId": "505d7cdb-33ce-4ef4-edd0-0c288602b99f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "flatten_5_input (InputLayer)    (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 784)          0           flatten_5_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 32)           25120       flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 32)           0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 10)           330         dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 10)           0           dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 10)           0           dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 10)           0           lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 20)           0           activation_6[0][0]               \n",
      "                                                                 activation_7[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 25,450\n",
      "Trainable params: 25,450\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Remove the softmax layer from the student network\n",
    "#student.layers.pop()\n",
    "\n",
    "# Now collect the logits from the last layer\n",
    "logits = studentA.layers[-2].output # This is going to be a tensor. And hence it needs to pass through a Activation layer\n",
    "probs = Activation('softmax')(logits)\n",
    "\n",
    "# softed probabilities at raised temperature\n",
    "logits_T = Lambda(lambda x: x / temp)(logits)\n",
    "probs_T = Activation('softmax')(logits_T)\n",
    "\n",
    "output = concatenate([probs, probs_T])\n",
    "\n",
    "\n",
    "# This is our new student model\n",
    "studentA = Model(studentA.input, output)\n",
    "\n",
    "studentA.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.11022913, 0.1789273 , 0.07496126, 0.10378706, 0.09180001,\n",
       "        0.10597233, 0.08889865, 0.07031129, 0.1326415 , 0.04247146,\n",
       "        0.11022913, 0.1789273 , 0.07496126, 0.10378706, 0.09180001,\n",
       "        0.10597233, 0.08889865, 0.07031129, 0.1326415 , 0.04247146]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "studentA.predict( X_train[0].reshape(1,28,28,1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GVz7tqSIi4GN"
   },
   "outputs": [],
   "source": [
    "# This will be a teacher trained student model. \n",
    "# --> This uses a knowledge distillation loss function\n",
    "\n",
    "# Declare knowledge distillation loss\n",
    "def knowledge_distillation_loss(y_true, y_pred, alpha):\n",
    "\n",
    "    # Extract the one-hot encoded values and the softs separately so that we can create two objective functions\n",
    "    y_true, y_logits = y_true[: , :nb_classes], y_true[: , nb_classes:]\n",
    "    \n",
    "    y_pred, y_pred_softs = y_pred[: , :nb_classes], y_pred[: , nb_classes:]\n",
    "    \n",
    "    loss = ( alpha*temp*logloss(y_logits, y_pred) ) + ( (1-alpha)*logloss(y_true,y_pred) ) \n",
    "    \n",
    "    return loss\n",
    "\n",
    "# For testing use regular output probabilities - without temperature\n",
    "def acc(y_true, y_pred):\n",
    "    y_true = y_true[:, :nb_classes]\n",
    "    y_pred = y_pred[:, :nb_classes]\n",
    "    return categorical_accuracy(y_true, y_pred)\n",
    "  \n",
    "# For testing use regular output probabilities - without temperature\n",
    "def true_loss(y_true, y_pred):\n",
    "    y_true = y_true[:, :nb_classes]\n",
    "    y_pred = y_pred[:, :nb_classes]\n",
    "    return logloss(y_true, y_pred)\n",
    "\n",
    "def logits_loss(y_true, y_pred):\n",
    "    y_true = y_true[:, nb_classes:]\n",
    "    y_pred = y_pred[:, :nb_classes]\n",
    "    return logloss(y_true, y_pred)\n",
    "\n",
    "studentA.compile(\n",
    "    #optimizer=optimizers.SGD(lr=1e-1, momentum=0.9, nesterov=True),\n",
    "    optimizer='adadelta',\n",
    "    loss=lambda y_true, y_pred: knowledge_distillation_loss(y_true, y_pred, 0.5),\n",
    "    #loss='categorical_crossentropy',\n",
    "    metrics=[acc] )#,true_loss,logits_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "colab_type": "code",
    "id": "697eUAMOi_Y0",
    "outputId": "42d53797-0f3e-4be9-f221-03507cbc07d7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/500\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.7267 - acc: 0.7879 - val_loss: 0.3124 - val_acc: 0.9119\n",
      "Epoch 2/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.3808 - acc: 0.8869 - val_loss: 0.2486 - val_acc: 0.9250\n",
      "Epoch 3/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.3187 - acc: 0.9068 - val_loss: 0.2151 - val_acc: 0.9362\n",
      "Epoch 4/500\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.2862 - acc: 0.9152 - val_loss: 0.1925 - val_acc: 0.9427\n",
      "Epoch 5/500\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.2645 - acc: 0.9226 - val_loss: 0.1816 - val_acc: 0.9445\n",
      "Epoch 6/500\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.2488 - acc: 0.9264 - val_loss: 0.1716 - val_acc: 0.9480\n",
      "Epoch 7/500\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.2367 - acc: 0.9303 - val_loss: 0.1655 - val_acc: 0.9485\n",
      "Epoch 8/500\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.2285 - acc: 0.9316 - val_loss: 0.1590 - val_acc: 0.9530\n",
      "Epoch 9/500\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.2204 - acc: 0.9340 - val_loss: 0.1527 - val_acc: 0.9525\n",
      "Epoch 10/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.2145 - acc: 0.9362 - val_loss: 0.1457 - val_acc: 0.9561\n",
      "Epoch 11/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.2085 - acc: 0.9365 - val_loss: 0.1429 - val_acc: 0.9571\n",
      "Epoch 12/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.2032 - acc: 0.9383 - val_loss: 0.1404 - val_acc: 0.9582\n",
      "Epoch 13/500\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.1978 - acc: 0.9405 - val_loss: 0.1409 - val_acc: 0.9570\n",
      "Epoch 14/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1927 - acc: 0.9421 - val_loss: 0.1358 - val_acc: 0.9592\n",
      "Epoch 15/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1890 - acc: 0.9432 - val_loss: 0.1313 - val_acc: 0.9617\n",
      "Epoch 16/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1839 - acc: 0.9449 - val_loss: 0.1294 - val_acc: 0.9616\n",
      "Epoch 17/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1840 - acc: 0.9442 - val_loss: 0.1282 - val_acc: 0.9609\n",
      "Epoch 18/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1793 - acc: 0.9448 - val_loss: 0.1274 - val_acc: 0.9612\n",
      "Epoch 19/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1763 - acc: 0.9466 - val_loss: 0.1283 - val_acc: 0.9610\n",
      "Epoch 20/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1751 - acc: 0.9470 - val_loss: 0.1238 - val_acc: 0.9628\n",
      "Epoch 21/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1740 - acc: 0.9467 - val_loss: 0.1255 - val_acc: 0.9633\n",
      "Epoch 22/500\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.1706 - acc: 0.9466 - val_loss: 0.1263 - val_acc: 0.9625\n",
      "Epoch 23/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1664 - acc: 0.9493 - val_loss: 0.1227 - val_acc: 0.9635\n",
      "Epoch 24/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1659 - acc: 0.9492 - val_loss: 0.1216 - val_acc: 0.9635\n",
      "Epoch 25/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1638 - acc: 0.9498 - val_loss: 0.1224 - val_acc: 0.9636\n",
      "Epoch 26/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1642 - acc: 0.9489 - val_loss: 0.1188 - val_acc: 0.9635\n",
      "Epoch 27/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1608 - acc: 0.9513 - val_loss: 0.1192 - val_acc: 0.9640\n",
      "Epoch 28/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1613 - acc: 0.9504 - val_loss: 0.1180 - val_acc: 0.9648\n",
      "Epoch 29/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1583 - acc: 0.9509 - val_loss: 0.1171 - val_acc: 0.9640\n",
      "Epoch 30/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1598 - acc: 0.9503 - val_loss: 0.1185 - val_acc: 0.9644\n",
      "Epoch 31/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1558 - acc: 0.9522 - val_loss: 0.1169 - val_acc: 0.9663\n",
      "Epoch 32/500\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.1542 - acc: 0.9525 - val_loss: 0.1198 - val_acc: 0.9644\n",
      "Epoch 33/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1531 - acc: 0.9531 - val_loss: 0.1180 - val_acc: 0.9647\n",
      "Epoch 34/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1519 - acc: 0.9525 - val_loss: 0.1198 - val_acc: 0.9635\n",
      "Epoch 35/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1518 - acc: 0.9527 - val_loss: 0.1215 - val_acc: 0.9654\n",
      "Epoch 36/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1503 - acc: 0.9543 - val_loss: 0.1182 - val_acc: 0.9645\n",
      "Epoch 37/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1464 - acc: 0.9558 - val_loss: 0.1221 - val_acc: 0.9631\n",
      "Epoch 38/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1471 - acc: 0.9541 - val_loss: 0.1195 - val_acc: 0.9649\n",
      "Epoch 39/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1458 - acc: 0.9549 - val_loss: 0.1210 - val_acc: 0.9643\n",
      "Epoch 40/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1445 - acc: 0.9552 - val_loss: 0.1178 - val_acc: 0.9642\n",
      "Epoch 41/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1456 - acc: 0.9539 - val_loss: 0.1199 - val_acc: 0.9651\n",
      "Epoch 42/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1432 - acc: 0.9553 - val_loss: 0.1181 - val_acc: 0.9649\n",
      "Epoch 43/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1410 - acc: 0.9560 - val_loss: 0.1181 - val_acc: 0.9649\n",
      "Epoch 44/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1444 - acc: 0.9541 - val_loss: 0.1172 - val_acc: 0.9652\n",
      "Epoch 45/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1438 - acc: 0.9547 - val_loss: 0.1186 - val_acc: 0.9650\n",
      "Epoch 46/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1405 - acc: 0.9567 - val_loss: 0.1166 - val_acc: 0.9649\n",
      "Epoch 47/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1388 - acc: 0.9561 - val_loss: 0.1164 - val_acc: 0.9657\n",
      "Epoch 48/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1405 - acc: 0.9555 - val_loss: 0.1209 - val_acc: 0.9641\n",
      "Epoch 49/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1376 - acc: 0.9577 - val_loss: 0.1160 - val_acc: 0.9664\n",
      "Epoch 50/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1369 - acc: 0.9578 - val_loss: 0.1168 - val_acc: 0.9649\n",
      "Epoch 51/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1374 - acc: 0.9572 - val_loss: 0.1197 - val_acc: 0.9654\n",
      "Epoch 52/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1366 - acc: 0.9566 - val_loss: 0.1173 - val_acc: 0.9650\n",
      "Epoch 53/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1346 - acc: 0.9580 - val_loss: 0.1170 - val_acc: 0.9650\n",
      "Epoch 54/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1349 - acc: 0.9579 - val_loss: 0.1194 - val_acc: 0.9647\n",
      "Epoch 55/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1337 - acc: 0.9582 - val_loss: 0.1192 - val_acc: 0.9646\n",
      "Epoch 56/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1346 - acc: 0.9576 - val_loss: 0.1179 - val_acc: 0.9648\n",
      "Epoch 57/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1323 - acc: 0.9585 - val_loss: 0.1187 - val_acc: 0.9643\n",
      "Epoch 58/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1354 - acc: 0.9582 - val_loss: 0.1182 - val_acc: 0.9649\n",
      "Epoch 59/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1331 - acc: 0.9581 - val_loss: 0.1193 - val_acc: 0.9644\n",
      "Epoch 60/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1323 - acc: 0.9581 - val_loss: 0.1177 - val_acc: 0.9656\n",
      "Epoch 61/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1298 - acc: 0.9608 - val_loss: 0.1155 - val_acc: 0.9653\n",
      "Epoch 62/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1307 - acc: 0.9585 - val_loss: 0.1177 - val_acc: 0.9649\n",
      "Epoch 63/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1291 - acc: 0.9590 - val_loss: 0.1177 - val_acc: 0.9655\n",
      "Epoch 64/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1294 - acc: 0.9606 - val_loss: 0.1178 - val_acc: 0.9645\n",
      "Epoch 65/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1288 - acc: 0.9591 - val_loss: 0.1181 - val_acc: 0.9651\n",
      "Epoch 66/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1265 - acc: 0.9599 - val_loss: 0.1173 - val_acc: 0.9648\n",
      "Epoch 67/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1296 - acc: 0.9589 - val_loss: 0.1182 - val_acc: 0.9644\n",
      "Epoch 68/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1284 - acc: 0.9589 - val_loss: 0.1186 - val_acc: 0.9647\n",
      "Epoch 69/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1288 - acc: 0.9592 - val_loss: 0.1183 - val_acc: 0.9664\n",
      "Epoch 70/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1275 - acc: 0.9599 - val_loss: 0.1178 - val_acc: 0.9651\n",
      "Epoch 71/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1260 - acc: 0.9601 - val_loss: 0.1201 - val_acc: 0.9638\n",
      "Epoch 72/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1262 - acc: 0.9606 - val_loss: 0.1247 - val_acc: 0.9633\n",
      "Epoch 73/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1259 - acc: 0.9599 - val_loss: 0.1197 - val_acc: 0.9651\n",
      "Epoch 74/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1239 - acc: 0.9603 - val_loss: 0.1209 - val_acc: 0.9646\n",
      "Epoch 75/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1251 - acc: 0.9601 - val_loss: 0.1213 - val_acc: 0.9649\n",
      "Epoch 76/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1241 - acc: 0.9611 - val_loss: 0.1229 - val_acc: 0.9646\n",
      "Epoch 77/500\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.1243 - acc: 0.9614 - val_loss: 0.1201 - val_acc: 0.9655\n",
      "Epoch 78/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1246 - acc: 0.9603 - val_loss: 0.1217 - val_acc: 0.9653\n",
      "Epoch 79/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1235 - acc: 0.9605 - val_loss: 0.1230 - val_acc: 0.9642\n",
      "Epoch 80/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1220 - acc: 0.9615 - val_loss: 0.1266 - val_acc: 0.9641\n",
      "Epoch 81/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1238 - acc: 0.9601 - val_loss: 0.1183 - val_acc: 0.9647\n",
      "Epoch 82/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1201 - acc: 0.9608 - val_loss: 0.1230 - val_acc: 0.9642\n",
      "Epoch 83/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1218 - acc: 0.9618 - val_loss: 0.1203 - val_acc: 0.9644\n",
      "Epoch 84/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1200 - acc: 0.9618 - val_loss: 0.1199 - val_acc: 0.9656\n",
      "Epoch 85/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1203 - acc: 0.9614 - val_loss: 0.1208 - val_acc: 0.9657\n",
      "Epoch 86/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1196 - acc: 0.9616 - val_loss: 0.1203 - val_acc: 0.9655\n",
      "Epoch 87/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1188 - acc: 0.9627 - val_loss: 0.1213 - val_acc: 0.9654\n",
      "Epoch 88/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1205 - acc: 0.9612 - val_loss: 0.1212 - val_acc: 0.9661\n",
      "Epoch 89/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1178 - acc: 0.9621 - val_loss: 0.1230 - val_acc: 0.9659\n",
      "Epoch 90/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1205 - acc: 0.9613 - val_loss: 0.1226 - val_acc: 0.9657\n",
      "Epoch 91/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1162 - acc: 0.9629 - val_loss: 0.1267 - val_acc: 0.9635\n",
      "Epoch 92/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1192 - acc: 0.9615 - val_loss: 0.1212 - val_acc: 0.9660\n",
      "Epoch 93/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1192 - acc: 0.9619 - val_loss: 0.1209 - val_acc: 0.9666\n",
      "Epoch 94/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1195 - acc: 0.9618 - val_loss: 0.1211 - val_acc: 0.9670\n",
      "Epoch 95/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1164 - acc: 0.9629 - val_loss: 0.1231 - val_acc: 0.9644\n",
      "Epoch 96/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1173 - acc: 0.9617 - val_loss: 0.1226 - val_acc: 0.9660\n",
      "Epoch 97/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1170 - acc: 0.9632 - val_loss: 0.1240 - val_acc: 0.9657\n",
      "Epoch 98/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1173 - acc: 0.9634 - val_loss: 0.1230 - val_acc: 0.9654\n",
      "Epoch 99/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1196 - acc: 0.9621 - val_loss: 0.1263 - val_acc: 0.9646\n",
      "Epoch 100/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1159 - acc: 0.9626 - val_loss: 0.1258 - val_acc: 0.9652\n",
      "Epoch 101/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1160 - acc: 0.9630 - val_loss: 0.1241 - val_acc: 0.9664\n",
      "Epoch 102/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1150 - acc: 0.9630 - val_loss: 0.1209 - val_acc: 0.9657\n",
      "Epoch 103/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1147 - acc: 0.9639 - val_loss: 0.1243 - val_acc: 0.9662\n",
      "Epoch 104/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1150 - acc: 0.9633 - val_loss: 0.1253 - val_acc: 0.9653\n",
      "Epoch 105/500\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.1139 - acc: 0.9627 - val_loss: 0.1235 - val_acc: 0.9658\n",
      "Epoch 106/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1143 - acc: 0.9634 - val_loss: 0.1265 - val_acc: 0.9656\n",
      "Epoch 107/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1175 - acc: 0.9632 - val_loss: 0.1230 - val_acc: 0.9647\n",
      "Epoch 108/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1125 - acc: 0.9644 - val_loss: 0.1225 - val_acc: 0.9653\n",
      "Epoch 109/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1148 - acc: 0.9637 - val_loss: 0.1254 - val_acc: 0.9649\n",
      "Epoch 110/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1132 - acc: 0.9636 - val_loss: 0.1248 - val_acc: 0.9648\n",
      "Epoch 111/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1171 - acc: 0.9619 - val_loss: 0.1226 - val_acc: 0.9656\n",
      "Epoch 112/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1140 - acc: 0.9627 - val_loss: 0.1261 - val_acc: 0.9668\n",
      "Epoch 113/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1138 - acc: 0.9633 - val_loss: 0.1222 - val_acc: 0.9662\n",
      "Epoch 114/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1118 - acc: 0.9638 - val_loss: 0.1239 - val_acc: 0.9669\n",
      "Epoch 115/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1107 - acc: 0.9646 - val_loss: 0.1243 - val_acc: 0.9654\n",
      "Epoch 116/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1147 - acc: 0.9631 - val_loss: 0.1262 - val_acc: 0.9655\n",
      "Epoch 117/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1140 - acc: 0.9632 - val_loss: 0.1266 - val_acc: 0.9650\n",
      "Epoch 118/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1087 - acc: 0.9655 - val_loss: 0.1252 - val_acc: 0.9647\n",
      "Epoch 119/500\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.1105 - acc: 0.9643 - val_loss: 0.1252 - val_acc: 0.9677\n",
      "Epoch 120/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1122 - acc: 0.9635 - val_loss: 0.1251 - val_acc: 0.9656\n",
      "Epoch 121/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1086 - acc: 0.9645 - val_loss: 0.1262 - val_acc: 0.9654\n",
      "Epoch 122/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1110 - acc: 0.9639 - val_loss: 0.1252 - val_acc: 0.9654\n",
      "Epoch 123/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1104 - acc: 0.9645 - val_loss: 0.1268 - val_acc: 0.9655\n",
      "Epoch 124/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1111 - acc: 0.9650 - val_loss: 0.1281 - val_acc: 0.9654\n",
      "Epoch 125/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1137 - acc: 0.9638 - val_loss: 0.1230 - val_acc: 0.9661\n",
      "Epoch 126/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1126 - acc: 0.9640 - val_loss: 0.1240 - val_acc: 0.9659\n",
      "Epoch 127/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1060 - acc: 0.9658 - val_loss: 0.1238 - val_acc: 0.9663\n",
      "Epoch 128/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1084 - acc: 0.9647 - val_loss: 0.1276 - val_acc: 0.9650\n",
      "Epoch 129/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1091 - acc: 0.9651 - val_loss: 0.1281 - val_acc: 0.9652\n",
      "Epoch 130/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1084 - acc: 0.9646 - val_loss: 0.1242 - val_acc: 0.9674\n",
      "Epoch 131/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1122 - acc: 0.9634 - val_loss: 0.1259 - val_acc: 0.9662\n",
      "Epoch 132/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1086 - acc: 0.9646 - val_loss: 0.1317 - val_acc: 0.9645\n",
      "Epoch 133/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1101 - acc: 0.9645 - val_loss: 0.1254 - val_acc: 0.9649\n",
      "Epoch 134/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1101 - acc: 0.9641 - val_loss: 0.1280 - val_acc: 0.9652\n",
      "Epoch 135/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1106 - acc: 0.9644 - val_loss: 0.1287 - val_acc: 0.9645\n",
      "Epoch 136/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1085 - acc: 0.9648 - val_loss: 0.1277 - val_acc: 0.9642\n",
      "Epoch 137/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1090 - acc: 0.9645 - val_loss: 0.1262 - val_acc: 0.9654\n",
      "Epoch 138/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1079 - acc: 0.9648 - val_loss: 0.1291 - val_acc: 0.9649\n",
      "Epoch 139/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1060 - acc: 0.9661 - val_loss: 0.1292 - val_acc: 0.9648\n",
      "Epoch 140/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1065 - acc: 0.9655 - val_loss: 0.1313 - val_acc: 0.9660\n",
      "Epoch 141/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1048 - acc: 0.9667 - val_loss: 0.1334 - val_acc: 0.9643\n",
      "Epoch 142/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1070 - acc: 0.9648 - val_loss: 0.1281 - val_acc: 0.9656\n",
      "Epoch 143/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1100 - acc: 0.9646 - val_loss: 0.1321 - val_acc: 0.9632\n",
      "Epoch 144/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1063 - acc: 0.9653 - val_loss: 0.1320 - val_acc: 0.9646\n",
      "Epoch 145/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1070 - acc: 0.9662 - val_loss: 0.1295 - val_acc: 0.9657\n",
      "Epoch 146/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1066 - acc: 0.9658 - val_loss: 0.1327 - val_acc: 0.9639\n",
      "Epoch 147/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1097 - acc: 0.9649 - val_loss: 0.1286 - val_acc: 0.9652\n",
      "Epoch 148/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1046 - acc: 0.9667 - val_loss: 0.1329 - val_acc: 0.9636\n",
      "Epoch 149/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1055 - acc: 0.9661 - val_loss: 0.1285 - val_acc: 0.9643\n",
      "Epoch 150/500\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.1071 - acc: 0.9649 - val_loss: 0.1279 - val_acc: 0.9652\n",
      "Epoch 151/500\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.1028 - acc: 0.9667 - val_loss: 0.1344 - val_acc: 0.9636\n",
      "Epoch 152/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1041 - acc: 0.9671 - val_loss: 0.1336 - val_acc: 0.9643\n",
      "Epoch 153/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1055 - acc: 0.9657 - val_loss: 0.1313 - val_acc: 0.9643\n",
      "Epoch 154/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1044 - acc: 0.9658 - val_loss: 0.1341 - val_acc: 0.9648\n",
      "Epoch 155/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1043 - acc: 0.9661 - val_loss: 0.1354 - val_acc: 0.9639\n",
      "Epoch 156/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1055 - acc: 0.9664 - val_loss: 0.1353 - val_acc: 0.9635\n",
      "Epoch 157/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1070 - acc: 0.9655 - val_loss: 0.1362 - val_acc: 0.9645\n",
      "Epoch 158/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1046 - acc: 0.9660 - val_loss: 0.1384 - val_acc: 0.9640\n",
      "Epoch 159/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1040 - acc: 0.9666 - val_loss: 0.1317 - val_acc: 0.9652\n",
      "Epoch 160/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1047 - acc: 0.9667 - val_loss: 0.1397 - val_acc: 0.9648\n",
      "Epoch 161/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1033 - acc: 0.9665 - val_loss: 0.1385 - val_acc: 0.9640\n",
      "Epoch 162/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1032 - acc: 0.9662 - val_loss: 0.1327 - val_acc: 0.9649\n",
      "Epoch 163/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1041 - acc: 0.9662 - val_loss: 0.1356 - val_acc: 0.9656\n",
      "Epoch 164/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1022 - acc: 0.9662 - val_loss: 0.1340 - val_acc: 0.9655\n",
      "Epoch 165/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1027 - acc: 0.9667 - val_loss: 0.1377 - val_acc: 0.9638\n",
      "Epoch 166/500\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.1033 - acc: 0.9670 - val_loss: 0.1373 - val_acc: 0.9630\n",
      "Epoch 167/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1044 - acc: 0.9658 - val_loss: 0.1333 - val_acc: 0.9643\n",
      "Epoch 168/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1012 - acc: 0.9667 - val_loss: 0.1337 - val_acc: 0.9639\n",
      "Epoch 169/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1026 - acc: 0.9664 - val_loss: 0.1389 - val_acc: 0.9637\n",
      "Epoch 170/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1040 - acc: 0.9666 - val_loss: 0.1353 - val_acc: 0.9634\n",
      "Epoch 171/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1027 - acc: 0.9663 - val_loss: 0.1367 - val_acc: 0.9634\n",
      "Epoch 172/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1039 - acc: 0.9667 - val_loss: 0.1347 - val_acc: 0.9643\n",
      "Epoch 173/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1014 - acc: 0.9671 - val_loss: 0.1390 - val_acc: 0.9632\n",
      "Epoch 174/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1035 - acc: 0.9667 - val_loss: 0.1373 - val_acc: 0.9637\n",
      "Epoch 175/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1017 - acc: 0.9665 - val_loss: 0.1337 - val_acc: 0.9653\n",
      "Epoch 176/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1030 - acc: 0.9667 - val_loss: 0.1373 - val_acc: 0.9634\n",
      "Epoch 177/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1040 - acc: 0.9656 - val_loss: 0.1367 - val_acc: 0.9636\n",
      "Epoch 178/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1015 - acc: 0.9677 - val_loss: 0.1341 - val_acc: 0.9647\n",
      "Epoch 179/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1031 - acc: 0.9671 - val_loss: 0.1415 - val_acc: 0.9639\n",
      "Epoch 180/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1009 - acc: 0.9670 - val_loss: 0.1348 - val_acc: 0.9652\n",
      "Epoch 181/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0978 - acc: 0.9676 - val_loss: 0.1351 - val_acc: 0.9646\n",
      "Epoch 182/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1010 - acc: 0.9670 - val_loss: 0.1355 - val_acc: 0.9641\n",
      "Epoch 183/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1029 - acc: 0.9663 - val_loss: 0.1367 - val_acc: 0.9635\n",
      "Epoch 184/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1004 - acc: 0.9674 - val_loss: 0.1386 - val_acc: 0.9645\n",
      "Epoch 185/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1021 - acc: 0.9672 - val_loss: 0.1367 - val_acc: 0.9640\n",
      "Epoch 186/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1027 - acc: 0.9676 - val_loss: 0.1379 - val_acc: 0.9634\n",
      "Epoch 187/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1015 - acc: 0.9673 - val_loss: 0.1356 - val_acc: 0.9631\n",
      "Epoch 188/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0990 - acc: 0.9671 - val_loss: 0.1398 - val_acc: 0.9634\n",
      "Epoch 189/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1005 - acc: 0.9672 - val_loss: 0.1367 - val_acc: 0.9633\n",
      "Epoch 190/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1001 - acc: 0.9679 - val_loss: 0.1378 - val_acc: 0.9631\n",
      "Epoch 191/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0997 - acc: 0.9676 - val_loss: 0.1395 - val_acc: 0.9650\n",
      "Epoch 192/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1012 - acc: 0.9671 - val_loss: 0.1355 - val_acc: 0.9639\n",
      "Epoch 193/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0971 - acc: 0.9684 - val_loss: 0.1380 - val_acc: 0.9652\n",
      "Epoch 194/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1004 - acc: 0.9677 - val_loss: 0.1378 - val_acc: 0.9633\n",
      "Epoch 195/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0994 - acc: 0.9677 - val_loss: 0.1386 - val_acc: 0.9640\n",
      "Epoch 196/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0968 - acc: 0.9683 - val_loss: 0.1400 - val_acc: 0.9647\n",
      "Epoch 197/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0969 - acc: 0.9680 - val_loss: 0.1399 - val_acc: 0.9631\n",
      "Epoch 198/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0992 - acc: 0.9672 - val_loss: 0.1430 - val_acc: 0.9641\n",
      "Epoch 199/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0984 - acc: 0.9677 - val_loss: 0.1378 - val_acc: 0.9638\n",
      "Epoch 200/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0974 - acc: 0.9686 - val_loss: 0.1458 - val_acc: 0.9633\n",
      "Epoch 201/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1007 - acc: 0.9670 - val_loss: 0.1368 - val_acc: 0.9644\n",
      "Epoch 202/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0986 - acc: 0.9682 - val_loss: 0.1372 - val_acc: 0.9640\n",
      "Epoch 203/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0980 - acc: 0.9675 - val_loss: 0.1399 - val_acc: 0.9642\n",
      "Epoch 204/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0976 - acc: 0.9689 - val_loss: 0.1384 - val_acc: 0.9645\n",
      "Epoch 205/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0963 - acc: 0.9694 - val_loss: 0.1432 - val_acc: 0.9641\n",
      "Epoch 206/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0976 - acc: 0.9679 - val_loss: 0.1401 - val_acc: 0.9632\n",
      "Epoch 207/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0969 - acc: 0.9685 - val_loss: 0.1437 - val_acc: 0.9641\n",
      "Epoch 208/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0982 - acc: 0.9687 - val_loss: 0.1419 - val_acc: 0.9630\n",
      "Epoch 209/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0959 - acc: 0.9691 - val_loss: 0.1395 - val_acc: 0.9633\n",
      "Epoch 210/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0983 - acc: 0.9674 - val_loss: 0.1440 - val_acc: 0.9629\n",
      "Epoch 211/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0973 - acc: 0.9683 - val_loss: 0.1426 - val_acc: 0.9634\n",
      "Epoch 212/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0975 - acc: 0.9678 - val_loss: 0.1413 - val_acc: 0.9628\n",
      "Epoch 213/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0957 - acc: 0.9691 - val_loss: 0.1392 - val_acc: 0.9632\n",
      "Epoch 214/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0973 - acc: 0.9681 - val_loss: 0.1409 - val_acc: 0.9636\n",
      "Epoch 215/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0972 - acc: 0.9675 - val_loss: 0.1450 - val_acc: 0.9640\n",
      "Epoch 216/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0984 - acc: 0.9673 - val_loss: 0.1427 - val_acc: 0.9639\n",
      "Epoch 217/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0959 - acc: 0.9685 - val_loss: 0.1427 - val_acc: 0.9632\n",
      "Epoch 218/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0955 - acc: 0.9685 - val_loss: 0.1485 - val_acc: 0.9627\n",
      "Epoch 219/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0967 - acc: 0.9686 - val_loss: 0.1415 - val_acc: 0.9631\n",
      "Epoch 220/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0984 - acc: 0.9677 - val_loss: 0.1449 - val_acc: 0.9630\n",
      "Epoch 221/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0985 - acc: 0.9679 - val_loss: 0.1489 - val_acc: 0.9629\n",
      "Epoch 222/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0960 - acc: 0.9689 - val_loss: 0.1449 - val_acc: 0.9638\n",
      "Epoch 223/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0941 - acc: 0.9692 - val_loss: 0.1440 - val_acc: 0.9636\n",
      "Epoch 224/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0977 - acc: 0.9679 - val_loss: 0.1469 - val_acc: 0.9631\n",
      "Epoch 225/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0960 - acc: 0.9684 - val_loss: 0.1407 - val_acc: 0.9635\n",
      "Epoch 226/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.0992 - acc: 0.9675 - val_loss: 0.1441 - val_acc: 0.9636\n",
      "Epoch 227/500\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0944 - acc: 0.9692 - val_loss: 0.1439 - val_acc: 0.9632\n",
      "Epoch 228/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.0982 - acc: 0.9678 - val_loss: 0.1464 - val_acc: 0.9625\n",
      "Epoch 229/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0954 - acc: 0.9689 - val_loss: 0.1469 - val_acc: 0.9629\n",
      "Epoch 230/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0950 - acc: 0.9679 - val_loss: 0.1497 - val_acc: 0.9631\n",
      "Epoch 231/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0990 - acc: 0.9683 - val_loss: 0.1445 - val_acc: 0.9632\n",
      "Epoch 232/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0977 - acc: 0.9689 - val_loss: 0.1479 - val_acc: 0.9624\n",
      "Epoch 233/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0941 - acc: 0.9700 - val_loss: 0.1504 - val_acc: 0.9623\n",
      "Epoch 234/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.0968 - acc: 0.9683 - val_loss: 0.1472 - val_acc: 0.9630\n",
      "Epoch 235/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0966 - acc: 0.9690 - val_loss: 0.1475 - val_acc: 0.9630\n",
      "Epoch 236/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0952 - acc: 0.9690 - val_loss: 0.1465 - val_acc: 0.9635\n",
      "Epoch 237/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0950 - acc: 0.9687 - val_loss: 0.1488 - val_acc: 0.9629\n",
      "Epoch 238/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0938 - acc: 0.9696 - val_loss: 0.1488 - val_acc: 0.9629\n",
      "Epoch 239/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0961 - acc: 0.9686 - val_loss: 0.1489 - val_acc: 0.9637\n",
      "Epoch 240/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0942 - acc: 0.9688 - val_loss: 0.1497 - val_acc: 0.9625\n",
      "Epoch 241/500\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0916 - acc: 0.9705 - val_loss: 0.1452 - val_acc: 0.9632\n",
      "Epoch 242/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0971 - acc: 0.9688 - val_loss: 0.1518 - val_acc: 0.9614\n",
      "Epoch 243/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0969 - acc: 0.9691 - val_loss: 0.1498 - val_acc: 0.9630\n",
      "Epoch 244/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0941 - acc: 0.9695 - val_loss: 0.1485 - val_acc: 0.9632\n",
      "Epoch 245/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0965 - acc: 0.9690 - val_loss: 0.1502 - val_acc: 0.9627\n",
      "Epoch 246/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0927 - acc: 0.9703 - val_loss: 0.1495 - val_acc: 0.9636\n",
      "Epoch 247/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0969 - acc: 0.9690 - val_loss: 0.1475 - val_acc: 0.9624\n",
      "Epoch 248/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0927 - acc: 0.9697 - val_loss: 0.1492 - val_acc: 0.9628\n",
      "Epoch 249/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0944 - acc: 0.9693 - val_loss: 0.1487 - val_acc: 0.9623\n",
      "Epoch 250/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0962 - acc: 0.9690 - val_loss: 0.1527 - val_acc: 0.9616\n",
      "Epoch 251/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0950 - acc: 0.9690 - val_loss: 0.1489 - val_acc: 0.9634\n",
      "Epoch 252/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0943 - acc: 0.9695 - val_loss: 0.1493 - val_acc: 0.9624\n",
      "Epoch 253/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0950 - acc: 0.9691 - val_loss: 0.1464 - val_acc: 0.9635\n",
      "Epoch 254/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0927 - acc: 0.9697 - val_loss: 0.1521 - val_acc: 0.9629\n",
      "Epoch 255/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.0930 - acc: 0.9701 - val_loss: 0.1507 - val_acc: 0.9628\n",
      "Epoch 256/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0953 - acc: 0.9688 - val_loss: 0.1522 - val_acc: 0.9631\n",
      "Epoch 257/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0928 - acc: 0.9700 - val_loss: 0.1517 - val_acc: 0.9621\n",
      "Epoch 258/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0943 - acc: 0.9686 - val_loss: 0.1498 - val_acc: 0.9632\n",
      "Epoch 259/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0931 - acc: 0.9700 - val_loss: 0.1452 - val_acc: 0.9635\n",
      "Epoch 260/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.0960 - acc: 0.9687 - val_loss: 0.1468 - val_acc: 0.9634\n",
      "Epoch 261/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0946 - acc: 0.9692 - val_loss: 0.1518 - val_acc: 0.9616\n",
      "Epoch 262/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0906 - acc: 0.9709 - val_loss: 0.1500 - val_acc: 0.9629\n",
      "Epoch 263/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0967 - acc: 0.9688 - val_loss: 0.1544 - val_acc: 0.9620\n",
      "Epoch 264/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0928 - acc: 0.9695 - val_loss: 0.1541 - val_acc: 0.9620\n",
      "Epoch 265/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0922 - acc: 0.9698 - val_loss: 0.1498 - val_acc: 0.9621\n",
      "Epoch 266/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0934 - acc: 0.9693 - val_loss: 0.1538 - val_acc: 0.9626\n",
      "Epoch 267/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0914 - acc: 0.9704 - val_loss: 0.1522 - val_acc: 0.9627\n",
      "Epoch 268/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0944 - acc: 0.9698 - val_loss: 0.1487 - val_acc: 0.9627\n",
      "Epoch 269/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0945 - acc: 0.9692 - val_loss: 0.1541 - val_acc: 0.9623\n",
      "Epoch 270/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0932 - acc: 0.9690 - val_loss: 0.1503 - val_acc: 0.9627\n",
      "Epoch 271/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0931 - acc: 0.9695 - val_loss: 0.1509 - val_acc: 0.9628\n",
      "Epoch 272/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0932 - acc: 0.9701 - val_loss: 0.1532 - val_acc: 0.9627\n",
      "Epoch 273/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0914 - acc: 0.9693 - val_loss: 0.1513 - val_acc: 0.9628\n",
      "Epoch 274/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0920 - acc: 0.9693 - val_loss: 0.1517 - val_acc: 0.9623\n",
      "Epoch 275/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0922 - acc: 0.9697 - val_loss: 0.1515 - val_acc: 0.9622\n",
      "Epoch 276/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0912 - acc: 0.9700 - val_loss: 0.1525 - val_acc: 0.9628\n",
      "Epoch 277/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0932 - acc: 0.9691 - val_loss: 0.1582 - val_acc: 0.9609\n",
      "Epoch 278/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0933 - acc: 0.9695 - val_loss: 0.1530 - val_acc: 0.9620\n",
      "Epoch 279/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0937 - acc: 0.9697 - val_loss: 0.1559 - val_acc: 0.9620\n",
      "Epoch 280/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0937 - acc: 0.9692 - val_loss: 0.1550 - val_acc: 0.9615\n",
      "Epoch 281/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0922 - acc: 0.9699 - val_loss: 0.1565 - val_acc: 0.9615\n",
      "Epoch 282/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0914 - acc: 0.9701 - val_loss: 0.1536 - val_acc: 0.9623\n",
      "Epoch 283/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0932 - acc: 0.9692 - val_loss: 0.1520 - val_acc: 0.9633\n",
      "Epoch 284/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0941 - acc: 0.9696 - val_loss: 0.1548 - val_acc: 0.9620\n",
      "Epoch 285/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0911 - acc: 0.9709 - val_loss: 0.1581 - val_acc: 0.9620\n",
      "Epoch 286/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0925 - acc: 0.9698 - val_loss: 0.1559 - val_acc: 0.9613\n",
      "Epoch 287/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0915 - acc: 0.9699 - val_loss: 0.1563 - val_acc: 0.9626\n",
      "Epoch 288/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0932 - acc: 0.9693 - val_loss: 0.1614 - val_acc: 0.9608\n",
      "Epoch 289/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0909 - acc: 0.9700 - val_loss: 0.1551 - val_acc: 0.9625\n",
      "Epoch 290/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0921 - acc: 0.9701 - val_loss: 0.1537 - val_acc: 0.9638\n",
      "Epoch 291/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0922 - acc: 0.9700 - val_loss: 0.1517 - val_acc: 0.9623\n",
      "Epoch 292/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0913 - acc: 0.9707 - val_loss: 0.1592 - val_acc: 0.9617\n",
      "Epoch 293/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0918 - acc: 0.9696 - val_loss: 0.1576 - val_acc: 0.9612\n",
      "Epoch 294/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0893 - acc: 0.9709 - val_loss: 0.1599 - val_acc: 0.9623\n",
      "Epoch 295/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0892 - acc: 0.9705 - val_loss: 0.1605 - val_acc: 0.9627\n",
      "Epoch 296/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0891 - acc: 0.9710 - val_loss: 0.1576 - val_acc: 0.9625\n",
      "Epoch 297/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0916 - acc: 0.9705 - val_loss: 0.1560 - val_acc: 0.9632\n",
      "Epoch 298/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0926 - acc: 0.9700 - val_loss: 0.1544 - val_acc: 0.9625\n",
      "Epoch 299/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0910 - acc: 0.9697 - val_loss: 0.1534 - val_acc: 0.9636\n",
      "Epoch 300/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0919 - acc: 0.9709 - val_loss: 0.1546 - val_acc: 0.9632\n",
      "Epoch 301/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0920 - acc: 0.9709 - val_loss: 0.1562 - val_acc: 0.9623\n",
      "Epoch 302/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0899 - acc: 0.9706 - val_loss: 0.1548 - val_acc: 0.9630\n",
      "Epoch 303/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0922 - acc: 0.9702 - val_loss: 0.1559 - val_acc: 0.9623\n",
      "Epoch 304/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0898 - acc: 0.9701 - val_loss: 0.1584 - val_acc: 0.9627\n",
      "Epoch 305/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0915 - acc: 0.9704 - val_loss: 0.1577 - val_acc: 0.9627\n",
      "Epoch 306/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0896 - acc: 0.9710 - val_loss: 0.1602 - val_acc: 0.9617\n",
      "Epoch 307/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0897 - acc: 0.9704 - val_loss: 0.1614 - val_acc: 0.9617\n",
      "Epoch 308/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0923 - acc: 0.9701 - val_loss: 0.1611 - val_acc: 0.9624\n",
      "Epoch 309/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0907 - acc: 0.9705 - val_loss: 0.1539 - val_acc: 0.9625\n",
      "Epoch 310/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0898 - acc: 0.9697 - val_loss: 0.1557 - val_acc: 0.9615\n",
      "Epoch 311/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0886 - acc: 0.9707 - val_loss: 0.1535 - val_acc: 0.9625\n",
      "Epoch 312/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0916 - acc: 0.9705 - val_loss: 0.1602 - val_acc: 0.9622\n",
      "Epoch 313/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0886 - acc: 0.9710 - val_loss: 0.1601 - val_acc: 0.9625\n",
      "Epoch 314/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0908 - acc: 0.9703 - val_loss: 0.1605 - val_acc: 0.9630\n",
      "Epoch 315/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0889 - acc: 0.9715 - val_loss: 0.1585 - val_acc: 0.9625\n",
      "Epoch 316/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0903 - acc: 0.9705 - val_loss: 0.1635 - val_acc: 0.9627\n",
      "Epoch 317/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0905 - acc: 0.9708 - val_loss: 0.1599 - val_acc: 0.9630\n",
      "Epoch 318/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0889 - acc: 0.9705 - val_loss: 0.1604 - val_acc: 0.9617\n",
      "Epoch 319/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0889 - acc: 0.9711 - val_loss: 0.1582 - val_acc: 0.9622\n",
      "Epoch 320/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0885 - acc: 0.9709 - val_loss: 0.1645 - val_acc: 0.9613\n",
      "Epoch 321/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0897 - acc: 0.9703 - val_loss: 0.1576 - val_acc: 0.9616\n",
      "Epoch 322/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0884 - acc: 0.9710 - val_loss: 0.1569 - val_acc: 0.9628\n",
      "Epoch 323/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0874 - acc: 0.9706 - val_loss: 0.1592 - val_acc: 0.9632\n",
      "Epoch 324/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0883 - acc: 0.9711 - val_loss: 0.1662 - val_acc: 0.9608\n",
      "Epoch 325/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0903 - acc: 0.9705 - val_loss: 0.1570 - val_acc: 0.9618\n",
      "Epoch 326/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0875 - acc: 0.9711 - val_loss: 0.1591 - val_acc: 0.9623\n",
      "Epoch 327/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0887 - acc: 0.9712 - val_loss: 0.1617 - val_acc: 0.9629\n",
      "Epoch 328/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0896 - acc: 0.9708 - val_loss: 0.1587 - val_acc: 0.9630\n",
      "Epoch 329/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0890 - acc: 0.9718 - val_loss: 0.1608 - val_acc: 0.9623\n",
      "Epoch 330/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0892 - acc: 0.9715 - val_loss: 0.1605 - val_acc: 0.9637\n",
      "Epoch 331/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0890 - acc: 0.9712 - val_loss: 0.1582 - val_acc: 0.9633\n",
      "Epoch 332/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0894 - acc: 0.9708 - val_loss: 0.1606 - val_acc: 0.9622\n",
      "Epoch 333/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0888 - acc: 0.9717 - val_loss: 0.1614 - val_acc: 0.9612\n",
      "Epoch 334/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0907 - acc: 0.9706 - val_loss: 0.1639 - val_acc: 0.9613\n",
      "Epoch 335/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0884 - acc: 0.9706 - val_loss: 0.1635 - val_acc: 0.9625\n",
      "Epoch 336/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0875 - acc: 0.9713 - val_loss: 0.1607 - val_acc: 0.9615\n",
      "Epoch 337/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0891 - acc: 0.9706 - val_loss: 0.1655 - val_acc: 0.9622\n",
      "Epoch 338/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0882 - acc: 0.9712 - val_loss: 0.1618 - val_acc: 0.9622\n",
      "Epoch 339/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0870 - acc: 0.9720 - val_loss: 0.1651 - val_acc: 0.9613\n",
      "Epoch 340/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0879 - acc: 0.9712 - val_loss: 0.1635 - val_acc: 0.9617\n",
      "Epoch 341/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0894 - acc: 0.9706 - val_loss: 0.1610 - val_acc: 0.9620\n",
      "Epoch 342/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0874 - acc: 0.9713 - val_loss: 0.1602 - val_acc: 0.9623\n",
      "Epoch 343/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.0895 - acc: 0.9712 - val_loss: 0.1635 - val_acc: 0.9618\n",
      "Epoch 344/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0872 - acc: 0.9716 - val_loss: 0.1662 - val_acc: 0.9622\n",
      "Epoch 345/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0889 - acc: 0.9713 - val_loss: 0.1638 - val_acc: 0.9611\n",
      "Epoch 346/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0886 - acc: 0.9712 - val_loss: 0.1590 - val_acc: 0.9609\n",
      "Epoch 347/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0845 - acc: 0.9729 - val_loss: 0.1586 - val_acc: 0.9616\n",
      "Epoch 348/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0879 - acc: 0.9714 - val_loss: 0.1610 - val_acc: 0.9617\n",
      "Epoch 349/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0889 - acc: 0.9701 - val_loss: 0.1620 - val_acc: 0.9615\n",
      "Epoch 350/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0868 - acc: 0.9715 - val_loss: 0.1671 - val_acc: 0.9609\n",
      "Epoch 351/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0868 - acc: 0.9714 - val_loss: 0.1637 - val_acc: 0.9628\n",
      "Epoch 352/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0877 - acc: 0.9711 - val_loss: 0.1651 - val_acc: 0.9622\n",
      "Epoch 353/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0869 - acc: 0.9716 - val_loss: 0.1656 - val_acc: 0.9621\n",
      "Epoch 354/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0876 - acc: 0.9719 - val_loss: 0.1633 - val_acc: 0.9623\n",
      "Epoch 355/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0886 - acc: 0.9714 - val_loss: 0.1660 - val_acc: 0.9616\n",
      "Epoch 356/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0838 - acc: 0.9728 - val_loss: 0.1675 - val_acc: 0.9608\n",
      "Epoch 357/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0868 - acc: 0.9716 - val_loss: 0.1705 - val_acc: 0.9610\n",
      "Epoch 358/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0873 - acc: 0.9719 - val_loss: 0.1646 - val_acc: 0.9615\n",
      "Epoch 359/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0877 - acc: 0.9711 - val_loss: 0.1672 - val_acc: 0.9619\n",
      "Epoch 360/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0866 - acc: 0.9719 - val_loss: 0.1661 - val_acc: 0.9621\n",
      "Epoch 361/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0855 - acc: 0.9723 - val_loss: 0.1698 - val_acc: 0.9609\n",
      "Epoch 362/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0841 - acc: 0.9727 - val_loss: 0.1625 - val_acc: 0.9622\n",
      "Epoch 363/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0871 - acc: 0.9720 - val_loss: 0.1644 - val_acc: 0.9617\n",
      "Epoch 364/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0849 - acc: 0.9729 - val_loss: 0.1677 - val_acc: 0.9615\n",
      "Epoch 365/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0883 - acc: 0.9708 - val_loss: 0.1634 - val_acc: 0.9614\n",
      "Epoch 366/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0868 - acc: 0.9721 - val_loss: 0.1641 - val_acc: 0.9626\n",
      "Epoch 367/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0858 - acc: 0.9718 - val_loss: 0.1637 - val_acc: 0.9613\n",
      "Epoch 368/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0876 - acc: 0.9719 - val_loss: 0.1628 - val_acc: 0.9614\n",
      "Epoch 369/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0853 - acc: 0.9724 - val_loss: 0.1673 - val_acc: 0.9608\n",
      "Epoch 370/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0864 - acc: 0.9718 - val_loss: 0.1702 - val_acc: 0.9610\n",
      "Epoch 371/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0869 - acc: 0.9726 - val_loss: 0.1661 - val_acc: 0.9611\n",
      "Epoch 372/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0871 - acc: 0.9710 - val_loss: 0.1632 - val_acc: 0.9618\n",
      "Epoch 373/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0879 - acc: 0.9718 - val_loss: 0.1726 - val_acc: 0.9608\n",
      "Epoch 374/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0879 - acc: 0.9713 - val_loss: 0.1667 - val_acc: 0.9607\n",
      "Epoch 375/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0868 - acc: 0.9717 - val_loss: 0.1630 - val_acc: 0.9615\n",
      "Epoch 376/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0867 - acc: 0.9715 - val_loss: 0.1719 - val_acc: 0.9611\n",
      "Epoch 377/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0869 - acc: 0.9707 - val_loss: 0.1668 - val_acc: 0.9612\n",
      "Epoch 378/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0881 - acc: 0.9712 - val_loss: 0.1722 - val_acc: 0.9608\n",
      "Epoch 379/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0870 - acc: 0.9713 - val_loss: 0.1657 - val_acc: 0.9627\n",
      "Epoch 380/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0850 - acc: 0.9724 - val_loss: 0.1712 - val_acc: 0.9614\n",
      "Epoch 381/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0905 - acc: 0.9706 - val_loss: 0.1645 - val_acc: 0.9621\n",
      "Epoch 382/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0871 - acc: 0.9722 - val_loss: 0.1711 - val_acc: 0.9614\n",
      "Epoch 383/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0868 - acc: 0.9714 - val_loss: 0.1682 - val_acc: 0.9600\n",
      "Epoch 384/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0854 - acc: 0.9722 - val_loss: 0.1656 - val_acc: 0.9619\n",
      "Epoch 385/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - acc: 0.9723 - val_loss: 0.1720 - val_acc: 0.9614\n",
      "Epoch 386/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0857 - acc: 0.9719 - val_loss: 0.1710 - val_acc: 0.9612\n",
      "Epoch 387/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0861 - acc: 0.9719 - val_loss: 0.1668 - val_acc: 0.9613\n",
      "Epoch 388/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0861 - acc: 0.9718 - val_loss: 0.1709 - val_acc: 0.9614\n",
      "Epoch 389/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.0858 - acc: 0.9721 - val_loss: 0.1681 - val_acc: 0.9618\n",
      "Epoch 390/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0867 - acc: 0.9715 - val_loss: 0.1643 - val_acc: 0.9613\n",
      "Epoch 391/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0859 - acc: 0.9723 - val_loss: 0.1707 - val_acc: 0.9611\n",
      "Epoch 392/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0865 - acc: 0.9717 - val_loss: 0.1644 - val_acc: 0.9620\n",
      "Epoch 393/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0865 - acc: 0.9722 - val_loss: 0.1760 - val_acc: 0.9614\n",
      "Epoch 394/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0819 - acc: 0.9728 - val_loss: 0.1712 - val_acc: 0.9603\n",
      "Epoch 395/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0887 - acc: 0.9716 - val_loss: 0.1670 - val_acc: 0.9598\n",
      "Epoch 396/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0866 - acc: 0.9726 - val_loss: 0.1683 - val_acc: 0.9619\n",
      "Epoch 397/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0856 - acc: 0.9718 - val_loss: 0.1691 - val_acc: 0.9619\n",
      "Epoch 398/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0851 - acc: 0.9719 - val_loss: 0.1704 - val_acc: 0.9616\n",
      "Epoch 399/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0874 - acc: 0.9713 - val_loss: 0.1719 - val_acc: 0.9617\n",
      "Epoch 400/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0850 - acc: 0.9722 - val_loss: 0.1743 - val_acc: 0.9611\n",
      "Epoch 401/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0845 - acc: 0.9722 - val_loss: 0.1673 - val_acc: 0.9621\n",
      "Epoch 402/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0842 - acc: 0.9728 - val_loss: 0.1712 - val_acc: 0.9619\n",
      "Epoch 403/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0814 - acc: 0.9730 - val_loss: 0.1723 - val_acc: 0.9601\n",
      "Epoch 404/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0844 - acc: 0.9731 - val_loss: 0.1713 - val_acc: 0.9611\n",
      "Epoch 405/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0849 - acc: 0.9725 - val_loss: 0.1734 - val_acc: 0.9619\n",
      "Epoch 406/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0834 - acc: 0.9722 - val_loss: 0.1733 - val_acc: 0.9602\n",
      "Epoch 407/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0839 - acc: 0.9731 - val_loss: 0.1755 - val_acc: 0.9623\n",
      "Epoch 408/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0847 - acc: 0.9727 - val_loss: 0.1727 - val_acc: 0.9616\n",
      "Epoch 409/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0872 - acc: 0.9717 - val_loss: 0.1724 - val_acc: 0.9598\n",
      "Epoch 410/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0841 - acc: 0.9726 - val_loss: 0.1745 - val_acc: 0.9609\n",
      "Epoch 411/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0853 - acc: 0.9721 - val_loss: 0.1685 - val_acc: 0.9619\n",
      "Epoch 412/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0841 - acc: 0.9721 - val_loss: 0.1737 - val_acc: 0.9605\n",
      "Epoch 413/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0837 - acc: 0.9726 - val_loss: 0.1736 - val_acc: 0.9603\n",
      "Epoch 414/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0847 - acc: 0.9722 - val_loss: 0.1706 - val_acc: 0.9613\n",
      "Epoch 415/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0835 - acc: 0.9730 - val_loss: 0.1734 - val_acc: 0.9616\n",
      "Epoch 416/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0835 - acc: 0.9727 - val_loss: 0.1736 - val_acc: 0.9618\n",
      "Epoch 417/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0862 - acc: 0.9722 - val_loss: 0.1705 - val_acc: 0.9615\n",
      "Epoch 418/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0859 - acc: 0.9719 - val_loss: 0.1769 - val_acc: 0.9600\n",
      "Epoch 419/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0841 - acc: 0.9725 - val_loss: 0.1727 - val_acc: 0.9607\n",
      "Epoch 420/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0865 - acc: 0.9722 - val_loss: 0.1747 - val_acc: 0.9618\n",
      "Epoch 421/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0829 - acc: 0.9733 - val_loss: 0.1756 - val_acc: 0.9617\n",
      "Epoch 422/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0836 - acc: 0.9726 - val_loss: 0.1717 - val_acc: 0.9615\n",
      "Epoch 423/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0833 - acc: 0.9726 - val_loss: 0.1767 - val_acc: 0.9615\n",
      "Epoch 424/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0825 - acc: 0.9733 - val_loss: 0.1763 - val_acc: 0.9614\n",
      "Epoch 425/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0836 - acc: 0.9732 - val_loss: 0.1759 - val_acc: 0.9613\n",
      "Epoch 426/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0846 - acc: 0.9723 - val_loss: 0.1727 - val_acc: 0.9618\n",
      "Epoch 427/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0870 - acc: 0.9723 - val_loss: 0.1738 - val_acc: 0.9617\n",
      "Epoch 428/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0835 - acc: 0.9723 - val_loss: 0.1749 - val_acc: 0.9614\n",
      "Epoch 429/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0812 - acc: 0.9735 - val_loss: 0.1746 - val_acc: 0.9611\n",
      "Epoch 430/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0844 - acc: 0.9726 - val_loss: 0.1712 - val_acc: 0.9610\n",
      "Epoch 431/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0849 - acc: 0.9731 - val_loss: 0.1743 - val_acc: 0.9603\n",
      "Epoch 432/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0847 - acc: 0.9729 - val_loss: 0.1756 - val_acc: 0.9613\n",
      "Epoch 433/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0824 - acc: 0.9726 - val_loss: 0.1794 - val_acc: 0.9614\n",
      "Epoch 434/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0811 - acc: 0.9733 - val_loss: 0.1723 - val_acc: 0.9610\n",
      "Epoch 435/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0845 - acc: 0.9716 - val_loss: 0.1740 - val_acc: 0.9603\n",
      "Epoch 436/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.0822 - acc: 0.9730 - val_loss: 0.1782 - val_acc: 0.9608\n",
      "Epoch 437/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0856 - acc: 0.9726 - val_loss: 0.1752 - val_acc: 0.9612\n",
      "Epoch 438/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.0833 - acc: 0.9726 - val_loss: 0.1747 - val_acc: 0.9608\n",
      "Epoch 439/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0829 - acc: 0.9721 - val_loss: 0.1809 - val_acc: 0.9621\n",
      "Epoch 440/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0826 - acc: 0.9731 - val_loss: 0.1818 - val_acc: 0.9608\n",
      "Epoch 441/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0853 - acc: 0.9718 - val_loss: 0.1780 - val_acc: 0.9601\n",
      "Epoch 442/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0819 - acc: 0.9729 - val_loss: 0.1803 - val_acc: 0.9597\n",
      "Epoch 443/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0814 - acc: 0.9733 - val_loss: 0.1767 - val_acc: 0.9613\n",
      "Epoch 444/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0852 - acc: 0.9719 - val_loss: 0.1793 - val_acc: 0.9603\n",
      "Epoch 445/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0839 - acc: 0.9731 - val_loss: 0.1786 - val_acc: 0.9611\n",
      "Epoch 446/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0846 - acc: 0.9719 - val_loss: 0.1790 - val_acc: 0.9610\n",
      "Epoch 447/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0834 - acc: 0.9723 - val_loss: 0.1774 - val_acc: 0.9619\n",
      "Epoch 448/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0853 - acc: 0.9722 - val_loss: 0.1791 - val_acc: 0.9613\n",
      "Epoch 449/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0837 - acc: 0.9721 - val_loss: 0.1807 - val_acc: 0.9609\n",
      "Epoch 450/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0844 - acc: 0.9732 - val_loss: 0.1780 - val_acc: 0.9619\n",
      "Epoch 451/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0843 - acc: 0.9732 - val_loss: 0.1777 - val_acc: 0.9612\n",
      "Epoch 452/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0806 - acc: 0.9738 - val_loss: 0.1760 - val_acc: 0.9612\n",
      "Epoch 453/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0843 - acc: 0.9719 - val_loss: 0.1756 - val_acc: 0.9614\n",
      "Epoch 454/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0836 - acc: 0.9731 - val_loss: 0.1810 - val_acc: 0.9610\n",
      "Epoch 455/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0822 - acc: 0.9731 - val_loss: 0.1751 - val_acc: 0.9609\n",
      "Epoch 456/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0830 - acc: 0.9732 - val_loss: 0.1767 - val_acc: 0.9602\n",
      "Epoch 457/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0843 - acc: 0.9731 - val_loss: 0.1743 - val_acc: 0.9614\n",
      "Epoch 458/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0839 - acc: 0.9721 - val_loss: 0.1788 - val_acc: 0.9611\n",
      "Epoch 459/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0824 - acc: 0.9727 - val_loss: 0.1773 - val_acc: 0.9607\n",
      "Epoch 460/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0837 - acc: 0.9729 - val_loss: 0.1767 - val_acc: 0.9610\n",
      "Epoch 461/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0838 - acc: 0.9729 - val_loss: 0.1814 - val_acc: 0.9607\n",
      "Epoch 462/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0834 - acc: 0.9719 - val_loss: 0.1737 - val_acc: 0.9613\n",
      "Epoch 463/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0840 - acc: 0.9728 - val_loss: 0.1766 - val_acc: 0.9614\n",
      "Epoch 464/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0833 - acc: 0.9727 - val_loss: 0.1764 - val_acc: 0.9595\n",
      "Epoch 465/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0831 - acc: 0.9736 - val_loss: 0.1801 - val_acc: 0.9605\n",
      "Epoch 466/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0819 - acc: 0.9733 - val_loss: 0.1798 - val_acc: 0.9607\n",
      "Epoch 467/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0830 - acc: 0.9731 - val_loss: 0.1776 - val_acc: 0.9612\n",
      "Epoch 468/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0813 - acc: 0.9731 - val_loss: 0.1793 - val_acc: 0.9602\n",
      "Epoch 469/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0799 - acc: 0.9744 - val_loss: 0.1787 - val_acc: 0.9609\n",
      "Epoch 470/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0852 - acc: 0.9722 - val_loss: 0.1832 - val_acc: 0.9600\n",
      "Epoch 471/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0822 - acc: 0.9736 - val_loss: 0.1798 - val_acc: 0.9605\n",
      "Epoch 472/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0822 - acc: 0.9721 - val_loss: 0.1783 - val_acc: 0.9615\n",
      "Epoch 473/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0829 - acc: 0.9729 - val_loss: 0.1769 - val_acc: 0.9608\n",
      "Epoch 474/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0831 - acc: 0.9734 - val_loss: 0.1815 - val_acc: 0.9605\n",
      "Epoch 475/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0823 - acc: 0.9737 - val_loss: 0.1820 - val_acc: 0.9597\n",
      "Epoch 476/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0822 - acc: 0.9731 - val_loss: 0.1746 - val_acc: 0.9621\n",
      "Epoch 477/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0841 - acc: 0.9723 - val_loss: 0.1790 - val_acc: 0.9605\n",
      "Epoch 478/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0827 - acc: 0.9732 - val_loss: 0.1829 - val_acc: 0.9608\n",
      "Epoch 479/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0825 - acc: 0.9734 - val_loss: 0.1850 - val_acc: 0.9604\n",
      "Epoch 480/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0808 - acc: 0.9738 - val_loss: 0.1820 - val_acc: 0.9605\n",
      "Epoch 481/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0822 - acc: 0.9732 - val_loss: 0.1742 - val_acc: 0.9607\n",
      "Epoch 482/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0813 - acc: 0.9728 - val_loss: 0.1784 - val_acc: 0.9603\n",
      "Epoch 483/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0798 - acc: 0.9740 - val_loss: 0.1818 - val_acc: 0.9612\n",
      "Epoch 484/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0814 - acc: 0.9731 - val_loss: 0.1788 - val_acc: 0.9608\n",
      "Epoch 485/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0818 - acc: 0.9735 - val_loss: 0.1823 - val_acc: 0.9606\n",
      "Epoch 486/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0821 - acc: 0.9724 - val_loss: 0.1837 - val_acc: 0.9610\n",
      "Epoch 487/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0830 - acc: 0.9733 - val_loss: 0.1789 - val_acc: 0.9593\n",
      "Epoch 488/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0842 - acc: 0.9730 - val_loss: 0.1797 - val_acc: 0.9596\n",
      "Epoch 489/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0828 - acc: 0.9728 - val_loss: 0.1830 - val_acc: 0.9604\n",
      "Epoch 490/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0820 - acc: 0.9735 - val_loss: 0.1795 - val_acc: 0.9604\n",
      "Epoch 491/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0801 - acc: 0.9732 - val_loss: 0.1787 - val_acc: 0.9612\n",
      "Epoch 492/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0814 - acc: 0.9729 - val_loss: 0.1841 - val_acc: 0.9614\n",
      "Epoch 493/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0815 - acc: 0.9731 - val_loss: 0.1803 - val_acc: 0.9610\n",
      "Epoch 494/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0825 - acc: 0.9731 - val_loss: 0.1846 - val_acc: 0.9602\n",
      "Epoch 495/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.0812 - acc: 0.9735 - val_loss: 0.1773 - val_acc: 0.9621\n",
      "Epoch 496/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0797 - acc: 0.9734 - val_loss: 0.1854 - val_acc: 0.9594\n",
      "Epoch 497/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0847 - acc: 0.9725 - val_loss: 0.1792 - val_acc: 0.9594\n",
      "Epoch 498/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0793 - acc: 0.9744 - val_loss: 0.1842 - val_acc: 0.9614\n",
      "Epoch 499/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0824 - acc: 0.9730 - val_loss: 0.1826 - val_acc: 0.9597\n",
      "Epoch 500/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0801 - acc: 0.9749 - val_loss: 0.1856 - val_acc: 0.9600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3c04761f98>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_dir = 'logs/loss_function_a/'\n",
    "logging = TensorBoard(log_dir=log_dir)\n",
    "checkpoint = ModelCheckpoint(log_dir + 'ep{epoch:03d}-acc{acc:.4f}-val_acc{val_acc:.4f}-loss{loss:.4f}-val_loss{val_loss:.4f}.h5',\n",
    "        monitor='val_acc', save_weights_only=True, save_best_only=True, period=3)\n",
    "\n",
    "history = studentA.fit(X_train, Y_train_new,\n",
    "                      batch_size=256,\n",
    "                      epochs=epochs,\n",
    "                      verbose=1,\n",
    "                      validation_data=(X_test, Y_test_new),\n",
    "            callbacks=[logging,checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_acc = history.history['acc'][-1]\n",
    "last_val_acc = history.history['val_acc'][-1]\n",
    "last_loss = history.history['loss'][-1]\n",
    "last_val_loss = history.history['val_loss'][-1]\n",
    "\n",
    "hist = \"acc{0:.4f}-val_acc{0:.4f}-loss{0:.4f}-val_loss{0:.4f}\".format(last_acc,last_val_acc,last_loss,last_val_loss)\n",
    "studentA.save_weights(log_dir + \"last_\"+ hist + \".h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DS80ohB0rbz9"
   },
   "source": [
    "# StudentB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "colab_type": "code",
    "id": "P-wb1CtCruYw",
    "outputId": "31cb533d-e088-48b0-c321-6ed0e38f32f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_6 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 25,450\n",
      "Trainable params: 25,450\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "studentB = Sequential()\n",
    "studentB.add(Flatten(input_shape=input_shape))\n",
    "studentB.add(Dense(32, activation='relu'))\n",
    "studentB.add(Dropout(0.2))\n",
    "studentB.add(Dense(nb_classes))\n",
    "studentB.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "##sgd = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "studentB.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy'])\n",
    "#studentB = Model(student.input,student.output)\n",
    "studentB.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "colab_type": "code",
    "id": "SBoelaeMmCim",
    "outputId": "71504111-ec8a-400d-ba09-4a942e56d40a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "flatten_6_input (InputLayer)    (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 784)          0           flatten_6_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 32)           25120       flatten_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 32)           0           dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 10)           330         dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 10)           0           dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 10)           0           dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 20)           0           activation_9[0][0]               \n",
      "                                                                 activation_10[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 25,450\n",
      "Trainable params: 25,450\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Remove the softmax layer from the student network\n",
    "#student.layers.pop()\n",
    "\n",
    "# Now collect the logits from the last layer\n",
    "logits = studentB.layers[-2].output # This is going to be a tensor. And hence it needs to pass through a Activation layer\n",
    "probs = Activation('softmax')(logits)\n",
    "\n",
    "# softed probabilities at raised temperature\n",
    "#logits_T = Lambda(lambda x: x / temp)(logits)\n",
    "probs_T = Activation('softmax')(logits)#(logits_T)\n",
    "\n",
    "output = concatenate([probs, probs_T])\n",
    "\n",
    "\n",
    "# This is our new student model\n",
    "studentB = Model(studentB.input, output)\n",
    "\n",
    "studentB.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.08061464, 0.10786386, 0.12631465, 0.07207894, 0.08930514,\n",
       "        0.09400028, 0.08867763, 0.09583522, 0.07001819, 0.1752915 ,\n",
       "        0.08061464, 0.10786386, 0.12631465, 0.07207894, 0.08930514,\n",
       "        0.09400028, 0.08867763, 0.09583522, 0.07001819, 0.1752915 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "studentB.predict( X_train[0].reshape(1,28,28,1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l5w93LM2kHh5"
   },
   "outputs": [],
   "source": [
    "# This will be a teacher trained student model. \n",
    "# --> This uses a knowledge distillation loss function\n",
    "\n",
    "# Declare knowledge distillation loss\n",
    "def knowledge_distillation_loss(y_true, y_pred, alpha,beta,gamma):\n",
    "\n",
    "    # Extract the one-hot encoded values and the softs separately so that we can create two objective functions\n",
    "    y_true, y_logits = y_true[: , :nb_classes], y_true[: , nb_classes:]\n",
    "    \n",
    "    y_pred, y_pred_softs = y_pred[: , :nb_classes], y_pred[: , nb_classes:]\n",
    "    \n",
    "    loss = ( alpha*logloss(y_true,y_logits) ) + ( beta*logloss(y_true, y_pred) ) +( gamma*logloss(y_logits, y_pred) )\n",
    "   \n",
    "    return loss\n",
    "\n",
    "# For testing use regular output probabilities - without temperature\n",
    "def acc(y_true, y_pred):\n",
    "    y_true = y_true[:, :nb_classes]\n",
    "    y_pred = y_pred[:, :nb_classes]\n",
    "    return categorical_accuracy(y_true, y_pred)\n",
    "  \n",
    "# For testing use regular output probabilities - without temperature\n",
    "def teacher_loss(y_true, y_pred):\n",
    "    y_true = y_true[:, :nb_classes]\n",
    "    y_pred = y_true[:, nb_classes:]\n",
    "    return logloss(y_true, y_pred)\n",
    "  \n",
    "def student_loss(y_true, y_pred):\n",
    "    y_true = y_true[:, :nb_classes]\n",
    "    y_pred = y_pred[:, :nb_classes]\n",
    "    return logloss(y_true, y_pred)\n",
    "\n",
    "def apprentice_loss(y_true, y_pred):\n",
    "    y_true = y_true[:, nb_classes:]\n",
    "    y_pred = y_pred[:, :nb_classes]\n",
    "    return logloss(y_true, y_pred)\n",
    "\n",
    "studentB.compile(\n",
    "    #optimizer=optimizers.SGD(lr=1e-1, momentum=0.9, nesterov=True),\n",
    "    optimizer='adadelta',\n",
    "    loss=lambda y_true, y_pred: knowledge_distillation_loss(y_true, y_pred, 1,0.5,0.5),\n",
    "    #loss='categorical_crossentropy',\n",
    "    metrics=[acc] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "colab_type": "code",
    "id": "NG14ErXnmbh3",
    "outputId": "88d76943-3d05-405f-aa5c-3bf22df5940c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/500\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.7071 - acc: 0.7963 - val_loss: 0.3558 - val_acc: 0.9098\n",
      "Epoch 2/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.3871 - acc: 0.8859 - val_loss: 0.2815 - val_acc: 0.9270\n",
      "Epoch 3/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.3260 - acc: 0.9042 - val_loss: 0.2491 - val_acc: 0.9349\n",
      "Epoch 4/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.2942 - acc: 0.9132 - val_loss: 0.2280 - val_acc: 0.9389\n",
      "Epoch 5/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.2684 - acc: 0.9197 - val_loss: 0.2153 - val_acc: 0.9435\n",
      "Epoch 6/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.2519 - acc: 0.9248 - val_loss: 0.1991 - val_acc: 0.9480\n",
      "Epoch 7/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.2403 - acc: 0.9281 - val_loss: 0.1913 - val_acc: 0.9499\n",
      "Epoch 8/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.2297 - acc: 0.9312 - val_loss: 0.1834 - val_acc: 0.9529\n",
      "Epoch 9/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.2181 - acc: 0.9340 - val_loss: 0.1744 - val_acc: 0.9565\n",
      "Epoch 10/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.2124 - acc: 0.9361 - val_loss: 0.1744 - val_acc: 0.9544\n",
      "Epoch 11/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.2050 - acc: 0.9378 - val_loss: 0.1685 - val_acc: 0.9578\n",
      "Epoch 12/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1995 - acc: 0.9390 - val_loss: 0.1652 - val_acc: 0.9592\n",
      "Epoch 13/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1981 - acc: 0.9400 - val_loss: 0.1621 - val_acc: 0.9602\n",
      "Epoch 14/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1895 - acc: 0.9418 - val_loss: 0.1588 - val_acc: 0.9620\n",
      "Epoch 15/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1861 - acc: 0.9435 - val_loss: 0.1564 - val_acc: 0.9629\n",
      "Epoch 16/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1818 - acc: 0.9443 - val_loss: 0.1546 - val_acc: 0.9624\n",
      "Epoch 17/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1793 - acc: 0.9461 - val_loss: 0.1572 - val_acc: 0.9620\n",
      "Epoch 18/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1773 - acc: 0.9464 - val_loss: 0.1513 - val_acc: 0.9628\n",
      "Epoch 19/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1753 - acc: 0.9467 - val_loss: 0.1506 - val_acc: 0.9638\n",
      "Epoch 20/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1723 - acc: 0.9472 - val_loss: 0.1480 - val_acc: 0.9650\n",
      "Epoch 21/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1685 - acc: 0.9491 - val_loss: 0.1496 - val_acc: 0.9645\n",
      "Epoch 22/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1672 - acc: 0.9483 - val_loss: 0.1480 - val_acc: 0.9660\n",
      "Epoch 23/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1641 - acc: 0.9485 - val_loss: 0.1461 - val_acc: 0.9643\n",
      "Epoch 24/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1619 - acc: 0.9508 - val_loss: 0.1457 - val_acc: 0.9654\n",
      "Epoch 25/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1604 - acc: 0.9505 - val_loss: 0.1498 - val_acc: 0.9628\n",
      "Epoch 26/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1585 - acc: 0.9513 - val_loss: 0.1441 - val_acc: 0.9648\n",
      "Epoch 27/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1570 - acc: 0.9524 - val_loss: 0.1448 - val_acc: 0.9664\n",
      "Epoch 28/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1567 - acc: 0.9520 - val_loss: 0.1405 - val_acc: 0.9655\n",
      "Epoch 29/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1538 - acc: 0.9520 - val_loss: 0.1404 - val_acc: 0.9654\n",
      "Epoch 30/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1522 - acc: 0.9524 - val_loss: 0.1381 - val_acc: 0.9670\n",
      "Epoch 31/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1523 - acc: 0.9526 - val_loss: 0.1403 - val_acc: 0.9669\n",
      "Epoch 32/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1499 - acc: 0.9531 - val_loss: 0.1392 - val_acc: 0.9663\n",
      "Epoch 33/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1504 - acc: 0.9528 - val_loss: 0.1383 - val_acc: 0.9680\n",
      "Epoch 34/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1488 - acc: 0.9537 - val_loss: 0.1381 - val_acc: 0.9686\n",
      "Epoch 35/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1489 - acc: 0.9531 - val_loss: 0.1368 - val_acc: 0.9670\n",
      "Epoch 36/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1455 - acc: 0.9554 - val_loss: 0.1385 - val_acc: 0.9675\n",
      "Epoch 37/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1462 - acc: 0.9538 - val_loss: 0.1388 - val_acc: 0.9672\n",
      "Epoch 38/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1437 - acc: 0.9553 - val_loss: 0.1369 - val_acc: 0.9678\n",
      "Epoch 39/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1440 - acc: 0.9547 - val_loss: 0.1391 - val_acc: 0.9681\n",
      "Epoch 40/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1432 - acc: 0.9547 - val_loss: 0.1397 - val_acc: 0.9670\n",
      "Epoch 41/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1422 - acc: 0.9557 - val_loss: 0.1384 - val_acc: 0.9675\n",
      "Epoch 42/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1399 - acc: 0.9557 - val_loss: 0.1377 - val_acc: 0.9674\n",
      "Epoch 43/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1400 - acc: 0.9561 - val_loss: 0.1381 - val_acc: 0.9673\n",
      "Epoch 44/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1382 - acc: 0.9562 - val_loss: 0.1357 - val_acc: 0.9689\n",
      "Epoch 45/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1370 - acc: 0.9556 - val_loss: 0.1343 - val_acc: 0.9683\n",
      "Epoch 46/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1368 - acc: 0.9567 - val_loss: 0.1355 - val_acc: 0.9696\n",
      "Epoch 47/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1366 - acc: 0.9572 - val_loss: 0.1369 - val_acc: 0.9685\n",
      "Epoch 48/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1359 - acc: 0.9577 - val_loss: 0.1353 - val_acc: 0.9687\n",
      "Epoch 49/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1377 - acc: 0.9566 - val_loss: 0.1365 - val_acc: 0.9686\n",
      "Epoch 50/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1325 - acc: 0.9582 - val_loss: 0.1353 - val_acc: 0.9676\n",
      "Epoch 51/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1355 - acc: 0.9572 - val_loss: 0.1360 - val_acc: 0.9691\n",
      "Epoch 52/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1331 - acc: 0.9577 - val_loss: 0.1351 - val_acc: 0.9677\n",
      "Epoch 53/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1312 - acc: 0.9592 - val_loss: 0.1365 - val_acc: 0.9689\n",
      "Epoch 54/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1321 - acc: 0.9587 - val_loss: 0.1358 - val_acc: 0.9678\n",
      "Epoch 55/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1299 - acc: 0.9598 - val_loss: 0.1377 - val_acc: 0.9689\n",
      "Epoch 56/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1303 - acc: 0.9577 - val_loss: 0.1369 - val_acc: 0.9677\n",
      "Epoch 57/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1294 - acc: 0.9584 - val_loss: 0.1366 - val_acc: 0.9683\n",
      "Epoch 58/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1287 - acc: 0.9585 - val_loss: 0.1393 - val_acc: 0.9681\n",
      "Epoch 59/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1267 - acc: 0.9598 - val_loss: 0.1360 - val_acc: 0.9684\n",
      "Epoch 60/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1285 - acc: 0.9591 - val_loss: 0.1354 - val_acc: 0.9695\n",
      "Epoch 61/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1258 - acc: 0.9600 - val_loss: 0.1345 - val_acc: 0.9704\n",
      "Epoch 62/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1295 - acc: 0.9589 - val_loss: 0.1350 - val_acc: 0.9684\n",
      "Epoch 63/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1251 - acc: 0.9603 - val_loss: 0.1346 - val_acc: 0.9696\n",
      "Epoch 64/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1295 - acc: 0.9593 - val_loss: 0.1348 - val_acc: 0.9690\n",
      "Epoch 65/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1235 - acc: 0.9611 - val_loss: 0.1354 - val_acc: 0.9683\n",
      "Epoch 66/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1246 - acc: 0.9610 - val_loss: 0.1356 - val_acc: 0.9687\n",
      "Epoch 67/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1243 - acc: 0.9609 - val_loss: 0.1362 - val_acc: 0.9691\n",
      "Epoch 68/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1249 - acc: 0.9604 - val_loss: 0.1362 - val_acc: 0.9685\n",
      "Epoch 69/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1235 - acc: 0.9607 - val_loss: 0.1347 - val_acc: 0.9697\n",
      "Epoch 70/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1271 - acc: 0.9595 - val_loss: 0.1337 - val_acc: 0.9692\n",
      "Epoch 71/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1240 - acc: 0.9607 - val_loss: 0.1360 - val_acc: 0.9692\n",
      "Epoch 72/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1229 - acc: 0.9602 - val_loss: 0.1378 - val_acc: 0.9691\n",
      "Epoch 73/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1239 - acc: 0.9605 - val_loss: 0.1364 - val_acc: 0.9698\n",
      "Epoch 74/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1212 - acc: 0.9608 - val_loss: 0.1357 - val_acc: 0.9701\n",
      "Epoch 75/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1222 - acc: 0.9609 - val_loss: 0.1363 - val_acc: 0.9690\n",
      "Epoch 76/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1197 - acc: 0.9618 - val_loss: 0.1396 - val_acc: 0.9691\n",
      "Epoch 77/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1194 - acc: 0.9618 - val_loss: 0.1377 - val_acc: 0.9695\n",
      "Epoch 78/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1183 - acc: 0.9619 - val_loss: 0.1388 - val_acc: 0.9687\n",
      "Epoch 79/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1222 - acc: 0.9611 - val_loss: 0.1380 - val_acc: 0.9696\n",
      "Epoch 80/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1184 - acc: 0.9620 - val_loss: 0.1378 - val_acc: 0.9697\n",
      "Epoch 81/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1194 - acc: 0.9616 - val_loss: 0.1380 - val_acc: 0.9694\n",
      "Epoch 82/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1192 - acc: 0.9616 - val_loss: 0.1361 - val_acc: 0.9688\n",
      "Epoch 83/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1187 - acc: 0.9623 - val_loss: 0.1396 - val_acc: 0.9679\n",
      "Epoch 84/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1185 - acc: 0.9623 - val_loss: 0.1385 - val_acc: 0.9687\n",
      "Epoch 85/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1173 - acc: 0.9624 - val_loss: 0.1388 - val_acc: 0.9692\n",
      "Epoch 86/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1173 - acc: 0.9623 - val_loss: 0.1383 - val_acc: 0.9697\n",
      "Epoch 87/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1175 - acc: 0.9622 - val_loss: 0.1396 - val_acc: 0.9683\n",
      "Epoch 88/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1186 - acc: 0.9619 - val_loss: 0.1385 - val_acc: 0.9699\n",
      "Epoch 89/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1159 - acc: 0.9624 - val_loss: 0.1381 - val_acc: 0.9693\n",
      "Epoch 90/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1167 - acc: 0.9626 - val_loss: 0.1392 - val_acc: 0.9688\n",
      "Epoch 91/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1155 - acc: 0.9622 - val_loss: 0.1388 - val_acc: 0.9690\n",
      "Epoch 92/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1163 - acc: 0.9628 - val_loss: 0.1378 - val_acc: 0.9698\n",
      "Epoch 93/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.1177 - acc: 0.9619 - val_loss: 0.1391 - val_acc: 0.9686\n",
      "Epoch 94/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1163 - acc: 0.9626 - val_loss: 0.1384 - val_acc: 0.9701\n",
      "Epoch 95/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1156 - acc: 0.9622 - val_loss: 0.1390 - val_acc: 0.9684\n",
      "Epoch 96/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1137 - acc: 0.9632 - val_loss: 0.1416 - val_acc: 0.9681\n",
      "Epoch 97/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1122 - acc: 0.9637 - val_loss: 0.1389 - val_acc: 0.9692\n",
      "Epoch 98/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1137 - acc: 0.9635 - val_loss: 0.1362 - val_acc: 0.9694\n",
      "Epoch 99/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1122 - acc: 0.9641 - val_loss: 0.1376 - val_acc: 0.9708\n",
      "Epoch 100/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1132 - acc: 0.9636 - val_loss: 0.1385 - val_acc: 0.9692\n",
      "Epoch 101/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1134 - acc: 0.9631 - val_loss: 0.1405 - val_acc: 0.9682\n",
      "Epoch 102/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1109 - acc: 0.9639 - val_loss: 0.1400 - val_acc: 0.9696\n",
      "Epoch 103/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1105 - acc: 0.9642 - val_loss: 0.1381 - val_acc: 0.9691\n",
      "Epoch 104/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1149 - acc: 0.9644 - val_loss: 0.1385 - val_acc: 0.9699\n",
      "Epoch 105/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1119 - acc: 0.9639 - val_loss: 0.1404 - val_acc: 0.9686\n",
      "Epoch 106/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1114 - acc: 0.9635 - val_loss: 0.1427 - val_acc: 0.9692\n",
      "Epoch 107/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1130 - acc: 0.9632 - val_loss: 0.1399 - val_acc: 0.9697\n",
      "Epoch 108/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1102 - acc: 0.9638 - val_loss: 0.1409 - val_acc: 0.9695\n",
      "Epoch 109/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1119 - acc: 0.9634 - val_loss: 0.1379 - val_acc: 0.9679\n",
      "Epoch 110/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1104 - acc: 0.9642 - val_loss: 0.1411 - val_acc: 0.9688\n",
      "Epoch 111/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1087 - acc: 0.9647 - val_loss: 0.1406 - val_acc: 0.9694\n",
      "Epoch 112/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1124 - acc: 0.9637 - val_loss: 0.1400 - val_acc: 0.9691\n",
      "Epoch 113/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1095 - acc: 0.9650 - val_loss: 0.1395 - val_acc: 0.9692\n",
      "Epoch 114/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1076 - acc: 0.9644 - val_loss: 0.1415 - val_acc: 0.9692\n",
      "Epoch 115/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1087 - acc: 0.9653 - val_loss: 0.1404 - val_acc: 0.9699\n",
      "Epoch 116/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1099 - acc: 0.9648 - val_loss: 0.1396 - val_acc: 0.9696\n",
      "Epoch 117/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1097 - acc: 0.9657 - val_loss: 0.1428 - val_acc: 0.9690\n",
      "Epoch 118/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1072 - acc: 0.9657 - val_loss: 0.1402 - val_acc: 0.9694\n",
      "Epoch 119/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1101 - acc: 0.9649 - val_loss: 0.1416 - val_acc: 0.9693\n",
      "Epoch 120/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1067 - acc: 0.9651 - val_loss: 0.1436 - val_acc: 0.9692\n",
      "Epoch 121/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1060 - acc: 0.9655 - val_loss: 0.1421 - val_acc: 0.9690\n",
      "Epoch 122/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1069 - acc: 0.9657 - val_loss: 0.1411 - val_acc: 0.9689\n",
      "Epoch 123/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1057 - acc: 0.9662 - val_loss: 0.1417 - val_acc: 0.9693\n",
      "Epoch 124/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1054 - acc: 0.9657 - val_loss: 0.1439 - val_acc: 0.9692\n",
      "Epoch 125/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1093 - acc: 0.9641 - val_loss: 0.1388 - val_acc: 0.9699\n",
      "Epoch 126/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1063 - acc: 0.9657 - val_loss: 0.1410 - val_acc: 0.9700\n",
      "Epoch 127/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1069 - acc: 0.9649 - val_loss: 0.1422 - val_acc: 0.9696\n",
      "Epoch 128/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1071 - acc: 0.9652 - val_loss: 0.1393 - val_acc: 0.9701\n",
      "Epoch 129/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1087 - acc: 0.9645 - val_loss: 0.1419 - val_acc: 0.9700\n",
      "Epoch 130/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1065 - acc: 0.9653 - val_loss: 0.1404 - val_acc: 0.9701\n",
      "Epoch 131/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1089 - acc: 0.9655 - val_loss: 0.1414 - val_acc: 0.9686\n",
      "Epoch 132/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1052 - acc: 0.9651 - val_loss: 0.1424 - val_acc: 0.9687\n",
      "Epoch 133/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1055 - acc: 0.9662 - val_loss: 0.1438 - val_acc: 0.9684\n",
      "Epoch 134/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1049 - acc: 0.9654 - val_loss: 0.1413 - val_acc: 0.9688\n",
      "Epoch 135/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1060 - acc: 0.9666 - val_loss: 0.1404 - val_acc: 0.9689\n",
      "Epoch 136/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1027 - acc: 0.9671 - val_loss: 0.1426 - val_acc: 0.9687\n",
      "Epoch 137/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1046 - acc: 0.9663 - val_loss: 0.1430 - val_acc: 0.9691\n",
      "Epoch 138/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1073 - acc: 0.9648 - val_loss: 0.1429 - val_acc: 0.9690\n",
      "Epoch 139/500\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.1049 - acc: 0.9660 - val_loss: 0.1429 - val_acc: 0.9691\n",
      "Epoch 140/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1038 - acc: 0.9673 - val_loss: 0.1419 - val_acc: 0.9699\n",
      "Epoch 141/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1063 - acc: 0.9651 - val_loss: 0.1458 - val_acc: 0.9683\n",
      "Epoch 142/500\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.1053 - acc: 0.9652 - val_loss: 0.1428 - val_acc: 0.9691\n",
      "Epoch 143/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1046 - acc: 0.9660 - val_loss: 0.1431 - val_acc: 0.9690\n",
      "Epoch 144/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1056 - acc: 0.9657 - val_loss: 0.1438 - val_acc: 0.9680\n",
      "Epoch 145/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1052 - acc: 0.9652 - val_loss: 0.1428 - val_acc: 0.9693\n",
      "Epoch 146/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1052 - acc: 0.9652 - val_loss: 0.1437 - val_acc: 0.9689\n",
      "Epoch 147/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1027 - acc: 0.9659 - val_loss: 0.1423 - val_acc: 0.9701\n",
      "Epoch 148/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1023 - acc: 0.9663 - val_loss: 0.1460 - val_acc: 0.9686\n",
      "Epoch 149/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1045 - acc: 0.9658 - val_loss: 0.1455 - val_acc: 0.9688\n",
      "Epoch 150/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1032 - acc: 0.9663 - val_loss: 0.1440 - val_acc: 0.9690\n",
      "Epoch 151/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1025 - acc: 0.9664 - val_loss: 0.1442 - val_acc: 0.9690\n",
      "Epoch 152/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1033 - acc: 0.9667 - val_loss: 0.1463 - val_acc: 0.9688\n",
      "Epoch 153/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1035 - acc: 0.9658 - val_loss: 0.1445 - val_acc: 0.9700\n",
      "Epoch 154/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1037 - acc: 0.9660 - val_loss: 0.1447 - val_acc: 0.9690\n",
      "Epoch 155/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1023 - acc: 0.9663 - val_loss: 0.1459 - val_acc: 0.9685\n",
      "Epoch 156/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1020 - acc: 0.9666 - val_loss: 0.1433 - val_acc: 0.9696\n",
      "Epoch 157/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1037 - acc: 0.9662 - val_loss: 0.1453 - val_acc: 0.9685\n",
      "Epoch 158/500\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.1022 - acc: 0.9666 - val_loss: 0.1488 - val_acc: 0.9691\n",
      "Epoch 159/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1010 - acc: 0.9668 - val_loss: 0.1480 - val_acc: 0.9683\n",
      "Epoch 160/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1015 - acc: 0.9664 - val_loss: 0.1474 - val_acc: 0.9687\n",
      "Epoch 161/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1014 - acc: 0.9669 - val_loss: 0.1492 - val_acc: 0.9686\n",
      "Epoch 162/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1022 - acc: 0.9664 - val_loss: 0.1459 - val_acc: 0.9696\n",
      "Epoch 163/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1008 - acc: 0.9668 - val_loss: 0.1449 - val_acc: 0.9694\n",
      "Epoch 164/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0990 - acc: 0.9681 - val_loss: 0.1488 - val_acc: 0.9691\n",
      "Epoch 165/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1027 - acc: 0.9671 - val_loss: 0.1488 - val_acc: 0.9684\n",
      "Epoch 166/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.1001 - acc: 0.9672 - val_loss: 0.1463 - val_acc: 0.9704\n",
      "Epoch 167/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0987 - acc: 0.9672 - val_loss: 0.1488 - val_acc: 0.9689\n",
      "Epoch 168/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1021 - acc: 0.9661 - val_loss: 0.1475 - val_acc: 0.9682\n",
      "Epoch 169/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0994 - acc: 0.9682 - val_loss: 0.1494 - val_acc: 0.9697\n",
      "Epoch 170/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1032 - acc: 0.9664 - val_loss: 0.1469 - val_acc: 0.9693\n",
      "Epoch 171/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0998 - acc: 0.9675 - val_loss: 0.1482 - val_acc: 0.9683\n",
      "Epoch 172/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0975 - acc: 0.9676 - val_loss: 0.1494 - val_acc: 0.9679\n",
      "Epoch 173/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.1001 - acc: 0.9678 - val_loss: 0.1502 - val_acc: 0.9680\n",
      "Epoch 174/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0991 - acc: 0.9685 - val_loss: 0.1489 - val_acc: 0.9684\n",
      "Epoch 175/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0992 - acc: 0.9677 - val_loss: 0.1474 - val_acc: 0.9677\n",
      "Epoch 176/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.0997 - acc: 0.9674 - val_loss: 0.1473 - val_acc: 0.9675\n",
      "Epoch 177/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0989 - acc: 0.9682 - val_loss: 0.1462 - val_acc: 0.9691\n",
      "Epoch 178/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.0974 - acc: 0.9681 - val_loss: 0.1477 - val_acc: 0.9687\n",
      "Epoch 179/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0993 - acc: 0.9672 - val_loss: 0.1481 - val_acc: 0.9697\n",
      "Epoch 180/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0969 - acc: 0.9686 - val_loss: 0.1485 - val_acc: 0.9685\n",
      "Epoch 181/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0990 - acc: 0.9679 - val_loss: 0.1494 - val_acc: 0.9690\n",
      "Epoch 182/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.1004 - acc: 0.9674 - val_loss: 0.1505 - val_acc: 0.9680\n",
      "Epoch 183/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0984 - acc: 0.9684 - val_loss: 0.1490 - val_acc: 0.9686\n",
      "Epoch 184/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.1002 - acc: 0.9683 - val_loss: 0.1497 - val_acc: 0.9682\n",
      "Epoch 185/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0977 - acc: 0.9684 - val_loss: 0.1467 - val_acc: 0.9685\n",
      "Epoch 186/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0985 - acc: 0.9680 - val_loss: 0.1523 - val_acc: 0.9675\n",
      "Epoch 187/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0990 - acc: 0.9681 - val_loss: 0.1508 - val_acc: 0.9678\n",
      "Epoch 188/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0985 - acc: 0.9679 - val_loss: 0.1518 - val_acc: 0.9670\n",
      "Epoch 189/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0974 - acc: 0.9688 - val_loss: 0.1543 - val_acc: 0.9676\n",
      "Epoch 190/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0969 - acc: 0.9688 - val_loss: 0.1515 - val_acc: 0.9681\n",
      "Epoch 191/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0957 - acc: 0.9701 - val_loss: 0.1515 - val_acc: 0.9680\n",
      "Epoch 192/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0997 - acc: 0.9672 - val_loss: 0.1508 - val_acc: 0.9677\n",
      "Epoch 193/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0981 - acc: 0.9679 - val_loss: 0.1535 - val_acc: 0.9671\n",
      "Epoch 194/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0972 - acc: 0.9684 - val_loss: 0.1532 - val_acc: 0.9686\n",
      "Epoch 195/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0984 - acc: 0.9679 - val_loss: 0.1500 - val_acc: 0.9691\n",
      "Epoch 196/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0974 - acc: 0.9675 - val_loss: 0.1508 - val_acc: 0.9684\n",
      "Epoch 197/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0970 - acc: 0.9687 - val_loss: 0.1502 - val_acc: 0.9681\n",
      "Epoch 198/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0945 - acc: 0.9687 - val_loss: 0.1525 - val_acc: 0.9680\n",
      "Epoch 199/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0974 - acc: 0.9673 - val_loss: 0.1550 - val_acc: 0.9667\n",
      "Epoch 200/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0972 - acc: 0.9687 - val_loss: 0.1510 - val_acc: 0.9672\n",
      "Epoch 201/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0969 - acc: 0.9681 - val_loss: 0.1536 - val_acc: 0.9672\n",
      "Epoch 202/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0966 - acc: 0.9689 - val_loss: 0.1504 - val_acc: 0.9681\n",
      "Epoch 203/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0955 - acc: 0.9687 - val_loss: 0.1524 - val_acc: 0.9689\n",
      "Epoch 204/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0954 - acc: 0.9689 - val_loss: 0.1505 - val_acc: 0.9679\n",
      "Epoch 205/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0975 - acc: 0.9681 - val_loss: 0.1532 - val_acc: 0.9677\n",
      "Epoch 206/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0977 - acc: 0.9677 - val_loss: 0.1506 - val_acc: 0.9683\n",
      "Epoch 207/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0965 - acc: 0.9688 - val_loss: 0.1541 - val_acc: 0.9663\n",
      "Epoch 208/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0944 - acc: 0.9688 - val_loss: 0.1520 - val_acc: 0.9679\n",
      "Epoch 209/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0975 - acc: 0.9681 - val_loss: 0.1519 - val_acc: 0.9685\n",
      "Epoch 210/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0943 - acc: 0.9687 - val_loss: 0.1523 - val_acc: 0.9679\n",
      "Epoch 211/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0958 - acc: 0.9696 - val_loss: 0.1515 - val_acc: 0.9673\n",
      "Epoch 212/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0937 - acc: 0.9692 - val_loss: 0.1502 - val_acc: 0.9686\n",
      "Epoch 213/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0951 - acc: 0.9687 - val_loss: 0.1555 - val_acc: 0.9678\n",
      "Epoch 214/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0949 - acc: 0.9687 - val_loss: 0.1509 - val_acc: 0.9681\n",
      "Epoch 215/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.0950 - acc: 0.9689 - val_loss: 0.1529 - val_acc: 0.9678\n",
      "Epoch 216/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0947 - acc: 0.9698 - val_loss: 0.1540 - val_acc: 0.9675\n",
      "Epoch 217/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0921 - acc: 0.9695 - val_loss: 0.1564 - val_acc: 0.9668\n",
      "Epoch 218/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0951 - acc: 0.9695 - val_loss: 0.1544 - val_acc: 0.9679\n",
      "Epoch 219/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0946 - acc: 0.9689 - val_loss: 0.1520 - val_acc: 0.9689\n",
      "Epoch 220/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0953 - acc: 0.9685 - val_loss: 0.1544 - val_acc: 0.9675\n",
      "Epoch 221/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0919 - acc: 0.9703 - val_loss: 0.1518 - val_acc: 0.9673\n",
      "Epoch 222/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0949 - acc: 0.9688 - val_loss: 0.1564 - val_acc: 0.9678\n",
      "Epoch 223/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0949 - acc: 0.9697 - val_loss: 0.1557 - val_acc: 0.9683\n",
      "Epoch 224/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0933 - acc: 0.9700 - val_loss: 0.1565 - val_acc: 0.9667\n",
      "Epoch 225/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0960 - acc: 0.9686 - val_loss: 0.1572 - val_acc: 0.9670\n",
      "Epoch 226/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0965 - acc: 0.9688 - val_loss: 0.1546 - val_acc: 0.9684\n",
      "Epoch 227/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.0938 - acc: 0.9687 - val_loss: 0.1556 - val_acc: 0.9674\n",
      "Epoch 228/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0937 - acc: 0.9694 - val_loss: 0.1517 - val_acc: 0.9675\n",
      "Epoch 229/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0928 - acc: 0.9702 - val_loss: 0.1571 - val_acc: 0.9676\n",
      "Epoch 230/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0929 - acc: 0.9698 - val_loss: 0.1565 - val_acc: 0.9679\n",
      "Epoch 231/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0937 - acc: 0.9699 - val_loss: 0.1543 - val_acc: 0.9681\n",
      "Epoch 232/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0922 - acc: 0.9701 - val_loss: 0.1582 - val_acc: 0.9672\n",
      "Epoch 233/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0955 - acc: 0.9691 - val_loss: 0.1572 - val_acc: 0.9669\n",
      "Epoch 234/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0947 - acc: 0.9690 - val_loss: 0.1543 - val_acc: 0.9678\n",
      "Epoch 235/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0921 - acc: 0.9701 - val_loss: 0.1576 - val_acc: 0.9681\n",
      "Epoch 236/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0935 - acc: 0.9693 - val_loss: 0.1580 - val_acc: 0.9680\n",
      "Epoch 237/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0936 - acc: 0.9694 - val_loss: 0.1561 - val_acc: 0.9679\n",
      "Epoch 238/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0918 - acc: 0.9707 - val_loss: 0.1584 - val_acc: 0.9681\n",
      "Epoch 239/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0949 - acc: 0.9690 - val_loss: 0.1616 - val_acc: 0.9675\n",
      "Epoch 240/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0931 - acc: 0.9698 - val_loss: 0.1576 - val_acc: 0.9685\n",
      "Epoch 241/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0923 - acc: 0.9696 - val_loss: 0.1569 - val_acc: 0.9673\n",
      "Epoch 242/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0925 - acc: 0.9693 - val_loss: 0.1560 - val_acc: 0.9672\n",
      "Epoch 243/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0925 - acc: 0.9694 - val_loss: 0.1582 - val_acc: 0.9677\n",
      "Epoch 244/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0926 - acc: 0.9702 - val_loss: 0.1574 - val_acc: 0.9692\n",
      "Epoch 245/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0914 - acc: 0.9701 - val_loss: 0.1557 - val_acc: 0.9672\n",
      "Epoch 246/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.0930 - acc: 0.9694 - val_loss: 0.1589 - val_acc: 0.9675\n",
      "Epoch 247/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0925 - acc: 0.9693 - val_loss: 0.1576 - val_acc: 0.9676\n",
      "Epoch 248/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0926 - acc: 0.9694 - val_loss: 0.1571 - val_acc: 0.9686\n",
      "Epoch 249/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0916 - acc: 0.9707 - val_loss: 0.1557 - val_acc: 0.9678\n",
      "Epoch 250/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0924 - acc: 0.9696 - val_loss: 0.1593 - val_acc: 0.9678\n",
      "Epoch 251/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0900 - acc: 0.9700 - val_loss: 0.1593 - val_acc: 0.9677\n",
      "Epoch 252/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.0931 - acc: 0.9692 - val_loss: 0.1610 - val_acc: 0.9680\n",
      "Epoch 253/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0912 - acc: 0.9706 - val_loss: 0.1593 - val_acc: 0.9684\n",
      "Epoch 254/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0933 - acc: 0.9695 - val_loss: 0.1605 - val_acc: 0.9667\n",
      "Epoch 255/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0926 - acc: 0.9702 - val_loss: 0.1606 - val_acc: 0.9667\n",
      "Epoch 256/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0918 - acc: 0.9705 - val_loss: 0.1598 - val_acc: 0.9669\n",
      "Epoch 257/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0924 - acc: 0.9695 - val_loss: 0.1608 - val_acc: 0.9680\n",
      "Epoch 258/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0928 - acc: 0.9700 - val_loss: 0.1588 - val_acc: 0.9683\n",
      "Epoch 259/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0906 - acc: 0.9709 - val_loss: 0.1598 - val_acc: 0.9680\n",
      "Epoch 260/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0895 - acc: 0.9706 - val_loss: 0.1600 - val_acc: 0.9677\n",
      "Epoch 261/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0894 - acc: 0.9711 - val_loss: 0.1595 - val_acc: 0.9674\n",
      "Epoch 262/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0933 - acc: 0.9700 - val_loss: 0.1593 - val_acc: 0.9680\n",
      "Epoch 263/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0898 - acc: 0.9706 - val_loss: 0.1586 - val_acc: 0.9667\n",
      "Epoch 264/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0905 - acc: 0.9705 - val_loss: 0.1620 - val_acc: 0.9672\n",
      "Epoch 265/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0902 - acc: 0.9699 - val_loss: 0.1588 - val_acc: 0.9677\n",
      "Epoch 266/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.0903 - acc: 0.9713 - val_loss: 0.1630 - val_acc: 0.9663\n",
      "Epoch 267/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0914 - acc: 0.9701 - val_loss: 0.1620 - val_acc: 0.9680\n",
      "Epoch 268/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0906 - acc: 0.9703 - val_loss: 0.1594 - val_acc: 0.9676\n",
      "Epoch 269/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0924 - acc: 0.9699 - val_loss: 0.1593 - val_acc: 0.9673\n",
      "Epoch 270/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0930 - acc: 0.9698 - val_loss: 0.1634 - val_acc: 0.9667\n",
      "Epoch 271/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0888 - acc: 0.9704 - val_loss: 0.1630 - val_acc: 0.9657\n",
      "Epoch 272/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0910 - acc: 0.9701 - val_loss: 0.1629 - val_acc: 0.9672\n",
      "Epoch 273/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0914 - acc: 0.9704 - val_loss: 0.1611 - val_acc: 0.9680\n",
      "Epoch 274/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0939 - acc: 0.9698 - val_loss: 0.1634 - val_acc: 0.9666\n",
      "Epoch 275/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0900 - acc: 0.9714 - val_loss: 0.1610 - val_acc: 0.9674\n",
      "Epoch 276/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0894 - acc: 0.9702 - val_loss: 0.1631 - val_acc: 0.9675\n",
      "Epoch 277/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0907 - acc: 0.9707 - val_loss: 0.1646 - val_acc: 0.9663\n",
      "Epoch 278/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0891 - acc: 0.9710 - val_loss: 0.1644 - val_acc: 0.9658\n",
      "Epoch 279/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0897 - acc: 0.9709 - val_loss: 0.1646 - val_acc: 0.9663\n",
      "Epoch 280/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.0883 - acc: 0.9720 - val_loss: 0.1645 - val_acc: 0.9658\n",
      "Epoch 281/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0881 - acc: 0.9709 - val_loss: 0.1623 - val_acc: 0.9664\n",
      "Epoch 282/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0885 - acc: 0.9709 - val_loss: 0.1636 - val_acc: 0.9673\n",
      "Epoch 283/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0901 - acc: 0.9706 - val_loss: 0.1619 - val_acc: 0.9672\n",
      "Epoch 284/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0897 - acc: 0.9704 - val_loss: 0.1640 - val_acc: 0.9669\n",
      "Epoch 285/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0887 - acc: 0.9714 - val_loss: 0.1655 - val_acc: 0.9663\n",
      "Epoch 286/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0895 - acc: 0.9706 - val_loss: 0.1620 - val_acc: 0.9667\n",
      "Epoch 287/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.0901 - acc: 0.9702 - val_loss: 0.1624 - val_acc: 0.9674\n",
      "Epoch 288/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0914 - acc: 0.9701 - val_loss: 0.1667 - val_acc: 0.9655\n",
      "Epoch 289/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0898 - acc: 0.9711 - val_loss: 0.1665 - val_acc: 0.9657\n",
      "Epoch 290/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0889 - acc: 0.9714 - val_loss: 0.1653 - val_acc: 0.9669\n",
      "Epoch 291/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0895 - acc: 0.9709 - val_loss: 0.1617 - val_acc: 0.9674\n",
      "Epoch 292/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0888 - acc: 0.9708 - val_loss: 0.1647 - val_acc: 0.9671\n",
      "Epoch 293/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0880 - acc: 0.9720 - val_loss: 0.1637 - val_acc: 0.9673\n",
      "Epoch 294/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0874 - acc: 0.9710 - val_loss: 0.1672 - val_acc: 0.9661\n",
      "Epoch 295/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0898 - acc: 0.9705 - val_loss: 0.1667 - val_acc: 0.9671\n",
      "Epoch 296/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0886 - acc: 0.9704 - val_loss: 0.1673 - val_acc: 0.9660\n",
      "Epoch 297/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0894 - acc: 0.9712 - val_loss: 0.1644 - val_acc: 0.9664\n",
      "Epoch 298/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0907 - acc: 0.9701 - val_loss: 0.1688 - val_acc: 0.9659\n",
      "Epoch 299/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0895 - acc: 0.9707 - val_loss: 0.1638 - val_acc: 0.9663\n",
      "Epoch 300/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0904 - acc: 0.9694 - val_loss: 0.1671 - val_acc: 0.9663\n",
      "Epoch 301/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0885 - acc: 0.9710 - val_loss: 0.1644 - val_acc: 0.9657\n",
      "Epoch 302/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0892 - acc: 0.9707 - val_loss: 0.1673 - val_acc: 0.9656\n",
      "Epoch 303/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.0860 - acc: 0.9712 - val_loss: 0.1706 - val_acc: 0.9667\n",
      "Epoch 304/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0883 - acc: 0.9713 - val_loss: 0.1674 - val_acc: 0.9658\n",
      "Epoch 305/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0903 - acc: 0.9708 - val_loss: 0.1667 - val_acc: 0.9656\n",
      "Epoch 306/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0853 - acc: 0.9722 - val_loss: 0.1657 - val_acc: 0.9664\n",
      "Epoch 307/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0882 - acc: 0.9712 - val_loss: 0.1686 - val_acc: 0.9661\n",
      "Epoch 308/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0856 - acc: 0.9716 - val_loss: 0.1651 - val_acc: 0.9669\n",
      "Epoch 309/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0870 - acc: 0.9715 - val_loss: 0.1641 - val_acc: 0.9660\n",
      "Epoch 310/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0856 - acc: 0.9716 - val_loss: 0.1640 - val_acc: 0.9658\n",
      "Epoch 311/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0874 - acc: 0.9710 - val_loss: 0.1674 - val_acc: 0.9657\n",
      "Epoch 312/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0848 - acc: 0.9722 - val_loss: 0.1649 - val_acc: 0.9655\n",
      "Epoch 313/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0874 - acc: 0.9715 - val_loss: 0.1682 - val_acc: 0.9656\n",
      "Epoch 314/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0878 - acc: 0.9702 - val_loss: 0.1667 - val_acc: 0.9661\n",
      "Epoch 315/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0899 - acc: 0.9705 - val_loss: 0.1639 - val_acc: 0.9677\n",
      "Epoch 316/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0864 - acc: 0.9717 - val_loss: 0.1671 - val_acc: 0.9656\n",
      "Epoch 317/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0878 - acc: 0.9713 - val_loss: 0.1708 - val_acc: 0.9655\n",
      "Epoch 318/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0869 - acc: 0.9723 - val_loss: 0.1649 - val_acc: 0.9665\n",
      "Epoch 319/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0860 - acc: 0.9713 - val_loss: 0.1675 - val_acc: 0.9660\n",
      "Epoch 320/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0874 - acc: 0.9714 - val_loss: 0.1654 - val_acc: 0.9662\n",
      "Epoch 321/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0893 - acc: 0.9720 - val_loss: 0.1691 - val_acc: 0.9665\n",
      "Epoch 322/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0873 - acc: 0.9716 - val_loss: 0.1621 - val_acc: 0.9659\n",
      "Epoch 323/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0872 - acc: 0.9711 - val_loss: 0.1704 - val_acc: 0.9653\n",
      "Epoch 324/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0866 - acc: 0.9719 - val_loss: 0.1679 - val_acc: 0.9656\n",
      "Epoch 325/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0871 - acc: 0.9716 - val_loss: 0.1727 - val_acc: 0.9661\n",
      "Epoch 326/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0863 - acc: 0.9722 - val_loss: 0.1680 - val_acc: 0.9657\n",
      "Epoch 327/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0882 - acc: 0.9718 - val_loss: 0.1686 - val_acc: 0.9654\n",
      "Epoch 328/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0869 - acc: 0.9718 - val_loss: 0.1683 - val_acc: 0.9672\n",
      "Epoch 329/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0877 - acc: 0.9718 - val_loss: 0.1690 - val_acc: 0.9676\n",
      "Epoch 330/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0849 - acc: 0.9715 - val_loss: 0.1704 - val_acc: 0.9670\n",
      "Epoch 331/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0877 - acc: 0.9714 - val_loss: 0.1686 - val_acc: 0.9664\n",
      "Epoch 332/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0873 - acc: 0.9714 - val_loss: 0.1676 - val_acc: 0.9664\n",
      "Epoch 333/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0856 - acc: 0.9721 - val_loss: 0.1677 - val_acc: 0.9656\n",
      "Epoch 334/500\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0878 - acc: 0.9709 - val_loss: 0.1704 - val_acc: 0.9659\n",
      "Epoch 335/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.0873 - acc: 0.9717 - val_loss: 0.1685 - val_acc: 0.9672\n",
      "Epoch 336/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0863 - acc: 0.9715 - val_loss: 0.1710 - val_acc: 0.9662\n",
      "Epoch 337/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0863 - acc: 0.9712 - val_loss: 0.1703 - val_acc: 0.9668\n",
      "Epoch 338/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0868 - acc: 0.9724 - val_loss: 0.1704 - val_acc: 0.9659\n",
      "Epoch 339/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0869 - acc: 0.9716 - val_loss: 0.1676 - val_acc: 0.9665\n",
      "Epoch 340/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0863 - acc: 0.9718 - val_loss: 0.1708 - val_acc: 0.9654\n",
      "Epoch 341/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0839 - acc: 0.9729 - val_loss: 0.1710 - val_acc: 0.9669\n",
      "Epoch 342/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0868 - acc: 0.9717 - val_loss: 0.1728 - val_acc: 0.9665\n",
      "Epoch 343/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0850 - acc: 0.9717 - val_loss: 0.1689 - val_acc: 0.9648\n",
      "Epoch 344/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0849 - acc: 0.9725 - val_loss: 0.1711 - val_acc: 0.9657\n",
      "Epoch 345/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0831 - acc: 0.9727 - val_loss: 0.1724 - val_acc: 0.9659\n",
      "Epoch 346/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0835 - acc: 0.9726 - val_loss: 0.1701 - val_acc: 0.9660\n",
      "Epoch 347/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0860 - acc: 0.9722 - val_loss: 0.1691 - val_acc: 0.9654\n",
      "Epoch 348/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0869 - acc: 0.9708 - val_loss: 0.1704 - val_acc: 0.9660\n",
      "Epoch 349/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0863 - acc: 0.9717 - val_loss: 0.1706 - val_acc: 0.9671\n",
      "Epoch 350/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0869 - acc: 0.9711 - val_loss: 0.1744 - val_acc: 0.9659\n",
      "Epoch 351/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0876 - acc: 0.9722 - val_loss: 0.1746 - val_acc: 0.9669\n",
      "Epoch 352/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.0862 - acc: 0.9715 - val_loss: 0.1741 - val_acc: 0.9651\n",
      "Epoch 353/500\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.0858 - acc: 0.9720 - val_loss: 0.1698 - val_acc: 0.9665\n",
      "Epoch 354/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0874 - acc: 0.9721 - val_loss: 0.1742 - val_acc: 0.9658\n",
      "Epoch 355/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0845 - acc: 0.9725 - val_loss: 0.1704 - val_acc: 0.9658\n",
      "Epoch 356/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0859 - acc: 0.9718 - val_loss: 0.1701 - val_acc: 0.9657\n",
      "Epoch 357/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0857 - acc: 0.9721 - val_loss: 0.1700 - val_acc: 0.9658\n",
      "Epoch 358/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0832 - acc: 0.9721 - val_loss: 0.1724 - val_acc: 0.9664\n",
      "Epoch 359/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.0866 - acc: 0.9716 - val_loss: 0.1726 - val_acc: 0.9661\n",
      "Epoch 360/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0881 - acc: 0.9708 - val_loss: 0.1722 - val_acc: 0.9666\n",
      "Epoch 361/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0855 - acc: 0.9721 - val_loss: 0.1757 - val_acc: 0.9653\n",
      "Epoch 362/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0849 - acc: 0.9719 - val_loss: 0.1696 - val_acc: 0.9651\n",
      "Epoch 363/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0851 - acc: 0.9721 - val_loss: 0.1725 - val_acc: 0.9657\n",
      "Epoch 364/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0844 - acc: 0.9724 - val_loss: 0.1736 - val_acc: 0.9664\n",
      "Epoch 365/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0851 - acc: 0.9723 - val_loss: 0.1749 - val_acc: 0.9660\n",
      "Epoch 366/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0848 - acc: 0.9716 - val_loss: 0.1708 - val_acc: 0.9649\n",
      "Epoch 367/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0834 - acc: 0.9723 - val_loss: 0.1755 - val_acc: 0.9658\n",
      "Epoch 368/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0864 - acc: 0.9713 - val_loss: 0.1718 - val_acc: 0.9674\n",
      "Epoch 369/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0843 - acc: 0.9728 - val_loss: 0.1764 - val_acc: 0.9668\n",
      "Epoch 370/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.0850 - acc: 0.9725 - val_loss: 0.1729 - val_acc: 0.9662\n",
      "Epoch 371/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0861 - acc: 0.9715 - val_loss: 0.1747 - val_acc: 0.9661\n",
      "Epoch 372/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.0829 - acc: 0.9718 - val_loss: 0.1780 - val_acc: 0.9660\n",
      "Epoch 373/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0857 - acc: 0.9720 - val_loss: 0.1742 - val_acc: 0.9664\n",
      "Epoch 374/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0834 - acc: 0.9724 - val_loss: 0.1778 - val_acc: 0.9647\n",
      "Epoch 375/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0855 - acc: 0.9721 - val_loss: 0.1752 - val_acc: 0.9660\n",
      "Epoch 376/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0866 - acc: 0.9714 - val_loss: 0.1759 - val_acc: 0.9664\n",
      "Epoch 377/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0849 - acc: 0.9723 - val_loss: 0.1759 - val_acc: 0.9657\n",
      "Epoch 378/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0852 - acc: 0.9729 - val_loss: 0.1723 - val_acc: 0.9669\n",
      "Epoch 379/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0848 - acc: 0.9728 - val_loss: 0.1772 - val_acc: 0.9666\n",
      "Epoch 380/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0822 - acc: 0.9733 - val_loss: 0.1730 - val_acc: 0.9660\n",
      "Epoch 381/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0851 - acc: 0.9720 - val_loss: 0.1732 - val_acc: 0.9656\n",
      "Epoch 382/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0862 - acc: 0.9714 - val_loss: 0.1788 - val_acc: 0.9652\n",
      "Epoch 383/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0846 - acc: 0.9725 - val_loss: 0.1769 - val_acc: 0.9650\n",
      "Epoch 384/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0845 - acc: 0.9724 - val_loss: 0.1797 - val_acc: 0.9646\n",
      "Epoch 385/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0838 - acc: 0.9732 - val_loss: 0.1827 - val_acc: 0.9645\n",
      "Epoch 386/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0839 - acc: 0.9718 - val_loss: 0.1792 - val_acc: 0.9642\n",
      "Epoch 387/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0824 - acc: 0.9735 - val_loss: 0.1811 - val_acc: 0.9651\n",
      "Epoch 388/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0820 - acc: 0.9726 - val_loss: 0.1747 - val_acc: 0.9654\n",
      "Epoch 389/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0835 - acc: 0.9729 - val_loss: 0.1789 - val_acc: 0.9661\n",
      "Epoch 390/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0855 - acc: 0.9721 - val_loss: 0.1789 - val_acc: 0.9647\n",
      "Epoch 391/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0868 - acc: 0.9720 - val_loss: 0.1785 - val_acc: 0.9664\n",
      "Epoch 392/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0848 - acc: 0.9719 - val_loss: 0.1791 - val_acc: 0.9652\n",
      "Epoch 393/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0868 - acc: 0.9718 - val_loss: 0.1758 - val_acc: 0.9657\n",
      "Epoch 394/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0837 - acc: 0.9723 - val_loss: 0.1734 - val_acc: 0.9657\n",
      "Epoch 395/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0835 - acc: 0.9733 - val_loss: 0.1773 - val_acc: 0.9649\n",
      "Epoch 396/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0814 - acc: 0.9733 - val_loss: 0.1788 - val_acc: 0.9658\n",
      "Epoch 397/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0803 - acc: 0.9743 - val_loss: 0.1772 - val_acc: 0.9663\n",
      "Epoch 398/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0816 - acc: 0.9727 - val_loss: 0.1783 - val_acc: 0.9654\n",
      "Epoch 399/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0829 - acc: 0.9722 - val_loss: 0.1790 - val_acc: 0.9652\n",
      "Epoch 400/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0825 - acc: 0.9736 - val_loss: 0.1804 - val_acc: 0.9643\n",
      "Epoch 401/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0822 - acc: 0.9730 - val_loss: 0.1750 - val_acc: 0.9649\n",
      "Epoch 402/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0828 - acc: 0.9723 - val_loss: 0.1829 - val_acc: 0.9648\n",
      "Epoch 403/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0817 - acc: 0.9734 - val_loss: 0.1781 - val_acc: 0.9653\n",
      "Epoch 404/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0819 - acc: 0.9735 - val_loss: 0.1828 - val_acc: 0.9655\n",
      "Epoch 405/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0826 - acc: 0.9730 - val_loss: 0.1769 - val_acc: 0.9660\n",
      "Epoch 406/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0804 - acc: 0.9740 - val_loss: 0.1786 - val_acc: 0.9651\n",
      "Epoch 407/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0845 - acc: 0.9719 - val_loss: 0.1811 - val_acc: 0.9647\n",
      "Epoch 408/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0837 - acc: 0.9721 - val_loss: 0.1803 - val_acc: 0.9647\n",
      "Epoch 409/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0850 - acc: 0.9724 - val_loss: 0.1811 - val_acc: 0.9653\n",
      "Epoch 410/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0823 - acc: 0.9734 - val_loss: 0.1789 - val_acc: 0.9656\n",
      "Epoch 411/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0849 - acc: 0.9727 - val_loss: 0.1794 - val_acc: 0.9655\n",
      "Epoch 412/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0832 - acc: 0.9730 - val_loss: 0.1861 - val_acc: 0.9641\n",
      "Epoch 413/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0824 - acc: 0.9730 - val_loss: 0.1813 - val_acc: 0.9650\n",
      "Epoch 414/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0831 - acc: 0.9732 - val_loss: 0.1800 - val_acc: 0.9660\n",
      "Epoch 415/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0823 - acc: 0.9724 - val_loss: 0.1829 - val_acc: 0.9648\n",
      "Epoch 416/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.0795 - acc: 0.9739 - val_loss: 0.1819 - val_acc: 0.9646\n",
      "Epoch 417/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0837 - acc: 0.9719 - val_loss: 0.1810 - val_acc: 0.9652\n",
      "Epoch 418/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0811 - acc: 0.9735 - val_loss: 0.1795 - val_acc: 0.9652\n",
      "Epoch 419/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0807 - acc: 0.9733 - val_loss: 0.1814 - val_acc: 0.9652\n",
      "Epoch 420/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0851 - acc: 0.9721 - val_loss: 0.1805 - val_acc: 0.9657\n",
      "Epoch 421/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0812 - acc: 0.9741 - val_loss: 0.1814 - val_acc: 0.9645\n",
      "Epoch 422/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0838 - acc: 0.9723 - val_loss: 0.1833 - val_acc: 0.9655\n",
      "Epoch 423/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0807 - acc: 0.9734 - val_loss: 0.1830 - val_acc: 0.9657\n",
      "Epoch 424/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0852 - acc: 0.9721 - val_loss: 0.1780 - val_acc: 0.9653\n",
      "Epoch 425/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0789 - acc: 0.9743 - val_loss: 0.1804 - val_acc: 0.9660\n",
      "Epoch 426/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0844 - acc: 0.9724 - val_loss: 0.1832 - val_acc: 0.9653\n",
      "Epoch 427/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0854 - acc: 0.9725 - val_loss: 0.1816 - val_acc: 0.9648\n",
      "Epoch 428/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0830 - acc: 0.9729 - val_loss: 0.1835 - val_acc: 0.9655\n",
      "Epoch 429/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0811 - acc: 0.9739 - val_loss: 0.1805 - val_acc: 0.9648\n",
      "Epoch 430/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0842 - acc: 0.9726 - val_loss: 0.1834 - val_acc: 0.9652\n",
      "Epoch 431/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0813 - acc: 0.9742 - val_loss: 0.1840 - val_acc: 0.9645\n",
      "Epoch 432/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0824 - acc: 0.9726 - val_loss: 0.1855 - val_acc: 0.9637\n",
      "Epoch 433/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0811 - acc: 0.9727 - val_loss: 0.1816 - val_acc: 0.9648\n",
      "Epoch 434/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0799 - acc: 0.9738 - val_loss: 0.1820 - val_acc: 0.9657\n",
      "Epoch 435/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0823 - acc: 0.9723 - val_loss: 0.1822 - val_acc: 0.9649\n",
      "Epoch 436/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0822 - acc: 0.9734 - val_loss: 0.1850 - val_acc: 0.9644\n",
      "Epoch 437/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0792 - acc: 0.9742 - val_loss: 0.1875 - val_acc: 0.9647\n",
      "Epoch 438/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0831 - acc: 0.9728 - val_loss: 0.1844 - val_acc: 0.9647\n",
      "Epoch 439/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0844 - acc: 0.9734 - val_loss: 0.1858 - val_acc: 0.9649\n",
      "Epoch 440/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0811 - acc: 0.9731 - val_loss: 0.1817 - val_acc: 0.9650\n",
      "Epoch 441/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0813 - acc: 0.9732 - val_loss: 0.1848 - val_acc: 0.9650\n",
      "Epoch 442/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0838 - acc: 0.9732 - val_loss: 0.1845 - val_acc: 0.9649\n",
      "Epoch 443/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0811 - acc: 0.9735 - val_loss: 0.1840 - val_acc: 0.9646\n",
      "Epoch 444/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0807 - acc: 0.9733 - val_loss: 0.1859 - val_acc: 0.9650\n",
      "Epoch 445/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0785 - acc: 0.9746 - val_loss: 0.1821 - val_acc: 0.9649\n",
      "Epoch 446/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0809 - acc: 0.9733 - val_loss: 0.1801 - val_acc: 0.9661\n",
      "Epoch 447/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0806 - acc: 0.9737 - val_loss: 0.1870 - val_acc: 0.9642\n",
      "Epoch 448/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0804 - acc: 0.9737 - val_loss: 0.1803 - val_acc: 0.9651\n",
      "Epoch 449/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0818 - acc: 0.9736 - val_loss: 0.1868 - val_acc: 0.9646\n",
      "Epoch 450/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0825 - acc: 0.9724 - val_loss: 0.1903 - val_acc: 0.9639\n",
      "Epoch 451/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0828 - acc: 0.9725 - val_loss: 0.1875 - val_acc: 0.9643\n",
      "Epoch 452/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0819 - acc: 0.9728 - val_loss: 0.1885 - val_acc: 0.9655\n",
      "Epoch 453/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0833 - acc: 0.9733 - val_loss: 0.1829 - val_acc: 0.9644\n",
      "Epoch 454/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0799 - acc: 0.9740 - val_loss: 0.1860 - val_acc: 0.9651\n",
      "Epoch 455/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0782 - acc: 0.9744 - val_loss: 0.1905 - val_acc: 0.9655\n",
      "Epoch 456/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0813 - acc: 0.9728 - val_loss: 0.1888 - val_acc: 0.9646\n",
      "Epoch 457/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.0810 - acc: 0.9735 - val_loss: 0.1861 - val_acc: 0.9643\n",
      "Epoch 458/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0798 - acc: 0.9734 - val_loss: 0.1925 - val_acc: 0.9640\n",
      "Epoch 459/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0828 - acc: 0.9729 - val_loss: 0.1863 - val_acc: 0.9656\n",
      "Epoch 460/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0815 - acc: 0.9731 - val_loss: 0.1860 - val_acc: 0.9642\n",
      "Epoch 461/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0812 - acc: 0.9740 - val_loss: 0.1874 - val_acc: 0.9649\n",
      "Epoch 462/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0798 - acc: 0.9738 - val_loss: 0.1893 - val_acc: 0.9644\n",
      "Epoch 463/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0815 - acc: 0.9734 - val_loss: 0.1856 - val_acc: 0.9644\n",
      "Epoch 464/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0800 - acc: 0.9737 - val_loss: 0.1853 - val_acc: 0.9655\n",
      "Epoch 465/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0820 - acc: 0.9737 - val_loss: 0.1870 - val_acc: 0.9649\n",
      "Epoch 466/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0826 - acc: 0.9731 - val_loss: 0.1838 - val_acc: 0.9663\n",
      "Epoch 467/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0839 - acc: 0.9728 - val_loss: 0.1869 - val_acc: 0.9649\n",
      "Epoch 468/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0825 - acc: 0.9734 - val_loss: 0.1863 - val_acc: 0.9647\n",
      "Epoch 469/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0804 - acc: 0.9736 - val_loss: 0.1880 - val_acc: 0.9650\n",
      "Epoch 470/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0821 - acc: 0.9729 - val_loss: 0.1874 - val_acc: 0.9651\n",
      "Epoch 471/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0820 - acc: 0.9737 - val_loss: 0.1857 - val_acc: 0.9656\n",
      "Epoch 472/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0819 - acc: 0.9732 - val_loss: 0.1862 - val_acc: 0.9647\n",
      "Epoch 473/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0763 - acc: 0.9748 - val_loss: 0.1874 - val_acc: 0.9655\n",
      "Epoch 474/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0849 - acc: 0.9733 - val_loss: 0.1866 - val_acc: 0.9637\n",
      "Epoch 475/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0827 - acc: 0.9736 - val_loss: 0.1892 - val_acc: 0.9642\n",
      "Epoch 476/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0804 - acc: 0.9737 - val_loss: 0.1837 - val_acc: 0.9652\n",
      "Epoch 477/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0805 - acc: 0.9737 - val_loss: 0.1841 - val_acc: 0.9650\n",
      "Epoch 478/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0832 - acc: 0.9730 - val_loss: 0.1913 - val_acc: 0.9647\n",
      "Epoch 479/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0792 - acc: 0.9740 - val_loss: 0.1854 - val_acc: 0.9648\n",
      "Epoch 480/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0810 - acc: 0.9742 - val_loss: 0.1918 - val_acc: 0.9653\n",
      "Epoch 481/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0792 - acc: 0.9743 - val_loss: 0.1870 - val_acc: 0.9657\n",
      "Epoch 482/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0815 - acc: 0.9730 - val_loss: 0.1982 - val_acc: 0.9629\n",
      "Epoch 483/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.0786 - acc: 0.9745 - val_loss: 0.1873 - val_acc: 0.9654\n",
      "Epoch 484/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0795 - acc: 0.9736 - val_loss: 0.1907 - val_acc: 0.9644\n",
      "Epoch 485/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0822 - acc: 0.9736 - val_loss: 0.1848 - val_acc: 0.9656\n",
      "Epoch 486/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0794 - acc: 0.9740 - val_loss: 0.1871 - val_acc: 0.9642\n",
      "Epoch 487/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0802 - acc: 0.9747 - val_loss: 0.1898 - val_acc: 0.9644\n",
      "Epoch 488/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0816 - acc: 0.9734 - val_loss: 0.1886 - val_acc: 0.9646\n",
      "Epoch 489/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0827 - acc: 0.9738 - val_loss: 0.1900 - val_acc: 0.9643\n",
      "Epoch 490/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0785 - acc: 0.9746 - val_loss: 0.1889 - val_acc: 0.9654\n",
      "Epoch 491/500\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.0801 - acc: 0.9733 - val_loss: 0.1843 - val_acc: 0.9656\n",
      "Epoch 492/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0793 - acc: 0.9741 - val_loss: 0.1870 - val_acc: 0.9654\n",
      "Epoch 493/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0811 - acc: 0.9735 - val_loss: 0.1907 - val_acc: 0.9649\n",
      "Epoch 494/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0834 - acc: 0.9726 - val_loss: 0.1910 - val_acc: 0.9644\n",
      "Epoch 495/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0814 - acc: 0.9732 - val_loss: 0.1908 - val_acc: 0.9655\n",
      "Epoch 496/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0808 - acc: 0.9722 - val_loss: 0.1901 - val_acc: 0.9650\n",
      "Epoch 497/500\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0772 - acc: 0.9755 - val_loss: 0.1892 - val_acc: 0.9652\n",
      "Epoch 498/500\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0775 - acc: 0.9737 - val_loss: 0.1896 - val_acc: 0.9652\n",
      "Epoch 499/500\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0823 - acc: 0.9731 - val_loss: 0.1932 - val_acc: 0.9644\n",
      "Epoch 500/500\n",
      "60000/60000 [==============================] - 1s 15us/step - loss: 0.0788 - acc: 0.9746 - val_loss: 0.1893 - val_acc: 0.9660\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3c040dc390>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_dir = 'logs/loss_function_b/'\n",
    "logging = TensorBoard(log_dir=log_dir)\n",
    "checkpoint = ModelCheckpoint(log_dir + 'ep{epoch:03d}-acc{acc:.4f}-val_acc{val_acc:.4f}-loss{loss:.4f}-val_loss{val_loss:.4f}.h5',\n",
    "        monitor='val_acc', save_weights_only=True, save_best_only=True, period=3)\n",
    "\n",
    "history = studentB.fit(X_train, Y_train_new,\n",
    "                      batch_size=256,\n",
    "                      epochs=epochs,\n",
    "                      verbose=1,\n",
    "                      validation_data=(X_test, Y_test_new),\n",
    "            callbacks=[logging,checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_acc = history.history['acc'][-1]\n",
    "last_val_acc = history.history['val_acc'][-1]\n",
    "last_loss = history.history['loss'][-1]\n",
    "last_val_loss = history.history['val_loss'][-1]\n",
    "\n",
    "hist = \"acc{0:.4f}-val_acc{0:.4f}-loss{0:.4f}-val_loss{0:.4f}\".format(last_acc,last_val_acc,last_loss,last_val_loss)\n",
    "studentB.save_weights(log_dir + \"last_\"+ hist + \".h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Teacher model\n",
    "\n",
    "input_shape = (28, 28, 1) # Input shape of each image\n",
    "\n",
    "# Hyperparameters\n",
    "nb_filters = 64 # number of convolutional filters to use\n",
    "pool_size = (2, 2) # size of pooling area for max pooling\n",
    "kernel_size = (3, 3) # convolution kernel size\n",
    "\n",
    "teacher = Sequential()\n",
    "teacher.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "teacher.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "teacher.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "teacher.add(Dropout(0.25)) # For reguralization\n",
    "\n",
    "teacher.add(Flatten())\n",
    "teacher.add(Dense(128, activation='relu'))\n",
    "teacher.add(Dropout(0.5)) # For reguralization\n",
    "\n",
    "teacher.add(Dense(nb_classes))\n",
    "teacher.add(Activation('softmax')) # Note that we add a normal softmax layer to begin with\n",
    "\n",
    "teacher.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "print(teacher.summary())\n",
    "# This is a standalone student model (same number of layers as original student model) trained on same data\n",
    "# for comparing it with teacher trained student.\n",
    "student = Sequential()\n",
    "student.add(Flatten(input_shape=input_shape))\n",
    "student.add(Dense(32, activation='relu'))\n",
    "student.add(Dropout(0.2))\n",
    "student.add(Dense(nb_classes))\n",
    "student.add(Activation('softmax'))\n",
    "\n",
    "student.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv2d_3_input (InputLayer)     (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 26, 26, 32)   320         conv2d_3_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 24, 24, 64)   18496       conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 12, 12, 64)   0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8_input (InputLayer)    (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 12, 12, 64)   0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 784)          0           flatten_8_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 9216)         0           dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 32)           25120       flatten_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 128)          1179776     flatten_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 32)           0           dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 128)          0           dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 10)           330         dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 10)           1290        dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 10)           0           dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 10)           0           dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 20)           0           activation_12[0][0]              \n",
      "                                                                 activation_11[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,225,332\n",
      "Trainable params: 1,225,332\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#label = Input(shape=(416, 416, 3))\n",
    "    \n",
    "student_layer = student.layers[-1].output # This is going to be a tensor. And hence it needs to pass through a Activation layer\n",
    "\n",
    "teacher_layer =  teacher.layers[-1].output\n",
    "\n",
    "\n",
    "output = concatenate([student_layer, teacher_layer])\n",
    "\n",
    "\n",
    "# This is our new student model\n",
    "student_teacher = Model( [student.input,teacher.input] , output )\n",
    "\n",
    "student_teacher.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.08676973, 0.19380417, 0.13423479, 0.06452694, 0.06149981,\n",
       "        0.07837997, 0.08502943, 0.09195261, 0.15730867, 0.04649395,\n",
       "        0.10032204, 0.09717463, 0.09943859, 0.10250463, 0.10463606,\n",
       "        0.09636588, 0.1015408 , 0.09520203, 0.09479003, 0.10802537]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_teacher.predict( [X_train[0].reshape(1,28,28,1), X_train[0].reshape(1,28,28,1) ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Knowledge-Distillation_AI.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "tf36",
   "language": "python",
   "name": "tf36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
